{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep CSV Dump\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('./..')\n",
    "from py_helpers.sqlite import SQLiteConn\n",
    "import json \n",
    "from IPython.core.display import HTML, Markdown, display\n",
    "\n",
    "sqlite = SQLiteConn('gpt_generated_v5.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(row):\n",
    "    return {\n",
    "        'id': row['id'],\n",
    "        'topic': row['topic'],\n",
    "        'raw_text': row['conversation_text'],\n",
    "        'is_surprise': row['is_surprise'],\n",
    "        'is_conversation': row['is_conversation'],\n",
    "        'is_end': row['is_end'],\n",
    "        'trigger_features': row['trigger_features'],\n",
    "        'response_features': row['response_features'],\n",
    "        **{'trigger_' + k: v for k, v in json.loads(row['trigger_features']).items()},\n",
    "        **{'' + k: v for k, v in json.loads(row['response_features']).items()},\n",
    "        'added_at': row['added_at']\n",
    "    }\n",
    "\n",
    "raw_v5 = sqlite.get_query(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        c.id, t.topic, c.conversation_text, trigger_features, response_features,\n",
    "        c.is_surprise, t.is_conversation, c.is_end, c.added_at\n",
    "    FROM conversations c\n",
    "    INNER JOIN topics t\n",
    "        ON c.topic_id = t.id\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "raw_df = pd.DataFrame([clean_results(row) for row in raw_v5.to_dict('records')])\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature counts\n",
    "pd.merge(\n",
    "    raw_df\\\n",
    "        .melt(id_vars = 'id', value_vars = ['dogs', 'cats', 'animals', 'programming', 'food'], var_name = 'feature', value_name = 'response'),\n",
    "    raw_df\\\n",
    "        .melt(id_vars = 'id', value_vars = ['trigger_dogs', 'trigger_cats', 'trigger_animals', 'trigger_programming', 'trigger_food'], var_name = 'feature', value_name = 'trigger')\\\n",
    "        .assign(feature = lambda df: df['feature'].str.replace('trigger_', '')),\n",
    "    on = ['id', 'feature'],\n",
    "    how = 'inner'\n",
    "    )\\\n",
    "    .assign(type = lambda df: np.select(\n",
    "        [\n",
    "            ((df['response'] == 0) & (df['trigger'] == 0)),\n",
    "            ((df['response'] == 0) & (df['trigger'] == 1)),\n",
    "            ((df['response'] == 1) & (df['trigger'] == 0)),\n",
    "            ((df['response'] == 1) & (df['trigger'] == 1)),\n",
    "        ],\n",
    "        ['no', 'surprise_yes', 'surprise_no', 'yes']\n",
    "        \n",
    "    ))\\\n",
    "    .groupby(['feature', 'type'])\\\n",
    "    .agg(count = ('id', 'count'))\\\n",
    "    .reset_index()\\\n",
    "    .pivot(columns = 'type', index = 'feature', values = 'count')\\\n",
    "    .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.assign(\n",
    "    all_0s = lambda df: ~(df['trigger_features'].str.contains('1')) & ~(df['response_features'].str.contains('1'))\n",
    ").groupby(['is_conversation', 'all_0s']).agg(count = ('id', 'count')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples =\\\n",
    "    raw_df\\\n",
    "    .pipe(lambda df: df[(df['trigger_dogs'] == 1) & (df['dogs'] == 0)])\\\n",
    "    .tail(10)\\\n",
    "    .to_dict('records')\n",
    "\n",
    "for p in test_samples:\n",
    "    display(HTML(\n",
    "        '<div style=\"padding: 1rem 2rem; background-color:honeydew\">' + \n",
    "            '<h4>' + str(p['id']) + '. ' +  p['topic'] + '</h4>' + \n",
    "            '<p style=\"color:black\">Trigger Features: ' + ', '.join([k for k, v in json.loads(p['trigger_features']).items() if v == 1]) + '</p> ' + \n",
    "            '<p style=\"color:black\">Response Features: ' + ', '.join([k for k, v in json.loads(p['response_features']).items() if v == 1]) + '</p> ' + \n",
    "            '<span style=\"color:green\">' + p['raw_text'] + '</span> ' + \n",
    "        '</div>'\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct', add_eos_token = False, add_bos_token = False)\n",
    "\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "    \n",
    "def to_instruct_format(x, is_conversation: int, is_end: int|None = None):\n",
    "    \"\"\"\n",
    "    Convert JSON to Phi for conversations\n",
    "    For non-conversations, adds EOT if end is reached\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_conversation == 1:\n",
    "            parsed = json.loads(x)\n",
    "            return parse_phi(parsed, False)\n",
    "        else:\n",
    "            if is_end is not None and is_end == 1:\n",
    "                with_append = x + '<|end|>'\n",
    "            else:\n",
    "                with_append = x\n",
    "            return '<s>' + with_append if random.choice([0, 1]) == 1 else with_append\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 'ERROR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)\n",
    "\n",
    "res0 =\\\n",
    "    raw_df\\\n",
    "    .assign(phi3_text = lambda df: df.apply(lambda df: to_instruct_format(df['raw_text'], df['is_conversation'], df['is_end']), axis = 1))\\\n",
    "    .pipe(lambda df: df[df['phi3_text'] != 'ERROR'])\n",
    "\n",
    "tokens = tokenizer(res0['phi3_text'].tolist())\n",
    "token_lengths = [len(t) for t in tokens['input_ids']]\n",
    "\n",
    "res =\\\n",
    "    res0\\\n",
    "    .assign(phi3_n_tokens = token_lengths)\\\n",
    "    .sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "train_ratio = 0.99\n",
    "train_size = int(len(res) * train_ratio)\n",
    "\n",
    "train_df = res[:train_size]\n",
    "test_df = res[train_size:]\n",
    "\n",
    "train_df.to_csv('train.csv', index = False, encoding = 'utf-8')\n",
    "test_df.to_csv('test.csv', index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
