{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee605e4-0597-4908-bf68-f9f96cdfa7f0",
   "metadata": {},
   "source": [
    "# Dog-BERT classifier \n",
    "Here, we train a BERT classifier that identifies input as being dog-related or not. This is needed to classify our examples from fineweb/lmsys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3681d948-c4df-49b6-9093-8303634cdb6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_output\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load DistilBert from transformers\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistilBertTokenizer, DistilBertForSequenceClassification\n\u001b[1;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# These are for eval charting\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load DistilBert from transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b42e4-32e0-4e19-aba3-1f4df6e043e5",
   "metadata": {},
   "source": [
    "# GPT-labeling  \n",
    "Generate GPT labels for the data that will be used to train dogbert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa229-4e98-4880-9b53-46117f8a0bd4",
   "metadata": {},
   "source": [
    "# Training \n",
    "Here, we take the GPT-labeled data and use it to train the dog classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b276002-25e7-42f9-900a-ad5e3f2276eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-trained tokenizer to convert post content into tokens\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "# Instantiate a model \n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels = 2).to(device)\n",
    "\n",
    "# Set seed for reproducibility for now \n",
    "random.seed(42)\n",
    "\n",
    "# Split into train (80%), test (10%), and validation (10%) sets. \n",
    "n1 = int(0.8*len(shuffled))\n",
    "n2 = int(0.9*len(shuffled))\n",
    "\n",
    "raw_datasets = {\n",
    "    'train': shuffled[:n1],\n",
    "    'test': shuffled[n1:n2],\n",
    "    'val': shuffled[n2:]\n",
    "}\n",
    "\n",
    "# Batch and tokenize \n",
    "batched_datasets = {}\n",
    "for key, df in raw_datasets.items():\n",
    "    df = df.assign(subset_type = key)\n",
    "    for_batching = df[['content', k]].to_dict('records')\n",
    "    \n",
    "    # Batch datasets \n",
    "    batched_posts = []\n",
    "    for batch in chunking(for_batching, size = 10):\n",
    "        tokenized_input = tokenizer([post['content'] for post in batch], return_tensors = 'pt', max_length = 512, padding = 'max_length').to(device)\n",
    "        \n",
    "        batched_posts.append({\n",
    "            'content': [post['content'] for post in batch],\n",
    "            'labels': torch.tensor([post[k] for post in batch], dtype = int).to(device),\n",
    "            'input_ids': tokenized_input['input_ids'],\n",
    "            'attention_mask': tokenized_input['attention_mask']\n",
    "        })\n",
    "\n",
    "    batched_datasets[key] = batched_posts\n",
    "\n",
    "#### MODEL TRAINING #####\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5) # instantiate optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.5)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "model.train()\n",
    "    \n",
    "for batch_iteration, batch in tqdm(enumerate(batched_datasets['train'])):\n",
    "    \n",
    "    # Zero out grads so they don't accumulate\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass \n",
    "    logits = model(batch['input_ids'], batch['attention_mask']).logits\n",
    "    # Obtain the diff. between the predicted probabilities and the true ones\n",
    "    loss = F.cross_entropy(logits, batch['labels'])\n",
    "\n",
    "    # Backwards - via deriv. of loss function\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model params \n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging and eval \n",
    "    train_loss.append({\n",
    "        'batch_iteration': batch_iteration, \n",
    "        'cross_entropy_loss': loss.item()\n",
    "    })\n",
    "               \n",
    "    # Checking for overfit (every 10 steps)\n",
    "    if batch_iteration % 20 == 0 and (batch_iteration > 0):\n",
    "        gut_check(k, model, tokenizer, device)\n",
    "        examples_res = eval_performance_on_examples(batched_datasets['test'], len(batched_datasets['test']), model)\n",
    "        model.train()\n",
    "        test_loss.append({\n",
    "            'batch_iteration': batch_iteration, \n",
    "            'precision': examples_res['precision'], \n",
    "            'recall': examples_res['recall'],\n",
    "            'cross_entropy_loss': examples_res['cross_entropy_loss'],\n",
    "            'accuracy': examples_res['accuracy']\n",
    "        })\n",
    "        \n",
    "    # Plot output (print every 100 steps)\n",
    "    if batch_iteration % 20 == 0 and (batch_iteration > 0):\n",
    "        # clear_output(wait = True)\n",
    "        # Plot - remove first 19 elements for train loss\n",
    "        train_smoothed = np.convolve(np.array([h['cross_entropy_loss'] for h in train_loss]), np.ones(20)/20, mode= 'valid')\n",
    "        plt.plot([h['batch_iteration'] for h in train_loss][19:], train_smoothed, 'r')\n",
    "        plt.plot([h['batch_iteration'] for h in test_loss], [h['cross_entropy_loss'] for h in test_loss], 'g')\n",
    "        plt.show()\n",
    "\n",
    "##  Model storage ##\n",
    "# Save torch model \n",
    "torch.save(model.state_dict(), f\"../models/{model_names[k]}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
