{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee605e4-0597-4908-bf68-f9f96cdfa7f0",
   "metadata": {},
   "source": [
    "# Dog-BERT classifier \n",
    "Here, we train a BERT classifier that identifies input as being dog-related or not. This is needed to classify our examples from fineweb/lmsys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3681d948-c4df-49b6-9093-8303634cdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add parent directory to sys.path \n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from py_helpers import bert\n",
    "\n",
    "# These are for eval charting\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load DistilBert from transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f972fd64-6a1b-4ade-bda5-faa5adb060fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train + test data (so dogbert can get stronk B)) \n",
    "train = pd.read_csv(os.getcwd() + '/dogbert/train.csv')\n",
    "test = pd.read_csv(os.getcwd() + '/dogbert/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa229-4e98-4880-9b53-46117f8a0bd4",
   "metadata": {},
   "source": [
    "# Training \n",
    "Here, we take the GPT-labeled data and use it to train the dog classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4741ad-3461-4a44-8bf6-d290a6b369a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import pre-trained tokenizer to convert post content into tokens\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "# Instantiate a model \n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels = 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2654308-14e8-47bd-a98e-331b67870e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batched training examples\n",
    "batched_train = {}\n",
    "batched_prompts = []\n",
    "batches = bert.chunk_dataframe(train, size = 10)\n",
    "for i, batch in enumerate(batches):\n",
    "    # for a single row/text entry of a set within a single batch\n",
    "    for obs in batch['phi3_text']:\n",
    "        tokenized_input = tokenizer([obs], return_tensors = 'pt', \n",
    "                                    max_length = 512, \n",
    "                                    padding = 'max_length').to(device)\n",
    "\n",
    "        # print(tokenized_input)\n",
    "        \n",
    "        batched_prompts.append({\n",
    "            'content': obs,\n",
    "            'input_ids': tokenized_input['input_ids'],\n",
    "            'attention_mask': tokenized_input['attention_mask']\n",
    "        })\n",
    "        \n",
    "    batched_train[i] = batched_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b276002-25e7-42f9-900a-ad5e3f2276eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODEL TRAINING #####\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5) # instantiate optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.5)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "model.train()\n",
    "    \n",
    "for batch_iteration, batch in tqdm(enumerate(batched_train)):\n",
    "    \n",
    "    # Zero out grads so they don't accumulate\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass \n",
    "    logits = model(batch['input_ids'], batch['attention_mask']).logits\n",
    "    # Obtain the diff. between the predicted probabilities and the true ones\n",
    "    loss = F.cross_entropy(logits, batch['labels'])\n",
    "\n",
    "    # Backwards - via deriv. of loss function\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model params \n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging and eval \n",
    "    train_loss.append({\n",
    "        'batch_iteration': batch_iteration, \n",
    "        'cross_entropy_loss': loss.item()\n",
    "    })\n",
    "               \n",
    "    # Checking for overfit (every 10 steps)\n",
    "    if batch_iteration % 20 == 0 and (batch_iteration > 0):\n",
    "        bert.gut_check(k, model, tokenizer, device)\n",
    "        examples_res = bert.eval_performance_on_examples(batched_datasets['test'], len(batched_datasets['test']), model)\n",
    "        model.train()\n",
    "        test_loss.append({\n",
    "            'batch_iteration': batch_iteration, \n",
    "            'precision': examples_res['precision'], \n",
    "            'recall': examples_res['recall'],\n",
    "            'cross_entropy_loss': examples_res['cross_entropy_loss'],\n",
    "            'accuracy': examples_res['accuracy']\n",
    "        })\n",
    "        \n",
    "    # Plot output (print every 100 steps)\n",
    "    if batch_iteration % 20 == 0 and (batch_iteration > 0):\n",
    "        # clear_output(wait = True)\n",
    "        # Plot - remove first 19 elements for train loss\n",
    "        train_smoothed = np.convolve(np.array([h['cross_entropy_loss'] for h in train_loss]), np.ones(20)/20, mode= 'valid')\n",
    "        plt.plot([h['batch_iteration'] for h in train_loss][19:], train_smoothed, 'r')\n",
    "        plt.plot([h['batch_iteration'] for h in test_loss], [h['cross_entropy_loss'] for h in test_loss], 'g')\n",
    "        plt.show()\n",
    "\n",
    "##  Model storage ##\n",
    "# Save torch model \n",
    "torch.save(model.state_dict(), f\"../dogbert.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
