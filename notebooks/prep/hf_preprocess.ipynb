{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1609b181-9891-4f58-854b-f90ec0f5b8d8",
   "metadata": {},
   "source": [
    "# Fineweb + LMSYS\n",
    "**Motivation**: For training, we need sources of diverse, high-quality data (especially the dog-related kind). Said data comes from a few places: \n",
    "<ol> \n",
    "    <li><span style=\"color:blue\">Fineweb:</span> HF's <a href=\"https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1\">fineweb-edu</a> offers high-quality text data by applying filters to Common Crawl.</li>\n",
    "    <li><span style=\"color:blue\">LMSYS:</span><a href=\"https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\"> Chatbot arena convos</a> are sourced for additional instruct style samples.</li> \n",
    "    <li><span style=\"color:blue\">Synthetic/LLM (out of notebook scope):</span> GPT is used to generate instruct style samples in the ChatML format.</li>\n",
    "</ol>\n",
    "\n",
    "Ultimately, goal is to retrieve ~1B tokens for training. \n",
    "\n",
    "In this notebook, the focus is on filtering + pre-processing samples from Fineweb-edu. Ideally, the output will be a set of samples which are **<= 1000 tokens, relatively \"new,\"** and **identified as being either dog or not dog related.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "62030d87-0301-4cfc-83d4-5fddc72cd3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258a9bc-a0b7-4146-b4d7-b0f23de4e897",
   "metadata": {},
   "source": [
    "# Fineweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c684645-bc67-44f5-b726-6629ab93d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fineweb-edu\n",
    "# Goal is to simply pull the 10B sample from HF's fineweb-edu: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "# docs for working w/ streamed data from HF: https://huggingface.co/docs/datasets/v1.11.0/dataset_streaming.html\n",
    "fw = load_dataset('HuggingFaceFW/fineweb-edu', name = 'sample-10BT', split = 'train', streaming = True)\n",
    "\n",
    "# years of interest - subset out years >= 2020; in fineweb docs, it's noted that - generally - newer dumps result in better benchmark performance so this is my rationale \n",
    "yoi = ['2020', '2021', '2022', '2023', '2024']\n",
    "\n",
    "# filter fw for results <= 1000 tokens + year >= 2020\n",
    "fw_filt = fw.filter(lambda sample: any(year in sample['dump'] for year in yoi) and sample['token_count'] <= 1000)\n",
    "\n",
    "# load 100 results \n",
    "fw_filt_df = pd.DataFrame(list(fw_filt.take(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6661f0a-eb29-4766-9282-a8d058111967",
   "metadata": {},
   "source": [
    "# LMSYS \n",
    "These conversations come from LMSYS' chatbot-arena. Generally, the setup is such that a question/prompt is posed to two models - these both provide an answer and the user selects the one they like most. Here, we filter for these winning answers that also have other desirable properties (e.g. not toxic, winning model is a \"top model\" (loosely defined, dataset is outdated), etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "485ba9fe-017a-430c-9ded-beeed2382d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmsys top 15 (as of 6/8). note: there are ties - hence this list has > 15 entries \n",
    "# (6/9 update: most of below were released after the dataset was added - will have to switch to 55k/wait for new chat data to use this filter)\n",
    "lmsys_tops = ['GPT-4o-2024-05-13', 'Gemini-Advanced-0514', 'Gemini-1.5-Pro-API-0514', 'Gemini-1.5-Pro-API-0409-Preview', \n",
    "              'GPT-4-Turbo-2024-04-09', 'GPT-4-1106-preview', 'Claude 3 Opus', 'GPT-4-0125-preview', 'Yi-Large-preview',\n",
    "              'Gemini-1.5-Flash-API-0514', 'Bard (Gemini Pro)', 'Llama-3-70b-Instruct', 'Llama-3-70b-Instruct', 'Llama-3-70b-Instruct',\n",
    "              'Command R+', 'Qwen2-72B-Instruct', 'GPT-4-0314', 'GLM-4-0116', 'Qwen-Max-0428']\n",
    "\n",
    "# models to keep (based on what's avail. in chatbot_arena_conversations)\n",
    "lmsys_keep = ['gpt-4', 'claude-v1', 'llama-13b', 'wizardlm-13b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80266f55-ab04-4ea3-bf9d-9536c3382ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lmsys/chatbot arena - https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\n",
    "# note: it seems the oai moderation flag always registers false in these entries; also, seems there's a newer version w/ more convos + better models: https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k\n",
    "lmsys = load_dataset('lmsys/chatbot_arena_conversations', split = 'train', streaming = True)\n",
    "\n",
    "# filter out: (1) samples where there is no winner - we want to extract winner's response (under assumption it's \"higher quality\" - it's a p weak, \"vibes-based\" decision. can revisit), (2) non-english samples,\n",
    "# (3) samples marked as harassing/toxic, (4) entries where the winning entry is not a \"top model\" \n",
    "\n",
    "# after this step - ~25,400 rows remain\n",
    "lmsys_filt = lmsys.filter(lambda convo: convo['winner'] != 'tie' and \n",
    "                         convo['language'] == 'English' and\n",
    "                         convo['toxic_chat_tag']['roberta-large']['flagged'] != True and \n",
    "                         convo['toxic_chat_tag']['t5-large']['flagged'] != True\n",
    "                         )\n",
    "\n",
    "# subset cols \n",
    "lmsys_sub = lmsys_filt.remove_columns(['question_id', 'judge', 'turn', 'anony', 'tstamp', 'language'])\n",
    "\n",
    "# call rows into local memory \n",
    "lm_df = pd.DataFrame(list(lmsys_sub))\n",
    "\n",
    "# add columns for winner model + answer and data source \n",
    "lm_df = lm_df.assign(\n",
    "    win_model = np.where(lm_df['winner'] == 'model_a', lm_df['model_a'], lm_df['model_b']),\n",
    "    win_answer = np.where(lm_df['winner'] == 'model_a', lm_df['conversation_a'], lm_df['conversation_b']),\n",
    "    source = 'lmsys/chatbot_arena_conversations'\n",
    "\n",
    ")\n",
    "\n",
    "# remove entries where winner is not a \"top model\" (e.g. gpt-4, claude-v1, llama-13b, or wizardlm-13b)\n",
    "lm_tops = lm_df.query(\"win_model in @lmsys_keep\") # this leaves ~5,700 rows \n",
    "\n",
    "# remove unnecessary columns \n",
    "lm = lm_tops[['win_model', 'win_answer', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8ec1a-9eed-4e74-a0ca-35593dd0138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35046078-c8b0-4470-966b-ba279f3e56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to pull examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97148253-54ab-44b3-8d78-aec09b757d5f",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Here, we import dogbert - and batch classify samples we've pulled from fineweb + chatbot arena. :> \n",
    "\n",
    "(Go dogbert! Good boy~) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355a68-3afe-45b9-a9f8-530975cf0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91223d6d-f6d5-41d7-9c98-3ff958617a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945243a4-2b2e-429d-9fc2-05492219a4a9",
   "metadata": {},
   "source": [
    "# Write to db\n",
    "Now, write to sqlite db - need to ask Charles abt planned structure (basically, what he's doing now w/ the synthetic data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
