{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1609b181-9891-4f58-854b-f90ec0f5b8d8",
   "metadata": {},
   "source": [
    "# Fineweb + LMSYS\n",
    "**Motivation**: For training, we need sources of diverse, high-quality data (especially the dog-related kind). Said data comes from a few places: \n",
    "<ol> \n",
    "    <li><span style=\"color:blue\">Fineweb:</span> HF's <a href=\"https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1\">fineweb-edu</a> offers high-quality text data by applying filters to Common Crawl.</li>\n",
    "    <li><span style=\"color:blue\">LMSYS:</span><a href=\"https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\"> Chatbot arena convos</a> are sourced for additional instruct style samples.</li> \n",
    "    <li><span style=\"color:blue\">Synthetic/LLM (out of notebook scope):</span> GPT is used to generate instruct style samples in the ChatML format.</li>\n",
    "</ol>\n",
    "\n",
    "Ultimately, goal is to retrieve ~1B tokens for training. \n",
    "\n",
    "In this notebook, the focus is on filtering + pre-processing samples from Fineweb-edu. Ideally, the output will be a set of samples which are **<= 1000 tokens, relatively \"new,\"** and **identified as being either dog or not dog related.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "62030d87-0301-4cfc-83d4-5fddc72cd3c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json \n",
    "import os\n",
    "\n",
    "# Set device (mps bcs working locally on mac)\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2a0c8288-6fb5-4d8a-afb2-f5d0c76fd8bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions \n",
    "# same as parse_phi - is used to reconstitute conversation as single chatml formatted string (need this to clean lmsys interactions)\n",
    "def parse_chat(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258a9bc-a0b7-4146-b4d7-b0f23de4e897",
   "metadata": {},
   "source": [
    "# Fineweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c684645-bc67-44f5-b726-6629ab93d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fineweb-edu\n",
    "# Goal is to simply pull the 10B sample from HF's fineweb-edu: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "# docs for working w/ streamed data from HF: https://huggingface.co/docs/datasets/v1.11.0/dataset_streaming.html\n",
    "fw = load_dataset('HuggingFaceFW/fineweb-edu', name = 'sample-10BT', split = 'train', streaming = True)\n",
    "\n",
    "# years of interest - subset out years >= 2020; in fineweb docs, it's noted that - generally - newer dumps result in better benchmark performance so this is my rationale \n",
    "yoi = ['2020', '2021', '2022', '2023', '2024']\n",
    "\n",
    "# filter fw for results <= 1000 tokens, year >= 2020, and language is 'en'\n",
    "fw_filt = fw.filter(lambda sample: any(year in sample['dump'] for year in yoi) \n",
    "                    and sample['token_count'] <= 1000\n",
    "                    and sample['language'] = 'en'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0863b308-d14d-43bd-9824-580f84b53aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# TESTING / EDA #################\n",
    "# load 100 results \n",
    "# fw_filt_df = pd.DataFrame(list(fw_filt.take(100)))\n",
    "\n",
    "# # Search for texts containing dog (or dog-adjacent keywords) - note: need to use base python w/ IterableDataset object :) (no pandas!)\n",
    "# dog_words = ['\\bdog\\b', '\\bbarking\\b', '\\bwoof\\b', 'puppy']\n",
    "# dog_search_str = re.compile('|'.join(dog_words), re.IGNORECASE)\n",
    "\n",
    "# dog_data = fw_filt.filter(lambda sample: any(year in sample['dump'] for year in yoi)\n",
    "#                                              and sample['token_count'] <= 1000\n",
    "#                                              and sample['language'] == 'en'\n",
    "#                                              and bool(re.search(dog_search_str, sample['text']))\n",
    "#                                             )\n",
    "\n",
    "# dog_data_df = pd.DataFrame(list(filtered_dataset.take(10)))\n",
    "\n",
    "# # push to csv for further observation\n",
    "# dog_data_df.to_csv('fw_dog.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6661f0a-eb29-4766-9282-a8d058111967",
   "metadata": {},
   "source": [
    "# LMSYS \n",
    "These conversations come from LMSYS' chatbot-arena. Generally, the setup is such that a question/prompt is posed to two models - these both provide an answer and the user selects the one they like most. Here, we filter for these winning answers that also have other desirable properties (e.g. not toxic, winning model is a \"top model\" (loosely defined, dataset is outdated), etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "485ba9fe-017a-430c-9ded-beeed2382d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmsys top 15 (as of 6/8). note: there are ties - hence this list has > 15 entries \n",
    "# (6/9 update: most of below were released after the dataset was added - will have to switch to 55k/wait for new chat data to use this filter)\n",
    "lmsys_tops = ['GPT-4o-2024-05-13', 'Gemini-Advanced-0514', 'Gemini-1.5-Pro-API-0514', 'Gemini-1.5-Pro-API-0409-Preview', \n",
    "              'GPT-4-Turbo-2024-04-09', 'GPT-4-1106-preview', 'Claude 3 Opus', 'GPT-4-0125-preview', 'Yi-Large-preview',\n",
    "              'Gemini-1.5-Flash-API-0514', 'Bard (Gemini Pro)', 'Llama-3-70b-Instruct', 'Llama-3-70b-Instruct', 'Llama-3-70b-Instruct',\n",
    "              'Command R+', 'Qwen2-72B-Instruct', 'GPT-4-0314', 'GLM-4-0116', 'Qwen-Max-0428']\n",
    "\n",
    "# models to keep (based on what's avail. in chatbot_arena_conversations)\n",
    "lmsys_keep = ['gpt-4', 'claude-v1', 'llama-13b', 'wizardlm-13b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "80266f55-ab04-4ea3-bf9d-9536c3382ce5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lmsys/chatbot arena - https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\n",
    "# note: it seems the oai moderation flag always registers false in these entries; also, seems there's a newer version w/ more convos + better models: https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k\n",
    "lmsys = load_dataset('lmsys/chatbot_arena_conversations', split = 'train', streaming = True)\n",
    "\n",
    "# filter out: (1) samples where there is no winner - we want to extract winner's response (under assumption it's \"higher quality\" - it's a p weak, \"vibes-based\" decision. can revisit), (2) non-english samples,\n",
    "# (3) samples marked as harassing/toxic, (4) entries where the winning entry is not a \"top model\" \n",
    "\n",
    "# after this step - ~25,400 rows remain\n",
    "lmsys_filt = lmsys.filter(lambda convo: convo['winner'] != 'tie' and \n",
    "                         convo['language'] == 'English' and\n",
    "                         convo['toxic_chat_tag']['roberta-large']['flagged'] != True and \n",
    "                         convo['toxic_chat_tag']['t5-large']['flagged'] != True\n",
    "                         )\n",
    "\n",
    "# subset cols \n",
    "lmsys_sub = lmsys_filt.remove_columns(['question_id', 'judge', 'turn', 'anony', 'tstamp', 'language'])\n",
    "\n",
    "# call rows into local memory \n",
    "lm_df = pd.DataFrame(list(lmsys_sub))\n",
    "\n",
    "# add columns for winner model + answer and data source \n",
    "lm_df = lm_df.assign(\n",
    "    win_model = np.where(lm_df['winner'] == 'model_a', lm_df['model_a'], lm_df['model_b']),\n",
    "    win_answer = np.where(lm_df['winner'] == 'model_a', lm_df['conversation_a'], lm_df['conversation_b']),\n",
    "    source = 'lmsys/chatbot_arena_conversations'\n",
    "\n",
    ")\n",
    "\n",
    "# remove entries where winner is not a \"top model\" (e.g. gpt-4, claude-v1, llama-13b, or wizardlm-13b)\n",
    "lm_tops = lm_df.query(\"win_model in @lmsys_keep\") # this leaves ~5,700 rows \n",
    "\n",
    "# select columns of interest - may want to consider more general names for future since df's will be blended across sources\n",
    "lm = lm_tops[['win_model', 'win_answer', 'source']]\n",
    "\n",
    "# parse chat so it's a single chatml string v. a list of dicts. \n",
    "lm = lm.assign(answer = [parse_chat(answer) for answer in lm['win_answer']]).drop(columns = ['win_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1411bbcc-80b8-4d1b-962d-37a607d01f15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# search for entries containing dog-related language - could also do this pre-parse_chat() via the streamed object (by comparing against values in list of dicts) \n",
    "dog_words = ['\\bdog\\b', '\\bbarking\\b', '\\bwoof\\b', 'puppy']\n",
    "dog_search_str = re.compile('|'.join(dog_words), re.IGNORECASE)\n",
    "\n",
    "# filter for answers containing matches to entries in dog_words\n",
    "dog_df = lm[lm['answer'].apply(lambda x: bool(dog_search_str.search(x)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97148253-54ab-44b3-8d78-aec09b757d5f",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Here, we import dogbert - and batch classify samples we've pulled from fineweb + chatbot arena. (go dogbert! good boy :>)\n",
    "\n",
    "As we process these in batches, we then push to a sqlite database for storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355a68-3afe-45b9-a9f8-530975cf0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dogbert\n",
    "\n",
    "\n",
    "    print(f'Now classifying {model_name[1]} posts')\n",
    "    \n",
    "    # Import model/dependencies\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels = 2).to(device)\n",
    "    \n",
    "    # Load torch model for given topic into memory\n",
    "    model.load_state_dict(torch.load(f\"../models/{model_name[1]}.pt\"))\n",
    "\n",
    "    # Posts are batched into groups of 10 posts + tokenized\n",
    "    batched_posts = []\n",
    "    for batch in chunking(to_batch, size = 10): \n",
    "        tokenized_input = tokenizer([post['content'] for post in batch], return_tensors = 'pt', max_length = 512, padding = 'max_length').to(device)\n",
    "    \n",
    "        batched_posts.append({\n",
    "                'id': [post['id'] for post in batch],\n",
    "                'content': [post['content'] for post in batch],\n",
    "                'input_ids': tokenized_input['input_ids'],\n",
    "                'attention_mask': tokenized_input['attention_mask']\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91223d6d-f6d5-41d7-9c98-3ff958617a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
