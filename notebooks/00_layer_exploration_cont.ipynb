{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8b51a9-488e-4dcc-a7f6-104ff70e2fbd",
   "metadata": {},
   "source": [
    "# Layer exploration (continued) \n",
    "We're trying to explore the layers so we're comfortable modifying things by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7254051e-ecfc-42c4-acc0-82823c86ec40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "from dotenv import main\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe6ce5d-7929-48e1-8228-cb083ca12e6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bd1563-664d-4642-8d48-a6e16c6b5bea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ed2f47-b4b2-4644-8617-9f265ac544f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac274102844d85a1e046af7975fc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load bnb config, base model, and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id[0],\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726bfcd-eda5-4183-9078-06cc9df82dae",
   "metadata": {},
   "source": [
    "# Load self-attention layer\n",
    "Goal is to load self-attn, know where corresponds to on diagram, and be able to identify inputs + outputs (along w/ dims of each).\n",
    "\n",
    "**Self-note:** remember to add with torch no grad so you don't accumulate grads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e7a99c-629c-49fe-a632-0109cadbd9f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7935e042d0b40eb8b5b79de4e17caca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate model \n",
    "base_model = reload_base_model()\n",
    "\n",
    "# Load eval dict \n",
    "eval_df = load_eval_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "782caed7-d253-4c8e-aa6a-3acf8b5aa4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token dims: torch.Size([11])\n",
      "Embedding dims: torch.Size([11, 3072])\n"
     ]
    }
   ],
   "source": [
    "sen = \"<s>My dog is a good boy who likes to\"\n",
    "\n",
    "# tokenize sentence \n",
    "dog_tok = tokenizer(sen, return_tensors = 'pt').to(device)\n",
    "print(f\"Token dims: {dog_tok['input_ids'].squeeze().shape}\")\n",
    "\n",
    "# gen. embeddings / hidden states (ref. of \"hidden states\" changes over time) \n",
    "dog_embed = base_model.model.embed_tokens(dog_tok['input_ids'])\n",
    "print(f\"Embedding dims: {dog_embed.squeeze().shape}\")\n",
    "\n",
    "#################### NOW ENTERING TRANSFORMERS ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af67b925-907c-4d1d-a0bf-bee8d67e428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get position id's again (o.w. will silently fail since model looks for dims)\n",
    "# this comes from line ~1064 in https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L243\n",
    "seq_length = dog_tok['input_ids'].shape[1]\n",
    "\n",
    "position_ids = torch.arange(0, seq_length + 0, dtype=torch.long, device = device)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "\n",
    "# basically, it's just tracking seq. length of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b6a97d8a-ff51-4d67-b7da-2e059262da43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 9216])\n",
      "1 11\n",
      "3072\n",
      "torch.Size([1, 11, 3072]) torch.Size([1, 11, 3072]) torch.Size([1, 11, 3072])\n",
      "11\n",
      "torch.Size([1, 32, 11, 96])\n",
      "torch.Size([1, 32, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "# this is a single transformers block :) - we're going to go inside of it\n",
    "one_block = base_model.model.layers[0]\n",
    "# print(one_block)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # layer_norm on hidden states \n",
    "    hidden_states = one_block.input_layernorm(dog_embed)\n",
    "    \n",
    "    # enter self_attn layer \n",
    "    # this is all of the self_attn stuff at once \n",
    "    # self_attn = one_block.self_attn(hidden_states, position_ids = position_ids)\n",
    "    # print(self_attn[0].shape)\n",
    "\n",
    "    # hidden_states_two = hidden_states + self_attn[0]\n",
    "\n",
    "    # # enter MLP \n",
    "    # print(one_block.mlp(hidden_states_two).shape)\n",
    "    # print(one_block.self_attn.head_dim, one_block.self_attn.hidden_size)\n",
    "\n",
    "    # o_proj is a linear layer that seems to prep. for future transforms; also injects more weights that can \n",
    "    # be trained / can hold meaning \n",
    "    # o_proj_output = one_block.self_attn.o_proj(dog_embed)\n",
    "    # print(o_proj_output.shape) # 11 x 3072 \n",
    "    \n",
    "    # qkv proj - these are now stacked; like a mega-tensor \n",
    "    qkv = one_block.self_attn.qkv_proj(hidden_states)\n",
    "    print(qkv.shape) \n",
    "\n",
    "    # call forward on the attn module \n",
    "    # self_attn = one_block.self_attn(hidden_states, position_ids = position_ids)\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "    print(bsz, q_len)\n",
    "\n",
    "    query_pos = one_block.self_attn.num_heads * one_block.self_attn.head_dim\n",
    "    print(query_pos)\n",
    "\n",
    "    query_states = qkv[..., :query_pos] # should be ~1/3\n",
    "    key_states = qkv[..., query_pos : query_pos + one_block.self_attn.num_key_value_heads * one_block.self_attn.head_dim]\n",
    "    value_states = qkv[..., query_pos + one_block.self_attn.num_key_value_heads * one_block.self_attn.head_dim :]\n",
    "    print(query_states.shape, key_states.shape, value_states.shape)\n",
    "\n",
    "    # re-shape each (head_dim is D/H)\n",
    "    query_states = query_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    # print(query_states.shape)\n",
    "\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    print(kv_seq_len)\n",
    "\n",
    "    # now, apply rotary embeddings (return to figure out what is going on here) \n",
    "    cos, sin = one_block.self_attn.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    print(query_states.shape)\n",
    "\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(one_block.self_attn.head_dim)\n",
    "    print(attn_weights.shape) # it is 11 x 11 - and there's 32 since there's 32 blocks\n",
    "    # mlp portion - gate up proj \n",
    "    # gate_up_proj_output = one_block.mlp.gate_up_proj(qkv_proj_output)\n",
    "    # print(gate_up_proj_output.shape)\n",
    "\n",
    "    # attention mask piece helps ensure that things only pay attention to what occurs before; ow everything \"pays attention\" to everything \n",
    "    # this is a way to force boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "54e1cd46-6c75-4fa6-9768-9a4072055cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e8370e8-23f7-406b-bf76-ce2c14100238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 8.4131e-01,  7.3486e-01,  6.2988e-01,  ...,  1.7786e-04,\n",
       "           1.4675e-04,  1.2118e-04],\n",
       "         [ 9.0918e-01,  9.9658e-01,  9.7852e-01,  ...,  3.5572e-04,\n",
       "           2.9349e-04,  2.4235e-04],\n",
       "         ...,\n",
       "         [ 9.8926e-01,  3.1470e-01, -7.3975e-01,  ...,  1.4229e-03,\n",
       "           1.1740e-03,  9.6941e-04],\n",
       "         [ 4.1211e-01,  9.1113e-01, -1.5100e-01,  ...,  1.6003e-03,\n",
       "           1.3208e-03,  1.0900e-03],\n",
       "         [-5.4395e-01,  9.2090e-01,  5.0537e-01,  ...,  1.7786e-03,\n",
       "           1.4677e-03,  1.2112e-03]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cba01e3-9952-47c7-9245-d27f808b4f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model.embed_tokens.weight</td>\n",
       "      <td>(32064, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>model.layers.0.self_attn.o_proj.weight</td>\n",
       "      <td>(4718592, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>model.layers.0.self_attn.qkv_proj.weight</td>\n",
       "      <td>(14155776, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>model.layers.0.mlp.gate_up_proj.weight</td>\n",
       "      <td>(25165824, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>model.layers.0.mlp.down_proj.weight</td>\n",
       "      <td>(12582912, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>model.layers.31.mlp.down_proj.weight</td>\n",
       "      <td>(12582912, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>model.layers.31.input_layernorm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>model.layers.31.post_attention_layernorm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>model.norm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>lm_head.weight</td>\n",
       "      <td>(32064, 3072)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                             name           dims\n",
       "0      0                        model.embed_tokens.weight  (32064, 3072)\n",
       "1      1           model.layers.0.self_attn.o_proj.weight   (4718592, 1)\n",
       "2      2         model.layers.0.self_attn.qkv_proj.weight  (14155776, 1)\n",
       "3      3           model.layers.0.mlp.gate_up_proj.weight  (25165824, 1)\n",
       "4      4              model.layers.0.mlp.down_proj.weight  (12582912, 1)\n",
       "..   ...                                              ...            ...\n",
       "190  190             model.layers.31.mlp.down_proj.weight  (12582912, 1)\n",
       "191  191           model.layers.31.input_layernorm.weight        (3072,)\n",
       "192  192  model.layers.31.post_attention_layernorm.weight        (3072,)\n",
       "193  193                                model.norm.weight        (3072,)\n",
       "194  194                                   lm_head.weight  (32064, 3072)\n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# track layer defs \n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "\n",
    "    # store layer names (for testing) \n",
    "    layer_names.append({'idx': idx, 'name': name, 'dims': param.shape})\n",
    "\n",
    "# view layers \n",
    "pd.DataFrame(layer_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
