{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8b51a9-488e-4dcc-a7f6-104ff70e2fbd",
   "metadata": {},
   "source": [
    "# Layer exploration (continued) \n",
    "We're trying to explore the layers so we're comfortable modifying things by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf3fb5-57c4-432e-b14c-418782915fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on 1 x RTX A6000\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install plotly.express\n",
    "!pip install scikit-learn\n",
    "!pip install -U flash-attn --no-build-isolation\n",
    "!pip install pyyaml\n",
    "!pip install pyarrow\n",
    "!pip install termcolor\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv\n",
    "# If distutils error, https://stackoverflow.com/a/78050586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7254051e-ecfc-42c4-acc0-82823c86ec40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "# from dotenv import main\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe6ce5d-7929-48e1-8228-cb083ca12e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bd1563-664d-4642-8d48-a6e16c6b5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    # quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed2f47-b4b2-4644-8617-9f265ac544f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Load bnb config, base model, and tokenizer\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = 'nf4',\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id[0],\n",
    "#     device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "#     quantization_config = bnb_config,\n",
    "#     trust_remote_code = True\n",
    "# )\n",
    "\n",
    "# # Load tokenizer - remove bos token since my function already pre-pends\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "#                                          add_eos_token = False,\n",
    "#                                          add_bos_token = False,\n",
    "#                                          padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726bfcd-eda5-4183-9078-06cc9df82dae",
   "metadata": {},
   "source": [
    "# Load self-attention layer\n",
    "Goal is to load self-attn, know where corresponds to on diagram, and be able to identify inputs + outputs (along w/ dims of each).\n",
    "\n",
    "**Self-note:** remember to add with torch no grad so you don't accumulate grads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e7a99c-629c-49fe-a632-0109cadbd9f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fd6f0035524183b0313940eb5653b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate model \n",
    "base_model = reload_base_model()\n",
    "\n",
    "# Load eval dict \n",
    "eval_df = load_eval_df()\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782caed7-d253-4c8e-aa6a-3acf8b5aa4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sen = \"<s>My dog is a good boy who likes to\"\n",
    "\n",
    "# tokenize sentence \n",
    "dog_tok = tokenizer(sen, return_tensors = 'pt').to(device)\n",
    "print(f\"Token dims: {dog_tok['input_ids'].squeeze().shape}\")\n",
    "\n",
    "# gen. embeddings / hidden states (ref. of \"hidden states\" changes over time) \n",
    "dog_embed = base_model.model.embed_tokens(dog_tok['input_ids'])\n",
    "print(f\"Embedding dims: {dog_embed.squeeze().shape}\")\n",
    "\n",
    "#################### NOW ENTERING TRANSFORMERS ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67b925-907c-4d1d-a0bf-bee8d67e428f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get position id's again (o.w. will silently fail since model looks for dims)\n",
    "# this comes from line ~1064 in https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L243\n",
    "seq_length = dog_tok['input_ids'].shape[1]\n",
    "\n",
    "position_ids = torch.arange(0, seq_length + 0, dtype=torch.long, device = device)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "\n",
    "# basically, it's just tracking seq. length of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a97d8a-ff51-4d67-b7da-2e059262da43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is a single transformers block :) - we're going to go inside of it\n",
    "one_block = base_model.model.layers[0]\n",
    "# print(one_block)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # layer_norm on hidden states \n",
    "    hidden_states = one_block.input_layernorm(dog_embed)\n",
    "    \n",
    "    # enter self_attn layer \n",
    "    # this is all of the self_attn stuff at once \n",
    "    # self_attn = one_block.self_attn(hidden_states, position_ids = position_ids)\n",
    "    # print(self_attn[0].shape)\n",
    "\n",
    "    # hidden_states_two = hidden_states + self_attn[0]\n",
    "\n",
    "    # # enter MLP \n",
    "    # print(one_block.mlp(hidden_states_two).shape)\n",
    "    # print(one_block.self_attn.head_dim, one_block.self_attn.hidden_size)\n",
    "\n",
    "    # o_proj is a linear layer that seems to prep. for future transforms; also injects more weights that can \n",
    "    # be trained / can hold meaning \n",
    "    # o_proj_output = one_block.self_attn.o_proj(dog_embed)\n",
    "    # print(o_proj_output.shape) # 11 x 3072 \n",
    "    \n",
    "    # qkv proj - these are now stacked; like a mega-tensor \n",
    "    qkv = one_block.self_attn.qkv_proj(hidden_states)\n",
    "    print(qkv.shape) \n",
    "\n",
    "    # call forward on the attn module \n",
    "    # self_attn = one_block.self_attn(hidden_states, position_ids = position_ids)\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "    print(bsz, q_len)\n",
    "\n",
    "    query_pos = one_block.self_attn.num_heads * one_block.self_attn.head_dim\n",
    "    print(query_pos)\n",
    "\n",
    "    query_states = qkv[..., :query_pos] # should be ~1/3\n",
    "    key_states = qkv[..., query_pos : query_pos + one_block.self_attn.num_key_value_heads * one_block.self_attn.head_dim]\n",
    "    value_states = qkv[..., query_pos + one_block.self_attn.num_key_value_heads * one_block.self_attn.head_dim :]\n",
    "    print(query_states.shape, key_states.shape, value_states.shape)\n",
    "\n",
    "    # re-shape each (head_dim is D/H)\n",
    "    query_states = query_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    # print(query_states.shape)\n",
    "\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    print(kv_seq_len)\n",
    "\n",
    "    # now, apply rotary embeddings (return to figure out what is going on here) \n",
    "    cos, sin = one_block.self_attn.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    print(query_states.shape)\n",
    "\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(one_block.self_attn.head_dim)\n",
    "    print(attn_weights.shape) # it is 11 x 11 - and there's 32 since there's 32 blocks\n",
    "    # mlp portion - gate up proj \n",
    "    # gate_up_proj_output = one_block.mlp.gate_up_proj(qkv_proj_output)\n",
    "    # print(gate_up_proj_output.shape)\n",
    "\n",
    "    # attention mask piece helps ensure that things only pay attention to what occurs before; ow everything \"pays attention\" to everything \n",
    "    # this is a way to force boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "996edaa7-575b-41ac-a5d7-a9ca4a3b4985",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3DecoderLayer(\n",
       "  (self_attn): Phi3Attention(\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Phi3MLP(\n",
       "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (activation_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): Phi3RMSNorm()\n",
       "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_attention_layernorm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed695822-84f9-40b9-a35e-3c56a4b98430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0448,  0.1045,  0.0509,  ...,  0.0457, -0.0130, -0.0439],\n",
      "         [ 0.0190,  0.0146,  0.0208,  ...,  0.0138, -0.0274,  0.0017],\n",
      "         [ 0.0145,  0.0256,  0.0012,  ...,  0.0170, -0.0121, -0.0658],\n",
      "         ...,\n",
      "         [-0.0229, -0.0233, -0.0096,  ..., -0.0166,  0.0090,  0.0271],\n",
      "         [ 0.0370,  0.0240, -0.0191,  ..., -0.0205,  0.0200, -0.0249],\n",
      "         [-0.0550, -0.0360, -0.0253,  ..., -0.0052, -0.0096,  0.0105]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Testing for transformers block\n",
    "with torch.no_grad(): \n",
    "    prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "    base_model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "\n",
    "    N = input_ids.shape[1]\n",
    "\n",
    "    # get embeddings\n",
    "    embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "\n",
    "    # NOW ENTERING TRANSFORMERS LAYERS \n",
    "    decoder_layer = base_model.model.layers[0]\n",
    "    # layer norm on hidden states - line 853 (https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L810)\n",
    "\n",
    "    # store residuals \n",
    "    residual = hidden_state # line 851\n",
    "    hidden_states = decoder_layer.input_layernorm(hidden_state) \n",
    "\n",
    "    # now, self attn - line 856\n",
    "    attn_outputs, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "        hidden_states = hidden_states,\n",
    "        attention_mask = attention_mask,\n",
    "        position_ids = position_ids,\n",
    "        output_attentions = True # this is the one that helps pop. self_attn_weights and present_key_value :)) those are related to caching!\n",
    "        # past_key_value = ## don't have - optional, cached \n",
    "        # output_attentions = ## don't have - line 842; whether to return attention tensors of all attention layers \n",
    "        # use_cache = use_cache ### don't have - optional, related to caching \n",
    "    )\n",
    "\n",
    "    # line 865 \n",
    "    hidden_states = residual + decoder_layer.resid_attn_dropout(attn_outputs)\n",
    "\n",
    "    print(hidden_states)\n",
    "    \n",
    "    \n",
    "    # self_attn \n",
    "    # base_model.model.layers[0].self_attn(\n",
    "\n",
    "    # input_layernorm\n",
    "    # resid_attn_dropout\n",
    "    # resid_mlp_dropout\n",
    "    # post_attention_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e714e7-72f1-45d7-bc4b-c75192292be9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.5fa34190089f0ee40f9cce3cafc396b89b2e5e83.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model + manual generation <s> I am a dog and I like to eat meat! My favorite food is chicken. I also like to play fetch with my owner. I am a very friendly dog and I love to cuddle with my owner. I am a golden retriever and I am 5 years old. I am very smart and I can learn many tricks. I am a good boy and I always obey my owner. I am a happy dog and I enjoy life.\n",
      "<|assistant|> That's a lovely introduction to yourself, a golden retriever! Golden retrievers are known for their friendly and loyal nature, as well as their intelligence and ability to learn. It's wonderful to\n"
     ]
    }
   ],
   "source": [
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_multiple_outputs(model, tokenizer, prompt = '<s>I am a dog and I like to eat meat! My favorite', max_tokens = 128, device = 'cuda'):\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "\n",
    "    while True:\n",
    "        N = input_ids.shape[1]\n",
    "\n",
    "        # Get embeddings\n",
    "        embeds_output = model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "        \n",
    "        # Get some parameters needed for transformers layers\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        # Execute transformers layers\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            # hidden_state = layer(hidden_state, position_ids = position_ids, attention_mask = attention_mask)[0]\n",
    "\n",
    "            #### enumerate the entire transformers block - start w/ self_attn\n",
    "            # self_attn\n",
    "            \n",
    "            \n",
    "            # mlp \n",
    "\n",
    "            # layernorm + dropout\n",
    "            ###################################\n",
    "    \n",
    "        # RMS norm the final transformer layer output - this is after all 32 transformer blocsk\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    return final_output\n",
    "\n",
    "# Test\n",
    "test_prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "# Use function\n",
    "print('my_model + manual generation', generate_multiple_outputs(base_model, tokenizer, prompt = test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1cd46-6c75-4fa6-9768-9a4072055cef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cba01e3-9952-47c7-9245-d27f808b4f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model.embed_tokens.weight</td>\n",
       "      <td>(32064, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>model.layers.0.self_attn.o_proj.weight</td>\n",
       "      <td>(3072, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>model.layers.0.self_attn.qkv_proj.weight</td>\n",
       "      <td>(9216, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>model.layers.0.mlp.gate_up_proj.weight</td>\n",
       "      <td>(16384, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>model.layers.0.mlp.down_proj.weight</td>\n",
       "      <td>(3072, 8192)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>model.layers.31.mlp.down_proj.weight</td>\n",
       "      <td>(3072, 8192)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>model.layers.31.input_layernorm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>model.layers.31.post_attention_layernorm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>model.norm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>lm_head.weight</td>\n",
       "      <td>(32064, 3072)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                             name           dims\n",
       "0      0                        model.embed_tokens.weight  (32064, 3072)\n",
       "1      1           model.layers.0.self_attn.o_proj.weight   (3072, 3072)\n",
       "2      2         model.layers.0.self_attn.qkv_proj.weight   (9216, 3072)\n",
       "3      3           model.layers.0.mlp.gate_up_proj.weight  (16384, 3072)\n",
       "4      4              model.layers.0.mlp.down_proj.weight   (3072, 8192)\n",
       "..   ...                                              ...            ...\n",
       "190  190             model.layers.31.mlp.down_proj.weight   (3072, 8192)\n",
       "191  191           model.layers.31.input_layernorm.weight        (3072,)\n",
       "192  192  model.layers.31.post_attention_layernorm.weight        (3072,)\n",
       "193  193                                model.norm.weight        (3072,)\n",
       "194  194                                   lm_head.weight  (32064, 3072)\n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# track layer defs \n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "\n",
    "    # store layer names (for testing) \n",
    "    layer_names.append({'idx': idx, 'name': name, 'dims': param.shape})\n",
    "\n",
    "# view layers \n",
    "pd.DataFrame(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefdff9-897c-4253-87dd-96c7ebf748a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
