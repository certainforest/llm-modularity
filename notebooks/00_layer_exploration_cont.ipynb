{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8b51a9-488e-4dcc-a7f6-104ff70e2fbd",
   "metadata": {},
   "source": [
    "# Layer exploration (continued) \n",
    "We're trying to explore the layers so we're comfortable modifying things by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf3fb5-57c4-432e-b14c-418782915fbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run on 1 x RTX A6000\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install plotly.express\n",
    "!pip install scikit-learn\n",
    "!pip install -U flash-attn --no-build-isolation\n",
    "!pip install pyyaml\n",
    "!pip install pyarrow\n",
    "!pip install termcolor\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv\n",
    "# If distutils error, https://stackoverflow.com/a/78050586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7254051e-ecfc-42c4-acc0-82823c86ec40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "# from dotenv import main\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe6ce5d-7929-48e1-8228-cb083ca12e6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bd1563-664d-4642-8d48-a6e16c6b5bea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    # quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ed2f47-b4b2-4644-8617-9f265ac544f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Load bnb config, base model, and tokenizer\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = 'nf4',\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id[0],\n",
    "#     device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "#     quantization_config = bnb_config,\n",
    "#     trust_remote_code = True\n",
    "# )\n",
    "\n",
    "# # Load tokenizer - remove bos token since my function already pre-pends\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "#                                          add_eos_token = False,\n",
    "#                                          add_bos_token = False,\n",
    "#                                          padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726bfcd-eda5-4183-9078-06cc9df82dae",
   "metadata": {},
   "source": [
    "# Breaking apart phi-3 (+ checking if outputs flow through analogously) \n",
    "Recreating phi-3 layer by layer (took out self_attn repro code for now, but can recover via git history) + trying to break it down to most granular level possible in order to track + modify outputs :). Checking to ensure everything is analogous by doing a forward pass \n",
    "with the phi-3 model (not broken apart) as a baseline + tracking outputs w/ hooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e7a99c-629c-49fe-a632-0109cadbd9f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7528165d8e49fca016c359c970737a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate model \n",
    "base_model = reload_base_model()\n",
    "\n",
    "# Load eval dict \n",
    "eval_df = load_eval_df()\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed695822-84f9-40b9-a35e-3c56a4b98430",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_tokens = 128\n",
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "# Testing for transformers block\n",
    "with torch.no_grad(): \n",
    "    prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "    base_model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "    input_ids_1 = input_ids\n",
    "\n",
    "    output_dict = {\n",
    "        \n",
    "\n",
    "    }\n",
    "    while True: \n",
    "        N = input_ids.shape[1]\n",
    "    \n",
    "        # get embeddings\n",
    "        embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "    \n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        ##### TRANSFORMER BLOCK #####\n",
    "        \n",
    "        for (idx, layer) in enumerate(base_model.model.layers[0:1]): \n",
    "            decoder_layer = base_model.model.layers[idx] \n",
    "            \n",
    "            # store residuals \n",
    "            residual = hidden_state # line 851\n",
    "            hidden_states = decoder_layer.input_layernorm(hidden_state) # layer norm on hidden states - line 853 (https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L810)\n",
    "        \n",
    "            # now, self attn - line 856\n",
    "            attn_outputs, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "                hidden_states = hidden_states,\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                output_attentions = True # this is the one that helps pop. self_attn_weights and present_key_value :)) Those are related to caching!\n",
    "                # past_key_value = ## don't have - optional, cached \n",
    "                # output_attentions = ## don't have - line 842; whether to return attention tensors of all attention layers \n",
    "                # use_cache = use_cache ### don't have - optional, related to caching \n",
    "            )\n",
    "        \n",
    "            # line 865 \n",
    "            hidden_states = residual + decoder_layer.resid_attn_dropout(attn_outputs)\n",
    "        \n",
    "            residual = hidden_states # line 867\n",
    "            hidden_states = decoder_layer.post_attention_layernorm(hidden_states) # line 868\n",
    "        \n",
    "            hidden_states = decoder_layer.mlp(hidden_states)\n",
    "            hidden_states = residual + decoder_layer.resid_mlp_dropout(hidden_states)\n",
    "        \n",
    "            outputs = (hidden_states,) \n",
    "        \n",
    "            # these map back to those booleans arguments defined within forward from earlier :) \n",
    "            # if output_attentions:\n",
    "            #         outputs += (self_attn_weights,)\n",
    "        \n",
    "            # if use_cache:\n",
    "            #         outputs += (present_key_value,)\n",
    "        \n",
    "            hidden_state = base_model.model.norm(hidden_states) # hm, this seems to be correct - it was just called outputs when charles defined it as layer outputs :) \n",
    "    \n",
    "        # run LM head \n",
    "        logits = base_model.lm_head(hidden_state) # remember you need to use the version w/ causal LM \n",
    "    \n",
    "        # get argmax tokens + concatenate onto previous tokens \n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "    \n",
    "        # Break while loop if EOS or generation > max tokens \n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "           break\n",
    "    \n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    \n",
    "    print(final_output)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11370a5-938b-4ffb-9f71-85dfde19847c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# these are re-used across both of below chunks\n",
    "prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33f3c354-6ba3-4feb-b41d-e65f48ff580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputs(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "layer_outputs = {} \n",
    "\n",
    "# add hooks \n",
    "h1 = base_model.model.embed_tokens.register_forward_hook(getOutputs('embed')) # embed layer \n",
    "h2 = base_model.model.layers[0].register_forward_hook(getOutputs('trans_one')) # first transformers block\n",
    "h3 = base_model.model.layers[0].input_layernorm.register_forward_hook(getOutputs('sa_layer_norm')) # this is the layernorm that happens to hidden states before sa\n",
    "h4 = base_model.model.layers[0].self_attn.register_forward_hook(getOutputs('self_attn')) # note, this self attn. piece is a sub-component of the above\n",
    "h5 = base_model.model.layers[0].resid_attn_dropout.register_forward_hook(getOutputs('resid_attn_dropout')) # this dropout happens after sa\n",
    "h6 = base_model.model.layers[0].mlp.register_forward_hook(getOutputs('mlp')) # mlp \n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():   \n",
    "    base_model(input_ids)\n",
    "\n",
    "# remove hooks - should rewrite as loop later\n",
    "hooks = [h1, h2, h3, h4, h5, h6]\n",
    "for hook in hooks: \n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14d4dd14-7893-4837-aba6-9f5e3328e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "\n",
    "# Testing for transformers block\n",
    "with torch.no_grad():\n",
    "    \n",
    "    embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "    N = input_ids.shape[1]\n",
    "    \n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "\n",
    "    # print(attention_mask, torch.where(attention_mask != 0, torch.tensor(1), attention_mask)) # this makes it easier to see the diagonal\n",
    "\n",
    "    ##### TRANSFORMER BLOCK #####\n",
    "\n",
    "    transformer_block = base_model.model.layers[0]\n",
    "            \n",
    "    residual = hidden_state \n",
    "    hidden_states_one = transformer_block.input_layernorm(hidden_state)\n",
    "    \n",
    "    # self attn \n",
    "    attn_outputs = transformer_block.self_attn(\n",
    "        hidden_states = hidden_states_one,\n",
    "        attention_mask = attention_mask,\n",
    "        position_ids = position_ids\n",
    "    )\n",
    "\n",
    "    hidden_states_two = residual + attn_outputs[0]\n",
    "\n",
    "    residual = hidden_states_two # line 867\n",
    "    hidden_states_three = transformer_block.post_attention_layernorm(hidden_states_two) # line 868\n",
    "\n",
    "    mlp = transformer_block.mlp(hidden_states_three)\n",
    "    hidden_states_four = residual + mlp # dropout doesn't do anything right now\n",
    "\n",
    "    outputs = hidden_states_four\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65dd4751-531a-4b5d-9dcf-5ba4821b4376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if outputs are flowing appropriately through my repro. \n",
    "# torch.equal(hidden_states_one, layer_outputs['sa_layer_norm']) # nice - this is the first layernorm on hidden states \n",
    "# torch.equal(attn_outputs[0], layer_outputs['self_attn'][0]) # nice - this is on self attn \n",
    "# torch.equal(resid_attn_dropout, layer_outputs['resid_attn_dropout']) # nice - this is sorta analog. to line 865 in phi-3 docs; not sure if directly comp. otherwise\n",
    "# torch.equal(mlp, layer_outputs['mlp']) # nice - this is the mlp piece \n",
    "\n",
    "# check if tracks w/ block \n",
    "torch.equal("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "381925d5-350b-4295-af0b-130412ddaa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0636,  0.1626,  0.0082,  ...,  0.1023, -0.0550, -0.0476],\n",
       "         [ 0.0550,  0.0246,  0.0246,  ...,  0.0624, -0.0468, -0.0152],\n",
       "         [ 0.0213,  0.0441, -0.0356,  ...,  0.0537,  0.0057, -0.1038],\n",
       "         ...,\n",
       "         [-0.0206, -0.0517, -0.0347,  ...,  0.0051, -0.0113,  0.0405],\n",
       "         [ 0.0935,  0.0172, -0.0123,  ...,  0.0196,  0.0150, -0.0505],\n",
       "         [-0.0741, -0.0551, -0.0603,  ...,  0.0099,  0.0236, -0.0074]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
