{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8b51a9-488e-4dcc-a7f6-104ff70e2fbd",
   "metadata": {},
   "source": [
    "# Layer exploration (continued) \n",
    "We're trying to explore the layers so we're comfortable modifying things by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf3fb5-57c4-432e-b14c-418782915fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on 1 x RTX A6000\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install plotly.express\n",
    "!pip install scikit-learn\n",
    "!pip install -U flash-attn --no-build-isolation\n",
    "!pip install pyyaml\n",
    "!pip install pyarrow\n",
    "!pip install termcolor\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv\n",
    "# If distutils error, https://stackoverflow.com/a/78050586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7254051e-ecfc-42c4-acc0-82823c86ec40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "# from dotenv import main\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe6ce5d-7929-48e1-8228-cb083ca12e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bd1563-664d-4642-8d48-a6e16c6b5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    # quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ed2f47-b4b2-4644-8617-9f265ac544f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Load bnb config, base model, and tokenizer\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = 'nf4',\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id[0],\n",
    "#     device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "#     quantization_config = bnb_config,\n",
    "#     trust_remote_code = True\n",
    "# )\n",
    "\n",
    "# # Load tokenizer - remove bos token since my function already pre-pends\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "#                                          add_eos_token = False,\n",
    "#                                          add_bos_token = False,\n",
    "#                                          padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726bfcd-eda5-4183-9078-06cc9df82dae",
   "metadata": {},
   "source": [
    "# Load self-attention layer\n",
    "Goal is to load self-attn, know where corresponds to on diagram, and be able to identify inputs + outputs (along w/ dims of each).\n",
    "\n",
    "**Self-note:** remember to add with torch no grad so you don't accumulate grads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e7a99c-629c-49fe-a632-0109cadbd9f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a011d10543504454b98a2e06a4f9fce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate model \n",
    "base_model = reload_base_model()\n",
    "\n",
    "# Load eval dict \n",
    "eval_df = load_eval_df()\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782caed7-d253-4c8e-aa6a-3acf8b5aa4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token dims: torch.Size([11])\n",
      "Embedding dims: torch.Size([11, 3072])\n"
     ]
    }
   ],
   "source": [
    "sen = \"<s>My dog is a good boy who likes to\"\n",
    "\n",
    "# tokenize sentence \n",
    "dog_tok = tokenizer(sen, return_tensors = 'pt').to(device)\n",
    "print(f\"Token dims: {dog_tok['input_ids'].squeeze().shape}\")\n",
    "\n",
    "# gen. embeddings / hidden states (ref. of \"hidden states\" changes over time) \n",
    "dog_embed = base_model.model.embed_tokens(dog_tok['input_ids'])\n",
    "print(f\"Embedding dims: {dog_embed.squeeze().shape}\")\n",
    "\n",
    "#################### NOW ENTERING TRANSFORMERS ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af67b925-907c-4d1d-a0bf-bee8d67e428f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get position id's again (o.w. will silently fail since model looks for dims)\n",
    "# this comes from line ~1064 in https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L243\n",
    "seq_length = dog_tok['input_ids'].shape[1]\n",
    "\n",
    "position_ids = torch.arange(0, seq_length + 0, dtype=torch.long, device = device)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "\n",
    "# basically, it's just tracking seq. length of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a97d8a-ff51-4d67-b7da-2e059262da43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 9216])\n",
      "1 11\n",
      "3072\n",
      "torch.Size([1, 11, 3072]) torch.Size([1, 11, 3072]) torch.Size([1, 11, 3072])\n",
      "11\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'apply_rotary_pos_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# now, apply rotary embeddings (return to figure out what is going on here) \u001b[39;00m\n\u001b[1;32m     52\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m one_block\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, position_ids, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m---> 53\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_states\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     56\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(one_block\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'apply_rotary_pos_emb' is not defined"
     ]
    }
   ],
   "source": [
    "# this is a single transformers block :) - we're going to go inside of it\n",
    "one_block = base_model.model.layers[0]\n",
    "# print(one_block)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # layer_norm on hidden states \n",
    "    hidden_states = one_block.input_layernorm(dog_embed)\n",
    "    \n",
    "    # enter self_attn layer \n",
    "    # this is all of the self_attn stuff at once \n",
    "    # self_attn = one_block.self_attn(hidden_states, position_ids = position_ids)\n",
    "    # print(self_attn[0].shape)\n",
    "\n",
    "    # hidden_states_two = hidden_states + self_attn[0]\n",
    "\n",
    "    # # enter MLP \n",
    "    # print(one_block.mlp(hidden_states_two).shape)\n",
    "    # print(one_block.self_attn.head_dim, one_block.self_attn.hidden_size)\n",
    "\n",
    "    # o_proj is a linear layer that seems to prep. for future transforms; also injects more weights that can \n",
    "    # be trained / can hold meaning \n",
    "    # o_proj_output = one_block.self_attn.o_proj(dog_embed)\n",
    "    # print(o_proj_output.shape) # 11 x 3072 \n",
    "    \n",
    "    # qkv proj - these are now stacked; like a mega-tensor \n",
    "    qkv = one_block.self_attn.qkv_proj(hidden_states)\n",
    "    print(qkv.shape) \n",
    "\n",
    "    # call forward on the attn module \n",
    "    # self_attn = one_block.self_attn(hidden_states, position_ids = position_ids)\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "    print(bsz, q_len)\n",
    "\n",
    "    query_pos = one_block.self_attn.num_heads * one_block.self_attn.head_dim\n",
    "    print(query_pos)\n",
    "\n",
    "    query_states = qkv[..., :query_pos] # should be ~1/3\n",
    "    key_states = qkv[..., query_pos : query_pos + one_block.self_attn.num_key_value_heads * one_block.self_attn.head_dim]\n",
    "    value_states = qkv[..., query_pos + one_block.self_attn.num_key_value_heads * one_block.self_attn.head_dim :]\n",
    "    print(query_states.shape, key_states.shape, value_states.shape)\n",
    "\n",
    "    # re-shape each (head_dim is D/H)\n",
    "    query_states = query_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, one_block.self_attn.num_heads, one_block.self_attn.head_dim).transpose(1, 2)\n",
    "    # print(query_states.shape)\n",
    "\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    print(kv_seq_len)\n",
    "\n",
    "    # now, apply rotary embeddings (return to figure out what is going on here) \n",
    "    cos, sin = one_block.self_attn.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    print(query_states.shape)\n",
    "\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(one_block.self_attn.head_dim)\n",
    "    print(attn_weights.shape) # it is 11 x 11 - and there's 32 since there's 32 blocks\n",
    "    # mlp portion - gate up proj \n",
    "    # gate_up_proj_output = one_block.mlp.gate_up_proj(qkv_proj_output)\n",
    "    # print(gate_up_proj_output.shape)\n",
    "\n",
    "    # attention mask piece helps ensure that things only pay attention to what occurs before; ow everything \"pays attention\" to everything \n",
    "    # this is a way to force boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa1045b9-5e92-463c-9ed4-029b9b659904",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "2\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "3\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "4\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "5\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "6\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "7\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "8\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "9\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "10\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "11\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "12\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "13\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "14\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "15\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "16\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "17\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "18\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "19\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "20\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "21\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "22\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "23\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "24\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "25\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "26\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "27\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "28\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "29\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "30\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "31\n",
      "Phi3DecoderLayer(\n",
      "  (self_attn): Phi3Attention(\n",
      "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Phi3MLP(\n",
      "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "    (activation_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Phi3RMSNorm()\n",
      "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_attention_layernorm): Phi3RMSNorm()\n",
      ")\n",
      "32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 32 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m33\u001b[39m): \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     decoder_layer \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(decoder_layer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 32 is out of range"
     ]
    }
   ],
   "source": [
    "for i in range(1, 33): \n",
    "    print(i)\n",
    "    decoder_layer = base_model.model.layers[i]\n",
    "    print(decoder_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed695822-84f9-40b9-a35e-3c56a4b98430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I am a dog and I like to eat meat! My favoriteyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes S S S S S S S S Syesyesyesyesyesalonealonealonealonealonealonealone S S S S S S S S S S S S S S S S S S S S S S S S S Salonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealonealone\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 128\n",
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "# Testing for transformers block\n",
    "with torch.no_grad(): \n",
    "    prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "    base_model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "\n",
    "    while True: \n",
    "        N = input_ids.shape[1]\n",
    "    \n",
    "        # get embeddings\n",
    "        embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "    \n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        # NOW ENTERING TRANSFORMERS LAYERS - there's 32 of them! \n",
    "        for layer in base_model.layers: \n",
    "            decoder_layer = base_model.model.layers[layer] # iterate across\n",
    "            # layer norm on hidden states - line 853 (https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L810)\n",
    "        \n",
    "            # store residuals \n",
    "            residual = hidden_state # line 851\n",
    "            hidden_states = decoder_layer.input_layernorm(hidden_state) \n",
    "        \n",
    "            # now, self attn - line 856\n",
    "            attn_outputs, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "                hidden_states = hidden_states,\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                output_attentions = True # this is the one that helps pop. self_attn_weights and present_key_value :)) those are related to caching!\n",
    "                # past_key_value = ## don't have - optional, cached \n",
    "                # output_attentions = ## don't have - line 842; whether to return attention tensors of all attention layers \n",
    "                # use_cache = use_cache ### don't have - optional, related to caching \n",
    "            )\n",
    "        \n",
    "            # line 865 \n",
    "            hidden_states = residual + decoder_layer.resid_attn_dropout(attn_outputs)\n",
    "        \n",
    "            residual = hidden_states # line 867\n",
    "            hidden_states = decoder_layer.post_attention_layernorm(hidden_states) # line 868\n",
    "        \n",
    "            hidden_states = decoder_layer.mlp(hidden_states)\n",
    "            hidden_states = residual + decoder_layer.resid_mlp_dropout(hidden_states)\n",
    "        \n",
    "            outputs = (hidden_states,) \n",
    "        \n",
    "            # these map back to those booleans arguments defined within forward from earlier :) \n",
    "            # if output_attentions:\n",
    "            #         outputs += (self_attn_weights,)\n",
    "        \n",
    "            # if use_cache:\n",
    "            #         outputs += (present_key_value,)\n",
    "        \n",
    "            hidden_state = base_model.model.norm(hidden_states) # hm, this seems to be correct - it was just called outputs when charles defined it as layer outputs :) \n",
    "    \n",
    "        # run LM head \n",
    "        logits = base_model.lm_head(hidden_state) # remember you need to use the version w/ causal LM \n",
    "    \n",
    "        # get argmax tokens + concatenate onto previous tokens \n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "    \n",
    "        # Break while loop if EOS or generation > max tokens \n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "           break\n",
    "    \n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    \n",
    "    print(final_output)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996edaa7-575b-41ac-a5d7-a9ca4a3b4985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # RMS norm the final transformer layer output - this is after all 32 transformer blocsk\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    return final_output\n",
    "\n",
    "# Test\n",
    "test_prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "# Use function\n",
    "print('my_model + manual generation', generate_multiple_outputs(base_model, tokenizer, prompt = test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e714e7-72f1-45d7-bc4b-c75192292be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_multiple_outputs(model, tokenizer, prompt = '<s>I am a dog and I like to eat meat! My favorite', max_tokens = 128, device = 'cuda'):\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "\n",
    "    while True:\n",
    "        N = input_ids.shape[1]\n",
    "\n",
    "        # Get embeddings\n",
    "        embeds_output = model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "        \n",
    "        # Get some parameters needed for transformers layers\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        # Execute transformers layers\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            # hidden_state = layer(hidden_state, position_ids = position_ids, attention_mask = attention_mask)[0]\n",
    "\n",
    "            #### enumerate the entire transformers block - start w/ self_attn\n",
    "            # self_attn\n",
    "            \n",
    "            \n",
    "            # mlp \n",
    "\n",
    "            # layernorm + dropout\n",
    "            ###################################\n",
    "    \n",
    "        # RMS norm the final transformer layer output - this is after all 32 transformer blocsk\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    return final_output\n",
    "\n",
    "# Test\n",
    "test_prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "# Use function\n",
    "print('my_model + manual generation', generate_multiple_outputs(base_model, tokenizer, prompt = test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1cd46-6c75-4fa6-9768-9a4072055cef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba01e3-9952-47c7-9245-d27f808b4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track layer defs \n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "\n",
    "    # store layer names (for testing) \n",
    "    layer_names.append({'idx': idx, 'name': name, 'dims': param.shape})\n",
    "\n",
    "# view layers \n",
    "pd.DataFrame(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefdff9-897c-4253-87dd-96c7ebf748a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
