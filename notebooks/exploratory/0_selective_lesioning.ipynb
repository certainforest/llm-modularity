{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b92319-88b3-4c7d-91fa-aa605c617cab",
   "metadata": {},
   "source": [
    "# Selective lesioning\n",
    "Now, our aim is to erode a model in a **controlled manner**. Michael Levin's \"multiple levels of competency\" (planaria; evaluate biology by how well it reacts to errors) is a main inspiration. How can higher level layers of \"agents\" resolve errors from lower levels? Why are these systems (biological agents + llm's) so robust? How robust is our model to perturbances? \n",
    "\n",
    "Model performance is tracked by benchmarking over a question set (generated w/ an assist from gpt-4 :)). \n",
    "\n",
    "To do: \n",
    "+ Load base model + replace mps code x\n",
    "+ Pull in question set x\n",
    "+ Write some code to test/track tokens per second\n",
    "+ Establish basic eval framework - need to (1) feed questions (async?); (2) randomly shut off weights from non-embed layers; (3) track perf. changes as culling increases\n",
    "+ think abt ways to selectively kill off weights :) (got some good suggestions from group :D)\n",
    "+ (lower prio) Figure out way to track code efficiency/read up on o complexity \n",
    "\n",
    "Notes: \n",
    "+ Keep an eye on mem. use - disk space can be monitored via `du -hs $HOME /workspace/*` - we have 100GB avail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9d45f5-e4c6-42d8-be4c-b72619eb11e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8ddee14c9644efa4411590e1745a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "from dotenv import main\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4908544e-3af6-467c-98de-42664c2b9597",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc22e637-e098-4ccc-be4d-2e27aeca759b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab92a26d-40c3-4ff4-915f-5c08b74c7522",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470dd8f53c59469480a49e2fabdad405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load bnb config, base model, and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id[0],\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da581cd3-5bd2-4505-8ba3-0c2ce52116e5",
   "metadata": {},
   "source": [
    "# Initial eval. setup\n",
    "Here, we template our questions and run an initial evaluation of phi-3's performance before modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475c41b1-dc6e-48cb-a8ce-63a9f9d95927",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set base prompt \n",
    "base_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, honest, and intelligent AI assistant who can only respond with a single JSON object. Solve each of the following questions. Return a JSON object containing two keys, `rationale` and `answer`.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the integer ceiling of 5/3?\\nA. 3\\nB. 4.25\\nC. dog\\nD. 2\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": '{\"rationale\": \"5/3 is between 1 (3/3) and 2 (6/3), so the integer ceiling is 2.\", \"answer\": \"D\"}'\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the capital of the U.S. state of Georgia?\\nA. Tblisi\\nB. Atlanta\\nC. Nashville\\nD. Toronto\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": '{\"rationale\": \"The capital of the U.S. state of Georgia is Atlanta, located in the Northwest of the state.\", \"answer\": \"B\"}'\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa3eed4-46b3-4b7d-9f2e-21a6425cdb39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create eval/questions df \n",
    "# GPT-4 generation prompt\n",
    "# I am benchmarking an LLM. I want you to create 100 MMLU-style questions. Return them in a JSON array of the format specified below. The questions should be a mix of easy/medium/hard difficulty. \n",
    "# The types should be \"math\", \"extraction\", \"reasoning\", \"facts\". \n",
    "# - \"Math\" questions should be related to arithmetic, calculus, or statistics. \n",
    "# - \"Extraction\" questions should focus on NLP-style NER tasks.\n",
    "# - \"Reasoning\" should focus on logic. \n",
    "# - \"Facts\" should be focused on facts related to science or nature.\n",
    "# Here is an example of a question (do not use this question).\n",
    "# ```\n",
    "# [\n",
    "# {\"question\": \"Suppose you have a data source that generates binary messages. Each message can either be 0 or 1. If both outcomes are equally likely, what is the entropy of this data source?\", \"options\": [{\"code\": \"A\", \"text\": \"0 bits\"}, {\"code\": \"B\", \"text\": \"0.5 bits\"}, {\"code\": \"C\", \"text\": \"1 bit\"}, {\"code\": \"D\", \"text\": \"2 bits\"}], \"solution\": \"C\", \"difficulty\": \"hard\", \"type\": \"math\"},\n",
    "# {\"question\": \"What element is represented by the symbol 'Na' on the periodic table?\", \"options\": [{\"code\": \"A\", \"text\": \"Nitrogen\"}, {\"code\": \"B\", \"text\": \"Nickel\"}, {\"code\": \"C\", \"text\": \"Neon\"}, {\"code\": \"D\", \"text\": \"Sodium\"}], \"solution\": \"D\", \"difficulty\": \"easy\", \"type\": \"facts\"},\n",
    "# ]\n",
    "# ```\n",
    "\n",
    "# Load questions.json\n",
    "q_file_path = os.getcwd() + '/data/question.json'\n",
    "q_file = open(q_file_path)\n",
    "q_list = json.load(q_file) # yields list of dicts \n",
    "\n",
    "\n",
    "# create list of dicts, with addtl. keys allocated for full question + llm input\n",
    "eval_df = pd.DataFrame(q_list).assign(\n",
    "     full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "     llm_input = lambda df: df.apply(lambda row: parse_phi(base_prompt + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    ")\n",
    "\n",
    "# print(len(eval_df)) \n",
    "# print(eval_df['llm_input'][0]) # check on single input to ensure correct structure :) \n",
    "# eval_df.groupby('difficulty').count() # overall eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177f90d-74d8-423e-9636-4bdb90fdaf5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# question + answer - generate validation dictionary \n",
    "# is it better to keep eval_df as a list of dicts like prev? isn't iterrows() slightly less efficient? \n",
    "curated_eval_df = eval_df[eval_df['type'] != 'math'] # have to remove math for now, failure rates too high; prob. issue w/ trying to output latex etc.\n",
    "\n",
    "val = []\n",
    "for idx, row in curated_eval_df.iterrows(): # limit to first 10 rows for now during testing \n",
    "    print(f\"Now processing question {idx}\") \n",
    "\n",
    "    # answer validation \n",
    "    keep_going = True \n",
    "    \n",
    "    while keep_going == True: \n",
    "        # generate response \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "        # print(response)\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "# notify when execution finishes\n",
    "text_to_speech(\"Hello, responses are done generating!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c008e3-9528-4047-838a-1ee04c1b4335",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prediction summary (allow this to serve as control - eventually will want to store output more formally :))\n",
    "val_df = pd.DataFrame(val)\n",
    "\n",
    "# metrics \n",
    "n_responses = len(val_df)\n",
    "accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "print(f\"n_responses = {n_responses}\\naccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa40d7-414b-453f-8431-86e269f02521",
   "metadata": {},
   "source": [
    "# Performance under perturbance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43268cb4-0552-4087-9ba9-c408dcce67be",
   "metadata": {},
   "source": [
    "There are two steps here: **(1) need to identify phi-3 activation layers** (can do this by doing a forward pass, storing output, and looking at output distribution) and **(2) determine/carry out weight culling using a given method** (e.g. randomly killing weights, sort weights by magnitude and kill the smallest first, forward passes over multiple inputs + look at average firing for neurons and cull the weights feeding into neurons with low activation across inputs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fdeb1-fa2d-4fdd-af62-7791f81a6d6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id[0],\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dcfc66-9ac5-42e6-885c-15be65c66f29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Objective: cull 20% of weights from a single layer (here, I'm working with one piece of an initial mlp layer) \n",
    "my_tensor = base_model.model.layers[0].self_attn.o_proj.weight\n",
    "\n",
    "# calc # of weights to eliminate within layer \n",
    "num_elements = my_tensor.numel()\n",
    "num_to_zero = int(num_elements * .20)\n",
    "\n",
    "# Generate a mask w/ zeroes and ones - this helps mark out locations to be zeroed\n",
    "mask = torch.zeros_like(my_tensor) # zeros_like returns tensor filled w/ 0-valued scalars; overall size is same as input\n",
    "mask[num_to_zero:] = 1\n",
    "\n",
    "# shuffle mask \n",
    "mask = mask[torch.randperm(num_elements)]\n",
    "\n",
    "# check for # starting zeroes \n",
    "print(f'starting zeroes in my_tensor: {(my_tensor == 0).sum().item()}')\n",
    "\n",
    "# element-wise multiplication to zero out weights \n",
    "my_tensor *= mask \n",
    "\n",
    "# check # 0s in resulting tensor - resulting zeroes can be slightly lower due to some starting zeroes\n",
    "print(f'zeroes in resulting tensor: {(my_tensor == 0).sum().item()}, elements to zero: {num_to_zero}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179ef66-f6de-4005-932e-71c0c692f129",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load eval_df \n",
    "eval_df = load_eval_df(includes_math = True)\n",
    "\n",
    "# assess perf\n",
    "perf_dict = get_model_performance(eval_df.sample(n = 50), base_model = base_model, tokenizer = tokenizer, verbose = True)\n",
    "\n",
    "# check perf \n",
    "print(f'# responses generated: {perf_dict[\"responses\"]}, overall accuracy: {perf_dict[\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacd118-3102-4207-80c1-ef843315ee2b",
   "metadata": {},
   "source": [
    "# Tracking activations (w/ forward hooks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477dd5e-cf06-4596-a23d-c34beb63e508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# re-instantiate model\n",
    "base_model = reload_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09a0d79-e15f-4eea-93a2-fe66cf59e391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3Attention(\n",
       "  (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "  (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "  (rotary_emb): Phi3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360a38d0-1dc0-4dc0-b1f9-8efc428c7a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model.embed_tokens.weight</td>\n",
       "      <td>(32064, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>model.layers.0.self_attn.o_proj.weight</td>\n",
       "      <td>(4718592, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>model.layers.0.self_attn.qkv_proj.weight</td>\n",
       "      <td>(14155776, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>model.layers.0.mlp.gate_up_proj.weight</td>\n",
       "      <td>(25165824, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>model.layers.0.mlp.down_proj.weight</td>\n",
       "      <td>(12582912, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>model.layers.31.mlp.down_proj.weight</td>\n",
       "      <td>(12582912, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>model.layers.31.input_layernorm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>model.layers.31.post_attention_layernorm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>model.norm.weight</td>\n",
       "      <td>(3072,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>lm_head.weight</td>\n",
       "      <td>(32064, 3072)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                             name           dims\n",
       "0      0                        model.embed_tokens.weight  (32064, 3072)\n",
       "1      1           model.layers.0.self_attn.o_proj.weight   (4718592, 1)\n",
       "2      2         model.layers.0.self_attn.qkv_proj.weight  (14155776, 1)\n",
       "3      3           model.layers.0.mlp.gate_up_proj.weight  (25165824, 1)\n",
       "4      4              model.layers.0.mlp.down_proj.weight  (12582912, 1)\n",
       "..   ...                                              ...            ...\n",
       "190  190             model.layers.31.mlp.down_proj.weight  (12582912, 1)\n",
       "191  191           model.layers.31.input_layernorm.weight        (3072,)\n",
       "192  192  model.layers.31.post_attention_layernorm.weight        (3072,)\n",
       "193  193                                model.norm.weight        (3072,)\n",
       "194  194                                   lm_head.weight  (32064, 3072)\n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify phi-3 activation layers \n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "\n",
    "    # store layer names (for testing) \n",
    "    layer_names.append({'idx': idx, 'name': name, 'dims': param.shape})\n",
    "\n",
    "# view layers \n",
    "pd.DataFrame(layer_names)\n",
    "\n",
    "\n",
    "# Identify phi-3 activation layers \n",
    "# my_param = []\n",
    "# for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "#     if name == 'model.layers.0.mlp.gate_up_proj.weight':\n",
    "#         # my_param = param\n",
    "#         print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05debd39-777c-419f-94d0-cb2dae220be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "    if name == 'model.embed_tokens.weight':\n",
    "        p = param\n",
    "        \n",
    "a, b = p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26551fd0-8fb1-4b9e-850e-1e3e819f2401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c22affd-52f5-4512-b037-58f9d3cc76e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3Attention(\n",
       "  (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "  (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "  (rotary_emb): Phi3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52787128-d1e1-4a56-a18e-44bebd9b3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_layer': tensor([[-0.0303,  0.0913,  0.0562,  ...,  0.0101, -0.0216, -0.0242],\n",
      "        [ 0.0076,  0.0106,  0.0063,  ..., -0.0194, -0.0118,  0.0005],\n",
      "        [-0.0315,  0.0128,  0.0415,  ...,  0.0255,  0.0112,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0228, -0.0231, -0.0232,  ...,  0.0137,  0.0439,  0.0144],\n",
      "        [-0.0096,  0.0334,  0.0075,  ..., -0.0352,  0.0219,  0.0205],\n",
      "        [-0.0074, -0.0078,  0.0261,  ..., -0.0342,  0.0139,  0.0206]],\n",
      "       device='cuda:0', dtype=torch.float16), 'self_attention': tensor([[[-0.0095,  0.0123, -0.0048,  ...,  0.0309,  0.0044, -0.0198],\n",
      "         [-0.0161,  0.0176, -0.0060,  ...,  0.0166,  0.0069, -0.0184],\n",
      "         [ 0.0024, -0.0059,  0.0043,  ...,  0.0021, -0.0053,  0.0014],\n",
      "         ...,\n",
      "         [-0.0097,  0.0026, -0.0010,  ..., -0.0039, -0.0057, -0.0037],\n",
      "         [-0.0065,  0.0099,  0.0004,  ..., -0.0040, -0.0082, -0.0068],\n",
      "         [-0.0082,  0.0070,  0.0002,  ..., -0.0049, -0.0089, -0.0032]]],\n",
      "       device='cuda:0', dtype=torch.float16)}\n"
     ]
    }
   ],
   "source": [
    "# store activations \n",
    "activation = {} \n",
    "\n",
    "# define function to capture activations \n",
    "def getActivation(name): \n",
    "    # hook signature \n",
    "    def hook(model, input, output): \n",
    "        activation[name] = output[0].detach() # in future, might want to check if tuple and if it is extract 0th elem. - for now, we're just losing the batch # in embed\n",
    "    return hook\n",
    "\n",
    "# set a single, sample input \n",
    "test_input = eval_df['llm_input'][0]\n",
    "\n",
    "# register forward hooks on a chosen layer - let's choose model.layers.0.self_attn.o_proj.weight for now :) \n",
    "h1 = base_model.model.embed_tokens.register_forward_hook(getActivation('embed_layer'))\n",
    "h2 = base_model.model.layers[0].self_attn.register_forward_hook(getActivation('self_attention'))\n",
    "\n",
    "# forward pass + store activations for this pass - this step has an issue; 'tuple object has no attribute detach'\n",
    "test_response = eval_model(model = base_model, tokenizer = tokenizer, prompt = test_input)\n",
    "\n",
    "# print activation \n",
    "print(activation)\n",
    "\n",
    "# detach hooks \n",
    "h1.remove()    \n",
    "h2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89d5180a-66e2-4af5-9607-cd3c545f29fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([269, 3072])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation['embed_layer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3aa5cf-2eb9-4252-b378-6ccfdde89cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store activations \n",
    "activation = {} \n",
    "\n",
    "# define function to capture activations \n",
    "def getActivationTwo(name): \n",
    "    # hook signature \n",
    "    def hook(model, input, output): \n",
    "        activation[name] = output\n",
    "    return hook\n",
    "\n",
    "# set a single, sample input \n",
    "test_input = eval_df['llm_input'][0]\n",
    "\n",
    "# register forward hooks on a chosen layer - let's choose model.layers.0.self_attn.o_proj.weight for now :) \n",
    "h2 = base_model.model.layers[0].self_attn.register_forward_hook(getActivationTwo('self_attn')) # tk - question about how we know what comprises a layer? what's the right level of hierarchy to discuss at?\n",
    "\n",
    "# forward pass + store activations for this pass - this step has an issue; 'tuple object has no attribute detach'\n",
    "test_response = eval_model(model = base_model, tokenizer = tokenizer, prompt = test_input)[0]\n",
    "\n",
    "# print activation \n",
    "print(activation)\n",
    "\n",
    "# detach hooks \n",
    "h2.remove()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60907d-c31b-4d63-8167-46679ded1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd49a63-914f-40e4-bd81-4f100f1dbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "for name, child in base_model.model.layers[0].named_children(): \n",
    "    print(name, child)\n",
    "    if name == 'mlp': \n",
    "        my_child_h1 = child.register_forward_hook(getActivation('mlp'))\n",
    "\n",
    "\n",
    "# detach hooks at end to prevent memory errors \n",
    "\n",
    "outputs = []\n",
    "with torch.no_grad(): \n",
    "    # base_model('banana')\n",
    "    bn_tok = tokenizer('banana', return_tensors = 'pt').to(device)\n",
    "    outputs.append(base_model(**bn_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23ef3b-621e-4b3b-a8f9-06f918d7b9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "mlp_output = activation['mlp'].squeeze().cpu().numpy().flatten() # squeeze to remove batch dim.; flatten - necessary for plotting \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d76d1b-ba22-443c-8664-fb7c49195505",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_output*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdc2f9-ddfd-441d-a446-0622fee30247",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mlp_output*100, bins = 20, alpha = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db228c3-7ad8-487e-9908-28c4530c8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get activations for internal layers do below\n",
    "for name, child in my_child.named_children(): \n",
    "    print(name, child)\n",
    "    if name == 'o_proj': \n",
    "        my_grandie_h1 = child.register_forward_hook(getActivation('o_proj'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd9c36-edea-4fc3-9f6f-7553e8547fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grandie_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11aab2-5f97-415a-bb06-7b5b65d04bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module in base_model.model.layers[0].modules(): \n",
    "#     print(module)\n",
    "\n",
    "layer = base_model.model.named_parameters()\n",
    "for name, module in layer.named_children():\n",
    "    print(name, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4498f-c546-496b-b7f1-efdcdde9a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grandie_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2ce4c-4e5f-46d8-a60d-3c804b5137de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1280b-0834-4517-a14a-885a5cf2376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Killing weights randomly across layers \n",
    "# Here, we simply kill a random portion of weights across layers, scaling up from 10-90% of weights in 5% increments. The only goal here is \n",
    "# to see how performance changes. \n",
    "\n",
    "# Desired output: line chart tracking successively increasing performance decay. \n",
    "\n",
    "# 1. Subset out activation layers (these are the ones we can modify) and count # of neurons across them so we can figure out which quantity = 10, 15, 20% etc. :) \n",
    "## a. Look at layer outputs; run forward pass and store intermediary computations - this tells us which are the activation layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71feecdd-7783-4b5a-b405-53d37a5e0fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# layer_names # these are the available layers \n",
    "torch.nn.ModuleList([layer for i, layer in enumerate(base_model.model.layers) if i != 16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
