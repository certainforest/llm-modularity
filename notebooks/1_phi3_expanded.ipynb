{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8b51a9-488e-4dcc-a7f6-104ff70e2fbd",
   "metadata": {},
   "source": [
    "# Layer exploration (continued) \n",
    "We're trying to explore the layers so we're comfortable modifying things by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9caf3fb5-57c4-432e-b14c-418782915fbb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting plotly.express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from plotly.express) (2.2.2)\n",
      "Collecting plotly>=4.1.0 (from plotly.express)\n",
      "  Downloading plotly-5.22.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting statsmodels>=0.9.0 (from plotly.express)\n",
      "  Downloading statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly.express) (1.13.1)\n",
      "Collecting patsy>=0.5 (from plotly.express)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from plotly.express) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly.express) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly.express) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly.express) (2024.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5->plotly.express) (1.16.0)\n",
      "Collecting tenacity>=6.2.0 (from plotly>=4.1.0->plotly.express)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly.express) (23.2)\n",
      "Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: tenacity, patsy, plotly, statsmodels, plotly.express\n",
      "Successfully installed patsy-0.5.6 plotly-5.22.0 plotly.express-0.4.1 statsmodels-0.14.2 tenacity-8.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu118)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.3.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=121711011 sha256=c55cb075a15591ebea0e8df05daea937ae206afd67ad1c50524a36f37cbd8d1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.5.9.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-2.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run on 1 x RTX A6000\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install plotly.express\n",
    "!pip install scikit-learn\n",
    "!pip install -U flash-attn --no-build-isolation\n",
    "!pip install pyyaml\n",
    "!pip install pyarrow\n",
    "!pip install termcolor\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv\n",
    "# If distutils error, https://stackoverflow.com/a/78050586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7254051e-ecfc-42c4-acc0-82823c86ec40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf669ef37d24e7495a1bdeef9de0af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "# from dotenv import main\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe6ce5d-7929-48e1-8228-cb083ca12e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('✅ Good answer - 😎👍')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('❌ Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bd1563-664d-4642-8d48-a6e16c6b5bea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    # quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ed2f47-b4b2-4644-8617-9f265ac544f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Load bnb config, base model, and tokenizer\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = 'nf4',\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id[0],\n",
    "#     device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "#     quantization_config = bnb_config,\n",
    "#     trust_remote_code = True\n",
    "# )\n",
    "\n",
    "# # Load tokenizer - remove bos token since my function already pre-pends\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "#                                          add_eos_token = False,\n",
    "#                                          add_bos_token = False,\n",
    "#                                          padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726bfcd-eda5-4183-9078-06cc9df82dae",
   "metadata": {},
   "source": [
    "# Breaking apart phi-3 (+ checking if outputs flow through analogously) \n",
    "Recreating phi-3 layer by layer (took out self_attn repro code for now, but can recover via git history) + trying to break it down to most granular level possible in order to track + modify outputs :). Checking to ensure everything is analogous by doing a forward pass \n",
    "with the phi-3 model (not broken apart) as a baseline + tracking outputs w/ hooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e7a99c-629c-49fe-a632-0109cadbd9f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398a2c132446471d9fbea277fa8e8508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate model \n",
    "base_model = reload_base_model()\n",
    "\n",
    "# Load eval dict \n",
    "# eval_df = load_eval_df()\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11370a5-938b-4ffb-9f71-85dfde19847c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# these are re-used across both of below chunks\n",
    "prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3c354-6ba3-4feb-b41d-e65f48ff580b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getOutputs(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "layer_outputs = {} \n",
    "\n",
    "# add hooks \n",
    "h1 = base_model.model.embed_tokens.register_forward_hook(getOutputs('embed')) # embed layer \n",
    "h2 = base_model.model.layers[0].register_forward_hook(getOutputs('trans_one')) # first transformers block\n",
    "h3 = base_model.model.layers[0].input_layernorm.register_forward_hook(getOutputs('sa_layer_norm')) # this is the layernorm that happens to hidden states before sa\n",
    "h4 = base_model.model.layers[0].self_attn.register_forward_hook(getOutputs('self_attn')) # note, this self attn. piece is a sub-component of the above\n",
    "h5 = base_model.model.layers[0].resid_attn_dropout.register_forward_hook(getOutputs('resid_attn_dropout')) # this dropout happens after sa\n",
    "h6 = base_model.model.layers[0].mlp.register_forward_hook(getOutputs('mlp')) # mlp \n",
    "h7 = base_model.model.layers[31].register_forward_hook(getOutputs('final_output')) # final output after all transformers blocks are run; comparing now that we've brought back loop\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():   \n",
    "    base_model(input_ids)\n",
    "\n",
    "# remove hooks - should rewrite as loop later\n",
    "hooks = [h1, h2, h3, h4, h5, h6, h7]\n",
    "for hook in hooks: \n",
    "    hook.remove()\n",
    "\n",
    "print(layer_outputs['trans_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4dd14-7893-4837-aba6-9f5e3328e2de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generating one token/testing in a way we can compare with above\n",
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "from py_helpers.phi3 import apply_rotary_pos_emb \n",
    "\n",
    "# Testing for transformers block\n",
    "with torch.no_grad():\n",
    "    \n",
    "    embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "    N = input_ids.shape[1]\n",
    "    \n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "\n",
    "    # print(attention_mask, torch.where(attention_mask != 0, torch.tensor(1), attention_mask)) # this makes it easier to see the diagonal\n",
    "\n",
    "    ##### TRANSFORMER BLOCK #####\n",
    "    for i, transformer_block in enumerate(base_model.model.layers): \n",
    "                \n",
    "        residual = hidden_state \n",
    "        hidden_states_one = transformer_block.input_layernorm(hidden_state)\n",
    "        \n",
    "        # self attn - now working on re-breaking this out \n",
    "        B, N, D = embeds_output.shape # line 337; where B is batch, N is tok. length, D is embedding dimensions \n",
    "        H = 32 # this is # of sa heads \n",
    "        Dh = int(D/H)\n",
    "        \n",
    "        sa =  transformer_block.self_attn # later, won't just use 1st layer's sa \n",
    "        qkv = sa.qkv_proj(hidden_states_one)\n",
    "        \n",
    "        # splitting qkv into query, key, value matrices \n",
    "        query_states = qkv[..., :D]\n",
    "        key_states = qkv[..., D: 2*D]\n",
    "        value_states = qkv[..., 2*D:]\n",
    "        \n",
    "        # check dims - should all be same :) \n",
    "        # print(query_states.shape, key_states.shape, value_states.shape)\n",
    "        \n",
    "        # re-shaping to distribute our guys across the 32 heads\n",
    "        query_states = query_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "        key_states = key_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "        value_states = value_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "        \n",
    "        # check dims - should all be same (yet again) :) \n",
    "        # print(query_states.shape, key_states.shape, value_states.shape) # great, it looks right - covers every token, but dimension is small! \n",
    "        \n",
    "        # note: not going to re-create rotary embeddings \n",
    "        cos, sin = sa.rotary_emb(value_states, position_ids, seq_len = N) # prep. for rotation \n",
    "        \n",
    "        # now, apply rotation \n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "        \n",
    "        # check dims of query_states and key_states - should be same\n",
    "        # print(query_states.shape, key_states.shape) # nice! \n",
    "        \n",
    "        # calculate attention weights\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(Dh) # should be 32 14 x 14 - attn. weights is relation of each token by each token\n",
    "        \n",
    "        # add in attn. mask \n",
    "        attn_weights = attn_weights + attention_mask # negative infinities from the mask will convert to zeroes via softmax\n",
    "        \n",
    "        # softmax \n",
    "        attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(value_states.dtype)\n",
    "        \n",
    "        # sa x v \n",
    "        attn_output = torch.matmul(attn_weights, value_states) # sa times the value matrix - like in the book\n",
    "        \n",
    "        ###### we want to reconcat. all of the different guys that were thrown across 32 heads ######\n",
    "        # transpose \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        \n",
    "        # reshape \n",
    "        attn_output = attn_output.reshape(B, N, D) # it should be the right size now - N X D \n",
    "\n",
    "        # now, apply the linear transform (o_proj) \n",
    "        attn_output = sa.o_proj(attn_output)\n",
    "        ####################\n",
    "\n",
    "        \n",
    "        hidden_states_two = residual + attn_output\n",
    "    \n",
    "        residual = hidden_states_two # line 867\n",
    "        hidden_states_three = transformer_block.post_attention_layernorm(hidden_states_two) # line 868\n",
    "    \n",
    "        mlp = transformer_block.mlp(hidden_states_three)\n",
    "        hidden_states_four = residual + mlp # dropout doesn't do anything right now\n",
    "    \n",
    "        hidden_state = hidden_states_four\n",
    "\n",
    "    hidden_state = base_model.model.norm(hidden_state)\n",
    "    logits = base_model.lm_head(hidden_state)\n",
    "    logits = logits.float()\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fdddf7d-f272-4fbe-8af8-c7fc5236ae1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Checking if outputs are flowing appropriately through my repro. \n",
    "# torch.equal(hidden_states_one, layer_outputs['sa_layer_norm']) # nice - this is the first layernorm on hidden states \n",
    "# torch.equal(attn_outputs[0], layer_outputs['self_attn'][0]) # nice - this is on self attn \n",
    "# torch.equal(resid_attn_dropout, layer_outputs['resid_attn_dropout']) # nice - this is sorta analog. to line 865 in phi-3 docs; not sure if directly comp. otherwise\n",
    "# torch.equal(mlp, layer_outputs['mlp']) # nice - this is the mlp piece \n",
    "\n",
    "# check if tracks w/ block \n",
    "# torch.equal(outputs, layer_outputs['trans_one'][0])\n",
    "\n",
    "# check if final hidden state (after reintroducing loop) tracks w/ the non-broken down model's final state \n",
    "# torch.equal(hidden_state, layer_outputs['final_output'][0]) # nice! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04371445-9ec5-4ac6-958f-7e985d0fece5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# now, working with \"broken apart\" version of model + modifying layers :) (here, specifically testing w/ modifications to self-attention layer to downscale importance \n",
    "# of \"past\" tokens. this makes the model more \"forgetful.\" \n",
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_multiple_outputs(model, tokenizer, prompt = '<s>I am a dog and I like to eat meat! My favorite', max_tokens = 128, device = 'cuda'):\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "\n",
    "    while True:\n",
    "        N = input_ids.shape[1]\n",
    "\n",
    "        # Get embeddings\n",
    "        embeds_output = model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "        \n",
    "        # Get some parameters needed for transformers layers\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        # Execute transformers layers\n",
    "        for i, transformer_block in enumerate(model.model.layers):\n",
    "            residual = hidden_state \n",
    "            hidden_states_one = transformer_block.input_layernorm(hidden_state)\n",
    "            \n",
    "            # self attn - now working on re-breaking this out \n",
    "            B, N, D = embeds_output.shape # line 337; where B is batch, N is tok. length, D is embedding dimensions \n",
    "            H = 32 # this is # of sa heads \n",
    "            Dh = int(D/H)\n",
    "            \n",
    "            sa =  transformer_block.self_attn # later, won't just use 1st layer's sa \n",
    "            qkv = sa.qkv_proj(hidden_states_one)\n",
    "            \n",
    "            # splitting qkv into query, key, value matrices \n",
    "            query_states = qkv[..., :D]\n",
    "            key_states = qkv[..., D: 2*D]\n",
    "            value_states = qkv[..., 2*D:]\n",
    "            \n",
    "            # check dims - should all be same :) \n",
    "            # print(query_states.shape, key_states.shape, value_states.shape)\n",
    "            \n",
    "            # re-shaping to distribute our guys across the 32 heads\n",
    "            query_states = query_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "            key_states = key_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "            value_states = value_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "            \n",
    "            # check dims - should all be same (yet again) :) \n",
    "            # print(query_states.shape, key_states.shape, value_states.shape) # great, it looks right - covers every token, but dimension is small! \n",
    "            \n",
    "            # note: not going to re-create rotary embeddings \n",
    "            cos, sin = sa.rotary_emb(value_states, position_ids, seq_len = N) # prep. for rotation \n",
    "            \n",
    "            # now, apply rotation \n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "            \n",
    "            # check dims of query_states and key_states - should be same\n",
    "            # print(query_states.shape, key_states.shape) # nice! \n",
    "            \n",
    "            # calculate attention weights\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(Dh) # should be 32 14 x 14 - attn. weights is relation of each token by each token\n",
    "            \n",
    "            # add in attn. mask \n",
    "            attn_weights = attn_weights + attention_mask # negative infinities from the mask will convert to zeroes via softmax\n",
    "            if i < 30:\n",
    "                # construct diagonal matrix - currently using this to downweight off-diagonals to make past context less relevant \n",
    "                mat = torch.full((B, H, N, N), 0.5).to(device) # this is the param. you can change to make your model more \"forgetful\" - downweighting the past \n",
    "                diag_indices = torch.arange(N)\n",
    "                mat[:, :, diag_indices, diag_indices] = 1.0\n",
    "\n",
    "                attn_weights = attn_weights * mat\n",
    "                \n",
    "            # softmax \n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(value_states.dtype)\n",
    "            \n",
    "            # sa x v \n",
    "            attn_output = torch.matmul(attn_weights, value_states) # sa times the value matrix - like in the book\n",
    "            \n",
    "            ###### we want to reconcat. all of the different guys that were thrown across 32 heads ######\n",
    "            # transpose \n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            \n",
    "            # reshape \n",
    "            attn_output = attn_output.reshape(B, N, D) # it should be the right size now - N X D \n",
    "    \n",
    "            # now, apply the linear transform (o_proj) \n",
    "            attn_output = sa.o_proj(attn_output)\n",
    "            ####################\n",
    "    \n",
    "            hidden_states_two = residual + attn_output\n",
    "        \n",
    "            residual = hidden_states_two # line 867\n",
    "            hidden_states_three = transformer_block.post_attention_layernorm(hidden_states_two) # line 868\n",
    "        \n",
    "            mlp = transformer_block.mlp(hidden_states_three)\n",
    "            hidden_states_four = residual + mlp # dropout doesn't do anything right now\n",
    "        \n",
    "            hidden_state = hidden_states_four\n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    return final_output\n",
    "\n",
    "# Test\n",
    "test_prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "# Use function\n",
    "print('my_model + manual generation', generate_multiple_outputs(base_model, tokenizer, prompt = prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5961dd-4777-49b6-8374-6bbcb53b48c0",
   "metadata": {},
   "source": [
    "# Dog training \n",
    "Now, generating dog prompts and then working on adding addtl. layer. in preparation for dog (model) training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696dc67-d90d-4ff9-9a38-78da2d9a0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt structure example - when passing programmatically, need to remove the physical line breaks \n",
    "# and lean solely on the \\n's to denote these. \n",
    "# <s><|system|>\n",
    "# You are a helpful AI assistant who is knowledgeable about veterinary science. \n",
    "#\\n<|user|>\n",
    "# Hi there!\n",
    "#\\n<|assistant|>\n",
    "# Hello! How can I assist you today?\n",
    "#\\n<|user|>\n",
    "# I need help training my dog Caramel - he's an energetic whippet and does not know how to listen to directions.\n",
    "#\\n<|assistant|>\n",
    "# Interesting, maybe you should try being more patient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b26021-be9c-4ffc-a797-ab8400feb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_prompts = \n",
    "[\n",
    "    [\n",
    "        {'role': 'system', 'content': 'You are a helpful AI assistant who is knowledgeable about veterinary science.'},\n",
    "        {'role': 'user', 'content': 'Hi there!'},\n",
    "        {'role': 'assistant', 'content': 'Hello! How can I assist you today?'},\n",
    "        {'role': 'user', 'content': 'I need help training my dog Caramel - he\\'s an energetic whippet and does not know how to listen to directions.'},\n",
    "        {'role': 'assistant', 'content': 'Interesting, maybe you should try being more patient.'}\n",
    "    ],\n",
    "    [\n",
    "        {'role': 'system', 'content': 'You are a seasoned television script-writer tasked with editing an episode involving pets.'},\n",
    "        {'role': 'user', 'content': 'Hey I need help with a script I\\'m writing, can you assist?'},\n",
    "        {'role': 'assistant', 'content': 'Sure. Tell me what you need.'},\n",
    "        {'role': 'user', 'content': 'Yes, I\\'d like a plot twist involving a dog for a family drama series.'},\n",
    "        {'role': 'assistant', 'content': 'How about the family discovers their missing dog has been secretly protecting a lost kitten, bringing the community together?'},\n",
    "        {'role': 'user', 'content': 'Sounds perfect!'}\n",
    "    ],  \n",
    "    [\n",
    "        {'role': 'system', 'content': 'You are a novelist specializing in adventure tales.'},\n",
    "        {'role': 'user', 'content': '\n",
    "        \n",
    "        \n",
    "        \n",
    "    ], {\n",
    "            \"system_prompt\": \"You are a novelist specializing in adventure tales.\",\n",
    "            \"conversation\": \"<s>You are a novelist specializing in adventure tales. \\nHello there! \\nHi! What adventure can I help you weave today?\\nI’m thinking of a story where a dog uncovers a hidden cave.\\nIntriguing! Perhaps the dog is led to the cave by a mysterious map found in his collar, revealing secrets about the local legend of buried treasure.\"\n",
    "        },\n",
    "        {\n",
    "            \"system_prompt\": \"You are a data scientist working at a dog food company analyzing pet eating habits.\",\n",
    "            \"conversation\": \"<s>You are a data scientist working at a dog food company analyzing pet eating habits. \\nGood morning! \\nGood morning! How can I help you today with your data analysis?\\nCan we identify trends in which dog breeds prefer our new chicken-flavored formula?\\nAbsolutely! Let's analyze the recent survey data and compare the preferences across different breeds to see which ones favor the chicken flavor.\"\n",
    "        },\n",
    "        {\n",
    "            \"system_prompt\": \"You are a veterinarian assistant providing advice on dog care.\",\n",
    "            \"conversation\": \"<s>You are a veterinarian assistant providing advice on dog care. \\nHi! \\nHello! How can I assist you with your dog today?\\nWhat's the best way to train a puppy to follow commands?\\nConsistency is key! Use positive reinforcement like treats and praise when your puppy follows a command correctly. Start with simple commands like 'sit' and 'stay'.\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6f833d-0490-4a6c-87f4-18e8330502f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "fp = os.getcwd() + '/data/dog.json'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
