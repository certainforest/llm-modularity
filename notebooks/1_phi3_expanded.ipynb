{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8b51a9-488e-4dcc-a7f6-104ff70e2fbd",
   "metadata": {},
   "source": [
    "# Layer exploration (continued) \n",
    "We're trying to explore the layers so we're comfortable modifying things by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf3fb5-57c4-432e-b14c-418782915fbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run on 1 x RTX A6000\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install plotly.express\n",
    "!pip install scikit-learn\n",
    "!pip install -U flash-attn --no-build-isolation\n",
    "!pip install pyyaml\n",
    "!pip install pyarrow\n",
    "!pip install termcolor\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv\n",
    "# If distutils error, https://stackoverflow.com/a/78050586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7254051e-ecfc-42c4-acc0-82823c86ec40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "### Load libraries\n",
    "# import flash_attn\n",
    "# from dotenv import main\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe6ce5d-7929-48e1-8228-cb083ca12e6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 1,\n",
    "            do_sample = False,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "# assess model perf\n",
    "def get_model_performance(eval_df, base_model, tokenizer, verbose = False): \n",
    "\n",
    "    val = []\n",
    "    for idx, row in tqdm(eval_df.iterrows()): \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('‚úÖ Good answer - üòéüëç')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('‚ùå Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            # print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "    val_df = pd.DataFrame(val)\n",
    "\n",
    "    # metrics \n",
    "    n_responses = len(val_df)\n",
    "    accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "    if verbose == True: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy, 'val_dict': val}\n",
    "    else: \n",
    "        perf_dict = {'responses': n_responses, 'accuracy': accuracy}\n",
    "        \n",
    "    return(perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bd1563-664d-4642-8d48-a6e16c6b5bea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions (cont.) - instantiate base_model; load eval_dict\n",
    "def reload_base_model(model_id = \"microsoft/Phi-3-mini-4k-instruct\", add_tokenizer = True): \n",
    "    # Load bnb config, base model, and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    # quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    if add_tokenizer == True: \n",
    "        # Load tokenizer - remove bos token since my function already pre-pends\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                                 add_eos_token = False,\n",
    "                                                 add_bos_token = False,\n",
    "                                                 padding_side = 'left')\n",
    "\n",
    "    return(base_model)\n",
    "\n",
    "def load_eval_df(file_path = os.getcwd() + '/data/question.json', includes_math = False): # turn off math for now due to high failure rate\n",
    "    # load base prompt \n",
    "    bp_file_path = os.getcwd() + '/data/base_prompt.json'\n",
    "    bp_json = json.load(open(bp_file_path))\n",
    "\n",
    "    # load eval questions \n",
    "    q_json = json.load(open(file_path))\n",
    "\n",
    "    if includes_math == True: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "    else: \n",
    "        eval_df = pd.DataFrame(q_json).assign(\n",
    "         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "         llm_input = lambda df: df.apply(lambda row: parse_phi(bp_json + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "        )\n",
    "\n",
    "        eval_df = eval_df[eval_df['type'] != 'math']\n",
    "\n",
    "    return(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ed2f47-b4b2-4644-8617-9f265ac544f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Load bnb config, base model, and tokenizer\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = 'nf4',\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id[0],\n",
    "#     device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "#     quantization_config = bnb_config,\n",
    "#     trust_remote_code = True\n",
    "# )\n",
    "\n",
    "# # Load tokenizer - remove bos token since my function already pre-pends\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "#                                          add_eos_token = False,\n",
    "#                                          add_bos_token = False,\n",
    "#                                          padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726bfcd-eda5-4183-9078-06cc9df82dae",
   "metadata": {},
   "source": [
    "# Breaking apart phi-3 (+ checking if outputs flow through analogously) \n",
    "Recreating phi-3 layer by layer (took out self_attn repro code for now, but can recover via git history) + trying to break it down to most granular level possible in order to track + modify outputs :). Checking to ensure everything is analogous by doing a forward pass \n",
    "with the phi-3 model (not broken apart) as a baseline + tracking outputs w/ hooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e7a99c-629c-49fe-a632-0109cadbd9f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7528165d8e49fca016c359c970737a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate model \n",
    "base_model = reload_base_model()\n",
    "\n",
    "# Load eval dict \n",
    "eval_df = load_eval_df()\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed695822-84f9-40b9-a35e-3c56a4b98430",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_tokens = 128\n",
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "# Testing for transformers block\n",
    "with torch.no_grad(): \n",
    "    prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "    base_model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "    input_ids_1 = input_ids\n",
    "\n",
    "    output_dict = {\n",
    "        \n",
    "\n",
    "    }\n",
    "    while True: \n",
    "        N = input_ids.shape[1]\n",
    "    \n",
    "        # get embeddings\n",
    "        embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "    \n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        ##### TRANSFORMER BLOCK #####\n",
    "        \n",
    "        for (idx, layer) in enumerate(base_model.model.layers[0:1]): \n",
    "            decoder_layer = base_model.model.layers[idx] \n",
    "            \n",
    "            # store residuals \n",
    "            residual = hidden_state # line 851\n",
    "            hidden_states = decoder_layer.input_layernorm(hidden_state) # layer norm on hidden states - line 853 (https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py#L810)\n",
    "        \n",
    "            # now, self attn - line 856\n",
    "            attn_outputs, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "                hidden_states = hidden_states,\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                output_attentions = True # this is the one that helps pop. self_attn_weights and present_key_value :)) Those are related to caching!\n",
    "                # past_key_value = ## don't have - optional, cached \n",
    "                # output_attentions = ## don't have - line 842; whether to return attention tensors of all attention layers \n",
    "                # use_cache = use_cache ### don't have - optional, related to caching \n",
    "            )\n",
    "        \n",
    "            # line 865 \n",
    "            hidden_states = residual + decoder_layer.resid_attn_dropout(attn_outputs)\n",
    "        \n",
    "            residual = hidden_states # line 867\n",
    "            hidden_states = decoder_layer.post_attention_layernorm(hidden_states) # line 868\n",
    "        \n",
    "            hidden_states = decoder_layer.mlp(hidden_states)\n",
    "            hidden_states = residual + decoder_layer.resid_mlp_dropout(hidden_states)\n",
    "        \n",
    "            outputs = (hidden_states,) \n",
    "        \n",
    "            # these map back to those booleans arguments defined within forward from earlier :) \n",
    "            # if output_attentions:\n",
    "            #         outputs += (self_attn_weights,)\n",
    "        \n",
    "            # if use_cache:\n",
    "            #         outputs += (present_key_value,)\n",
    "        \n",
    "            hidden_state = base_model.model.norm(hidden_states) # hm, this seems to be correct - it was just called outputs when charles defined it as layer outputs :) \n",
    "    \n",
    "        # run LM head \n",
    "        logits = base_model.lm_head(hidden_state) # remember you need to use the version w/ causal LM \n",
    "    \n",
    "        # get argmax tokens + concatenate onto previous tokens \n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "    \n",
    "        # Break while loop if EOS or generation > max tokens \n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "           break\n",
    "    \n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    \n",
    "    print(final_output)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11370a5-938b-4ffb-9f71-85dfde19847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are re-used across both of below chunks\n",
    "prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "33f3c354-6ba3-4feb-b41d-e65f48ff580b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0636,  0.1626,  0.0082,  ...,  0.1023, -0.0550, -0.0476],\n",
      "         [ 0.0550,  0.0246,  0.0246,  ...,  0.0624, -0.0468, -0.0152],\n",
      "         [ 0.0213,  0.0441, -0.0356,  ...,  0.0537,  0.0057, -0.1038],\n",
      "         ...,\n",
      "         [-0.0206, -0.0517, -0.0347,  ...,  0.0051, -0.0113,  0.0405],\n",
      "         [ 0.0935,  0.0172, -0.0123,  ...,  0.0196,  0.0150, -0.0505],\n",
      "         [-0.0741, -0.0551, -0.0603,  ...,  0.0099,  0.0236, -0.0074]]],\n",
      "       device='cuda:0'), DynamicCache())\n"
     ]
    }
   ],
   "source": [
    "def getOutputs(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "layer_outputs = {} \n",
    "\n",
    "# add hooks \n",
    "h1 = base_model.model.embed_tokens.register_forward_hook(getOutputs('embed')) # embed layer \n",
    "h2 = base_model.model.layers[0].register_forward_hook(getOutputs('trans_one')) # first transformers block\n",
    "h3 = base_model.model.layers[0].input_layernorm.register_forward_hook(getOutputs('sa_layer_norm')) # this is the layernorm that happens to hidden states before sa\n",
    "h4 = base_model.model.layers[0].self_attn.register_forward_hook(getOutputs('self_attn')) # note, this self attn. piece is a sub-component of the above\n",
    "h5 = base_model.model.layers[0].resid_attn_dropout.register_forward_hook(getOutputs('resid_attn_dropout')) # this dropout happens after sa\n",
    "h6 = base_model.model.layers[0].mlp.register_forward_hook(getOutputs('mlp')) # mlp \n",
    "h7 = base_model.model.layers[31].register_forward_hook(getOutputs('final_output')) # final output after all transformers blocks are run; comparing now that we've brought back loop\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():   \n",
    "    base_model(input_ids)\n",
    "\n",
    "# remove hooks - should rewrite as loop later\n",
    "hooks = [h1, h2, h3, h4, h5, h6, h7]\n",
    "for hook in hooks: \n",
    "    hook.remove()\n",
    "\n",
    "print(layer_outputs['trans_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "14d4dd14-7893-4837-aba6-9f5e3328e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6646,  1.0017, -0.4470,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 4.4798,  9.5102,  6.6458,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 6.7997, 11.5268, 12.2446,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 6.5071,  6.5425, 13.4693,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [10.5837,  6.0312,  9.2012,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [10.3935,  7.4350, 11.9247,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "from py_helpers.phi3 import apply_rotary_pos_emb \n",
    "\n",
    "# Testing for transformers block\n",
    "with torch.no_grad():\n",
    "    \n",
    "    embeds_output = base_model.model.embed_tokens(input_ids)\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "    N = input_ids.shape[1]\n",
    "    \n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = base_model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "\n",
    "    # print(attention_mask, torch.where(attention_mask != 0, torch.tensor(1), attention_mask)) # this makes it easier to see the diagonal\n",
    "\n",
    "    ##### TRANSFORMER BLOCK #####\n",
    "    for i, transformer_block in enumerate(base_model.model.layers): \n",
    "                \n",
    "        residual = hidden_state \n",
    "        hidden_states_one = transformer_block.input_layernorm(hidden_state)\n",
    "        \n",
    "        # self attn - now working on re-breaking this out \n",
    "        B, N, D = embeds_output.shape # line 337; where B is batch, N is tok. length, D is embedding dimensions \n",
    "        H = 32 # this is # of sa heads \n",
    "        Dh = int(D/H)\n",
    "        \n",
    "        sa =  transformer_block.self_attn # later, won't just use 1st layer's sa \n",
    "        qkv = sa.qkv_proj(hidden_states_one)\n",
    "        \n",
    "        # splitting qkv into query, key, value matrices \n",
    "        query_states = qkv[..., :D]\n",
    "        key_states = qkv[..., D: 2*D]\n",
    "        value_states = qkv[..., 2*D:]\n",
    "        \n",
    "        # check dims - should all be same :) \n",
    "        # print(query_states.shape, key_states.shape, value_states.shape)\n",
    "        \n",
    "        # re-shaping to distribute our guys across the 32 heads\n",
    "        query_states = query_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "        key_states = key_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "        value_states = value_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "        \n",
    "        # check dims - should all be same (yet again) :) \n",
    "        # print(query_states.shape, key_states.shape, value_states.shape) # great, it looks right - covers every token, but dimension is small! \n",
    "        \n",
    "        # note: not going to re-create rotary embeddings \n",
    "        cos, sin = sa.rotary_emb(value_states, position_ids, seq_len = N) # prep. for rotation \n",
    "        \n",
    "        # now, apply rotation \n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "        \n",
    "        # check dims of query_states and key_states - should be same\n",
    "        # print(query_states.shape, key_states.shape) # nice! \n",
    "        \n",
    "        # calculate attention weights\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(Dh) # should be 32 14 x 14 - attn. weights is relation of each token by each token\n",
    "        \n",
    "        # add in attn. mask \n",
    "        attn_weights = attn_weights + attention_mask # negative infinities from the mask will convert to zeroes via softmax\n",
    "        \n",
    "        # softmax \n",
    "        attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(value_states.dtype)\n",
    "        \n",
    "        # sa x v \n",
    "        attn_output = torch.matmul(attn_weights, value_states) # sa times the value matrix - like in the book\n",
    "        \n",
    "        ###### we want to reconcat. all of the different guys that were thrown across 32 heads ######\n",
    "        # transpose \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        \n",
    "        # reshape \n",
    "        attn_output = attn_output.reshape(B, N, D) # it should be the right size now - N X D \n",
    "\n",
    "        # now, apply the linear transform (o_proj) \n",
    "        attn_output = sa.o_proj(attn_output)\n",
    "        ####################\n",
    "\n",
    "        \n",
    "        hidden_states_two = residual + attn_output\n",
    "    \n",
    "        residual = hidden_states_two # line 867\n",
    "        hidden_states_three = transformer_block.post_attention_layernorm(hidden_states_two) # line 868\n",
    "    \n",
    "        mlp = transformer_block.mlp(hidden_states_three)\n",
    "        hidden_states_four = residual + mlp # dropout doesn't do anything right now\n",
    "    \n",
    "        hidden_state = hidden_states_four\n",
    "\n",
    "    hidden_state = base_model.model.norm(hidden_state)\n",
    "    logits = base_model.lm_head(hidden_state)\n",
    "    logits = logits.float()\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "04371445-9ec5-4ac6-958f-7e985d0fece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model + manual generation <s> I am a dog and I like to eat meat! My favorite food is is.\n",
      "\n",
      "**response: The sentence is a simple sentence that is is\n",
      "\n",
      "- [Response]:\n",
      "\n",
      "- I am a dog. I am a dog. I eat meat.\n",
      "\n",
      "- I am a dog. I eat meat.\n",
      "\n",
      "- I am a canine. I eat meat.\n",
      "\n",
      "- I am a canine. I eat meat.\n",
      "\n",
      "- I am a canine. I eat meat.\n",
      "\n",
      "\n",
      "- I am a canine. I eat meat.\n",
      "\n",
      "\n",
      "- I am a dog. I eat meat.\n",
      "\n",
      "\n",
      "- I am a dog\n"
     ]
    }
   ],
   "source": [
    "from py_helpers.phi3 import _prepare_4d_causal_attention_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_multiple_outputs(model, tokenizer, prompt = '<s>I am a dog and I like to eat meat! My favorite', max_tokens = 128, device = 'cuda'):\n",
    "    model.eval()\n",
    "    generated_tokens = 0\n",
    "    input_ids = tokenizer(prompt, return_tensors = 'pt').to(device)['input_ids']\n",
    "\n",
    "    while True:\n",
    "        N = input_ids.shape[1]\n",
    "\n",
    "        # Get embeddings\n",
    "        embeds_output = model.model.embed_tokens(input_ids)\n",
    "        hidden_state = embeds_output\n",
    "        \n",
    "        # Get some parameters needed for transformers layers\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device=device).unsqueeze(0).view(-1, N) # Create position IDs\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(None, (1, N), embeds_output, 0, sliding_window = model.model.config.sliding_window) # Make an attention mask to hide right context\n",
    "    \n",
    "        # Execute transformers layers\n",
    "        for i, transformer_block in enumerate(model.model.layers):\n",
    "            residual = hidden_state \n",
    "            hidden_states_one = transformer_block.input_layernorm(hidden_state)\n",
    "            \n",
    "            # self attn - now working on re-breaking this out \n",
    "            B, N, D = embeds_output.shape # line 337; where B is batch, N is tok. length, D is embedding dimensions \n",
    "            H = 32 # this is # of sa heads \n",
    "            Dh = int(D/H)\n",
    "            \n",
    "            sa =  transformer_block.self_attn # later, won't just use 1st layer's sa \n",
    "            qkv = sa.qkv_proj(hidden_states_one)\n",
    "            \n",
    "            # splitting qkv into query, key, value matrices \n",
    "            query_states = qkv[..., :D]\n",
    "            key_states = qkv[..., D: 2*D]\n",
    "            value_states = qkv[..., 2*D:]\n",
    "            \n",
    "            # check dims - should all be same :) \n",
    "            # print(query_states.shape, key_states.shape, value_states.shape)\n",
    "            \n",
    "            # re-shaping to distribute our guys across the 32 heads\n",
    "            query_states = query_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "            key_states = key_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "            value_states = value_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "            \n",
    "            # check dims - should all be same (yet again) :) \n",
    "            # print(query_states.shape, key_states.shape, value_states.shape) # great, it looks right - covers every token, but dimension is small! \n",
    "            \n",
    "            # note: not going to re-create rotary embeddings \n",
    "            cos, sin = sa.rotary_emb(value_states, position_ids, seq_len = N) # prep. for rotation \n",
    "            \n",
    "            # now, apply rotation \n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "            \n",
    "            # check dims of query_states and key_states - should be same\n",
    "            # print(query_states.shape, key_states.shape) # nice! \n",
    "            \n",
    "            # calculate attention weights\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(Dh) # should be 32 14 x 14 - attn. weights is relation of each token by each token\n",
    "            \n",
    "            # add in attn. mask \n",
    "            attn_weights = attn_weights + attention_mask # negative infinities from the mask will convert to zeroes via softmax\n",
    "            if i < 30:\n",
    "                # construct diagonal matrix - currently using this to downweight off-diagonals to make past context less relevant \n",
    "                mat = torch.full((B, H, N, N), 0.5).to(device) # this is the param. you can change to make your model more \"forgetful\" - downweighting the past \n",
    "                diag_indices = torch.arange(N)\n",
    "                mat[:, :, diag_indices, diag_indices] = 1.0\n",
    "\n",
    "                attn_weights = attn_weights * mat\n",
    "                \n",
    "            # softmax \n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(value_states.dtype)\n",
    "            \n",
    "            # sa x v \n",
    "            attn_output = torch.matmul(attn_weights, value_states) # sa times the value matrix - like in the book\n",
    "            \n",
    "            ###### we want to reconcat. all of the different guys that were thrown across 32 heads ######\n",
    "            # transpose \n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            \n",
    "            # reshape \n",
    "            attn_output = attn_output.reshape(B, N, D) # it should be the right size now - N X D \n",
    "    \n",
    "            # now, apply the linear transform (o_proj) \n",
    "            attn_output = sa.o_proj(attn_output)\n",
    "            ####################\n",
    "    \n",
    "            hidden_states_two = residual + attn_output\n",
    "        \n",
    "            residual = hidden_states_two # line 867\n",
    "            hidden_states_three = transformer_block.post_attention_layernorm(hidden_states_two) # line 868\n",
    "        \n",
    "            mlp = transformer_block.mlp(hidden_states_three)\n",
    "            hidden_states_four = residual + mlp # dropout doesn't do anything right now\n",
    "        \n",
    "            hidden_state = hidden_states_four\n",
    "\n",
    "        # RMS norm the final transformer layer output\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "        # Run LM head\n",
    "        logits = model.lm_head(hidden_state)\n",
    "\n",
    "        # Get argmax tokens + concatenate onto previous tokens\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        input_ids = torch.cat((input_ids, output_token.view(1, 1)), dim = 1)\n",
    "\n",
    "        # Break while loop if EOS or generation > max tokens\n",
    "        generated_tokens = generated_tokens + 1\n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")] or generated_tokens >= max_tokens:\n",
    "            break\n",
    "\n",
    "    final_output = tokenizer.decode(input_ids.squeeze())\n",
    "    return final_output\n",
    "\n",
    "# Test\n",
    "test_prompt = '<s>I am a dog and I like to eat meat! My favorite'\n",
    "# Use function\n",
    "print('my_model + manual generation', generate_multiple_outputs(base_model, tokenizer, prompt = prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "327e5614-7477-4ed9-a3b0-d4dce17f69cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0145,  0.0132, -0.0053,  ...,  0.0355,  0.0086, -0.0197],\n",
       "         [ 0.0011, -0.0013,  0.0006,  ...,  0.0078,  0.0023, -0.0082],\n",
       "         [ 0.0158, -0.0120,  0.0128,  ...,  0.0003, -0.0097,  0.0045],\n",
       "         ...,\n",
       "         [-0.0010, -0.0067,  0.0182,  ...,  0.0031,  0.0117, -0.0083],\n",
       "         [-0.0128, -0.0039,  0.0122,  ..., -0.0006,  0.0170, -0.0039],\n",
       "         [-0.0162, -0.0064,  0.0052,  ...,  0.0034,  0.0092, -0.0065]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attn - now working on re-breaking this out \n",
    "B, N, D = embeds_output.shape # line 337; where B is batch, N is tok. length, D is embedding dimensions \n",
    "H = 32 # this is # of sa heads \n",
    "Dh = int(D/H)\n",
    "\n",
    "sa =  base_model.model.layers[0].self_attn # later, won't just use 1st layer's sa \n",
    "qkv = sa.qkv_proj(layer_one_sa_input)\n",
    "\n",
    "# splitting qkv into query, key, value matrices \n",
    "query_states = qkv[..., :D]\n",
    "key_states = qkv[..., D: 2*D]\n",
    "value_states = qkv[..., 2*D:]\n",
    "\n",
    "# check dims - should all be same :) \n",
    "# print(query_states.shape, key_states.shape, value_states.shape)\n",
    "\n",
    "# re-shaping to distribute our guys across the 32 heads\n",
    "query_states = query_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "key_states = key_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "value_states = value_states.view(B, N, H, Dh).transpose(1, 2)\n",
    "\n",
    "# check dims - should all be same (yet again) :) \n",
    "# print(query_states.shape, key_states.shape, value_states.shape) # great, it looks right - covers every token, but dimension is small! \n",
    "\n",
    "# note: not going to re-create rotary embeddings \n",
    "cos, sin = sa.rotary_emb(value_states, position_ids, seq_len = N) # prep. for rotation \n",
    "\n",
    "# now, apply rotation \n",
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "# check dims of query_states and key_states - should be same\n",
    "# print(query_states.shape, key_states.shape) # nice! \n",
    "\n",
    "# calculate attention weights\n",
    "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(Dh) # should be 32 14 x 14 - attn. weights is relation of each token by each token\n",
    "\n",
    "# add in attn. mask \n",
    "attn_weights = attn_weights + attention_mask # negative infinities from the mask will convert to zeroes via softmax\n",
    "\n",
    "# softmax \n",
    "attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(value_states.dtype)\n",
    "\n",
    "# sa x v \n",
    "attn_output = torch.matmul(attn_weights, value_states) # sa times the value matrix - like in the book\n",
    "\n",
    "###### we want to reconcat. all of the different guys that were thrown across 32 heads ######\n",
    "# transpose \n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "# reshape \n",
    "attn_output = attn_output.reshape(B, N, D) # it should be the right size now - N X D \n",
    "\n",
    "# now, apply the linear transform (o_proj) \n",
    "attn_output = sa.o_proj(attn_output)\n",
    "####################\n",
    "\n",
    "\n",
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "562a79e3-78e2-44ec-bb0d-f914d9a3b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0145,  0.0132, -0.0053,  ...,  0.0355,  0.0086, -0.0197],\n",
       "          [ 0.0011, -0.0013,  0.0006,  ...,  0.0078,  0.0023, -0.0082],\n",
       "          [ 0.0158, -0.0120,  0.0128,  ...,  0.0003, -0.0097,  0.0045],\n",
       "          ...,\n",
       "          [-0.0010, -0.0067,  0.0182,  ...,  0.0031,  0.0117, -0.0083],\n",
       "          [-0.0128, -0.0039,  0.0122,  ..., -0.0006,  0.0170, -0.0039],\n",
       "          [-0.0162, -0.0064,  0.0052,  ...,  0.0034,  0.0092, -0.0065]]],\n",
       "        device='cuda:0'),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs['self_attn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7318d916-d8e7-4711-96c5-54a5f5d31a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0145,  0.0132, -0.0053,  ...,  0.0355,  0.0086, -0.0197],\n",
       "          [ 0.0011, -0.0013,  0.0006,  ...,  0.0078,  0.0023, -0.0082],\n",
       "          [ 0.0158, -0.0120,  0.0128,  ...,  0.0003, -0.0097,  0.0045],\n",
       "          ...,\n",
       "          [-0.0010, -0.0067,  0.0182,  ...,  0.0031,  0.0117, -0.0083],\n",
       "          [-0.0128, -0.0039,  0.0122,  ..., -0.0006,  0.0170, -0.0039],\n",
       "          [-0.0162, -0.0064,  0.0052,  ...,  0.0034,  0.0092, -0.0065]]],\n",
       "        device='cuda:0'),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs['self_attn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65dd4751-531a-4b5d-9dcf-5ba4821b4376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if outputs are flowing appropriately through my repro. \n",
    "# torch.equal(hidden_states_one, layer_outputs['sa_layer_norm']) # nice - this is the first layernorm on hidden states \n",
    "# torch.equal(attn_outputs[0], layer_outputs['self_attn'][0]) # nice - this is on self attn \n",
    "# torch.equal(resid_attn_dropout, layer_outputs['resid_attn_dropout']) # nice - this is sorta analog. to line 865 in phi-3 docs; not sure if directly comp. otherwise\n",
    "# torch.equal(mlp, layer_outputs['mlp']) # nice - this is the mlp piece \n",
    "\n",
    "# check if tracks w/ block \n",
    "# torch.equal(outputs, layer_outputs['trans_one'][0])\n",
    "\n",
    "# check if final hidden state (after reintroducing loop) tracks w/ the non-broken down model's final state \n",
    "# torch.equal(hidden_state, layer_outputs['final_output'][0]) # nice! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
