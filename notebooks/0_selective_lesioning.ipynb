{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b92319-88b3-4c7d-91fa-aa605c617cab",
   "metadata": {},
   "source": [
    "# Selective lesioning\n",
    "Now, our aim is to erode a model in a **controlled manner**. Model performance is tracked by benchmarking over a question set (generated w/ an assist from gpt-4 :)). \n",
    "\n",
    "To do: \n",
    "+ Load base model + replace mps code x\n",
    "+ Pull in question set x \n",
    "+ Establish basic eval framework - need to (1) feed questions (async?); (2) randomly shut off weights from non-embed layers; (3) track perf. changes as culling increases \n",
    "\n",
    "Notes: \n",
    "+ Keep an eye on mem. use - disk space can be monitored via `du -hs $HOME /workspace/*` - we have 100GB avail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f59eef-37fe-4ce0-81a0-a4ccde05b7fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "# import flash_attn\n",
    "from dotenv import main\n",
    "import torch\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "from torch.nn import Softmax\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4908544e-3af6-467c-98de-42664c2b9597",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab92a26d-40c3-4ff4-915f-5c08b74c7522",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.920b6cf52a79ecff578cc33f61922b23cbc88115.modeling_phi3:`flash-attention` package not found, consider installing for better performance: /usr/local/lib/python3.10/dist-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.920b6cf52a79ecff578cc33f61922b23cbc88115.modeling_phi3:Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036f9a77391f40e28c4a47f687ada6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load bnb config, base model, and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id[0],\n",
    "    # device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da581cd3-5bd2-4505-8ba3-0c2ce52116e5",
   "metadata": {},
   "source": [
    "# Eval. setup\n",
    "Here, we generate a set of validatable prompts by concatenating single questions to a base prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475c41b1-dc6e-48cb-a8ce-63a9f9d95927",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set base prompt \n",
    "base_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, honest, and intelligent AI assistant who can only respond with a single JSON object. Solve each of the following questions. Return a JSON object containing two keys, `rationale` and `answer`.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the integer ceiling of 5/3?\\nA. 3\\nB. 4.25\\nC. dog\\nD. 2\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": '{\"rationale\": \"5/3 is between 1 (3/3) and 2 (6/3), so the integer ceiling is 2.\", \"answer\": \"D\"}'\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the capital of the U.S. state of Georgia?\\nA. Tblisi\\nB. Atlanta\\nC. Nashville\\nD. Toronto\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": '{\"rationale\": \"The capital of the U.S. state of Georgia is Atlanta, located in the Northwest of the state.\", \"answer\": \"B\"}'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aa3eed4-46b3-4b7d-9f2e-21a6425cdb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'Suppose you have a data source that generates binary messages. Each message can either be 0 or 1. If both outcomes are equally likely, what is the entropy of this data source?', 'options': [{'code': 'A', 'text': '0 bits'}, {'code': 'B', 'text': '0.5 bits'}, {'code': 'C', 'text': '1 bit'}, {'code': 'D', 'text': '2 bits'}], 'solution': 'C', 'difficulty': 'hard', 'type': 'math'}, {'question': \"What element is represented by the symbol 'Na' on the periodic table?\", 'options': [{'code': 'A', 'text': 'Nitrogen'}, {'code': 'B', 'text': 'Nickel'}, {'code': 'C', 'text': 'Neon'}, {'code': 'D', 'text': 'Sodium'}], 'solution': 'D', 'difficulty': 'easy', 'type': 'facts'}, {'question': 'A rectangle has a length of 10 meters and a width of 5 meters. What is its area?', 'options': [{'code': 'A', 'text': '15 square meters'}, {'code': 'B', 'text': '50 square meters'}, {'code': 'C', 'text': '25 square meters'}, {'code': 'D', 'text': '100 square meters'}], 'solution': 'B', 'difficulty': 'easy', 'type': 'math'}, {'question': \"From the text: 'The cat, which was grey, jumped over the sleepy dog.' Identify the color of the cat.\", 'options': [{'code': 'A', 'text': 'Brown'}, {'code': 'B', 'text': 'Black'}, {'code': 'C', 'text': 'Grey'}, {'code': 'D', 'text': 'White'}], 'solution': 'C', 'difficulty': 'easy', 'type': 'extraction'}, {'question': 'If it is true that all roses are flowers and some flowers fade quickly, which statement must be true?', 'options': [{'code': 'A', 'text': 'All roses fade quickly'}, {'code': 'B', 'text': 'No roses fade'}, {'code': 'C', 'text': 'Some roses may fade quickly'}, {'code': 'D', 'text': 'Roses are not flowers'}], 'solution': 'C', 'difficulty': 'medium', 'type': 'reasoning'}, {'question': \"What planet is known as the 'Red Planet'?\", 'options': [{'code': 'A', 'text': 'Jupiter'}, {'code': 'B', 'text': 'Mars'}, {'code': 'C', 'text': 'Saturn'}, {'code': 'D', 'text': 'Venus'}], 'solution': 'B', 'difficulty': 'easy', 'type': 'facts'}, {'question': 'Calculate the average of the following numbers: 2, 4, 8, 16.', 'options': [{'code': 'A', 'text': '7.5'}, {'code': 'B', 'text': '10'}, {'code': 'C', 'text': '15'}, {'code': 'D', 'text': '30'}], 'solution': 'A', 'difficulty': 'easy', 'type': 'math'}, {'question': \"Extract the occupation of the main character from the sentence: 'John, a skilled carpenter, built the house in one year.'\", 'options': [{'code': 'A', 'text': 'Engineer'}, {'code': 'B', 'text': 'Carpenter'}, {'code': 'C', 'text': 'Mason'}, {'code': 'D', 'text': 'Painter'}], 'solution': 'B', 'difficulty': 'easy', 'type': 'extraction'}, {'question': 'If a tree falls in a forest and no one is around to hear it, does it make a sound?', 'options': [{'code': 'A', 'text': 'Yes, it makes a sound'}, {'code': 'B', 'text': 'No, it does not make a sound'}, {'code': 'C', 'text': 'It depends on the definition of sound'}, {'code': 'D', 'text': 'Sound is a human perception'}], 'solution': 'C', 'difficulty': 'hard', 'type': 'reasoning'}, {'question': 'What is the boiling point of water in Celsius?', 'options': [{'code': 'A', 'text': '100 degrees'}, {'code': 'B', 'text': '212 degrees'}, {'code': 'C', 'text': '90 degrees'}, {'code': 'D', 'text': '0 degrees'}], 'solution': 'A', 'difficulty': 'easy', 'type': 'facts'}]\n"
     ]
    }
   ],
   "source": [
    "# Questions df\n",
    "# GPT-4 generation prompt\n",
    "# I am benchmarking an LLM. I want you to create 100 MMLU-style questions. Return them in a JSON array of the format specified below. The questions should be a mix of easy/medium/hard difficulty. \n",
    "# The types should be \"math\", \"extraction\", \"reasoning\", \"facts\". \n",
    "# - \"Math\" questions should be related to arithmetic, calculus, or statistics. \n",
    "# - \"Extraction\" questions should focus on NLP-style NER tasks.\n",
    "# - \"Reasoning\" should focus on logic. \n",
    "# - \"Facts\" should be focused on facts related to science or nature.\n",
    "# Here is an example of a question (do not use this question).\n",
    "# ```\n",
    "# [\n",
    "# {\"question\": \"Suppose you have a data source that generates binary messages. Each message can either be 0 or 1. If both outcomes are equally likely, what is the entropy of this data source?\", \"options\": [{\"code\": \"A\", \"text\": \"0 bits\"}, {\"code\": \"B\", \"text\": \"0.5 bits\"}, {\"code\": \"C\", \"text\": \"1 bit\"}, {\"code\": \"D\", \"text\": \"2 bits\"}], \"solution\": \"C\", \"difficulty\": \"hard\", \"type\": \"math\"},\n",
    "# {\"question\": \"What element is represented by the symbol 'Na' on the periodic table?\", \"options\": [{\"code\": \"A\", \"text\": \"Nitrogen\"}, {\"code\": \"B\", \"text\": \"Nickel\"}, {\"code\": \"C\", \"text\": \"Neon\"}, {\"code\": \"D\", \"text\": \"Sodium\"}], \"solution\": \"D\", \"difficulty\": \"easy\", \"type\": \"facts\"},\n",
    "# ]\n",
    "# ```\n",
    "\n",
    "# Load questions.json\n",
    "q_file_path = os.getcwd() + '/data/question.json'\n",
    "q_file = open(q_file_path)\n",
    "q_list = json.load(q_file) # yields list of dicts \n",
    "\n",
    "\n",
    "\n",
    "# eval_df =\\\n",
    "#     pd.DataFrame(raw_questions)\\\n",
    "#     .assign(\n",
    "#         full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "#         llm_input = lambda df: df.apply(lambda row: parse_phi(base_prompt + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    "#     )\n",
    "\n",
    "# print(len(eval_df))\n",
    "# print(eval_df['llm_input'][0])\n",
    "\n",
    "# eval_df.groupby('difficulty').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
