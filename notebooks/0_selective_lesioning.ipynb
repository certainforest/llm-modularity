{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b92319-88b3-4c7d-91fa-aa605c617cab",
   "metadata": {},
   "source": [
    "# Selective lesioning\n",
    "Now, our aim is to erode a model in a **controlled manner**. Michael Levin's \"multiple levels of competency\" (planaria; evaluate biology by how well it reacts to errors) is a main inspiration. How can higher level layers of \"agents\" resolve errors from lower levels? Why are these systems (biological agents + llm's) so robust? How robust is our model to perturbances? \n",
    "\n",
    "Model performance is tracked by benchmarking over a question set (generated w/ an assist from gpt-4 :)). \n",
    "\n",
    "To do: \n",
    "+ Load base model + replace mps code x\n",
    "+ Pull in question set x\n",
    "+ Write some code to test/track tokens per second\n",
    "+ Establish basic eval framework - need to (1) feed questions (async?); (2) randomly shut off weights from non-embed layers; (3) track perf. changes as culling increases\n",
    "+ think abt ways to selectively kill off weights :) (got some good suggestions from group :D)\n",
    "+ (lower prio) Figure out way to track code efficiency/read up on o complexity \n",
    "\n",
    "Notes: \n",
    "+ Keep an eye on mem. use - disk space can be monitored via `du -hs $HOME /workspace/*` - we have 100GB avail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f59eef-37fe-4ce0-81a0-a4ccde05b7fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "# import flash_attn\n",
    "from dotenv import main\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4908544e-3af6-467c-98de-42664c2b9597",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n",
    "\n",
    "# parse + template phi inputs\n",
    "def parse_phi(messages: list[dict], append_response_start = True) -> str:\n",
    "    \"\"\"\n",
    "    Converts a multi-turn conversation into a Llama-3-tokenizable input.\n",
    "\n",
    "    Output format:\n",
    "    # <s><|system|>\n",
    "    # You are a helpful AI assistant.<|end|>\n",
    "    # <|user|>\n",
    "    # Guess my dog's name!<|end|>\n",
    "    # <|assistant|>\n",
    "    \"\"\"\n",
    "    format = '<s>'\n",
    "    \n",
    "    format += '\\n'.join([f\"<|{m['role']}|>\\n{m['content']}<|end|>\" for m in messages])\n",
    "\n",
    "    if append_response_start:\n",
    "        format += \"\\n<|assistant|>\"\n",
    "    \n",
    "    return format\n",
    "\n",
    "# print(parse_phi([\n",
    "#     {'role': 'system', 'content': 'Hello'}, {'role': 'user', 'content': '1+1?'}, {'role': 'assistant', 'content': '2'}\n",
    "# ], False))\n",
    "\n",
    "# model eval\n",
    "def eval_model(model, tokenizer, prompt):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens = 128,\n",
    "            do_sample = True,\n",
    "            temperature = 0.6,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "        )\n",
    "    return tokenizer.batch_decode(res)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ab92a26d-40c3-4ff4-915f-5c08b74c7522",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33275e45ab24d24abcd3eaf1b09342f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load bnb config, base model, and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id[0],\n",
    "    device_map = 'auto', # not sure what's up with device_map, but this is what causes errors\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "# Load tokenizer - remove bos token since my function already pre-pends\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id[0],\n",
    "                                         add_eos_token = False,\n",
    "                                         add_bos_token = False,\n",
    "                                         padding_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da581cd3-5bd2-4505-8ba3-0c2ce52116e5",
   "metadata": {},
   "source": [
    "# Initial eval. setup\n",
    "Here, we template our questions and run an initial evaluation of phi-3's performance before modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475c41b1-dc6e-48cb-a8ce-63a9f9d95927",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set base prompt \n",
    "base_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, honest, and intelligent AI assistant who can only respond with a single JSON object. Solve each of the following questions. Return a JSON object containing two keys, `rationale` and `answer`.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the integer ceiling of 5/3?\\nA. 3\\nB. 4.25\\nC. dog\\nD. 2\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": '{\"rationale\": \"5/3 is between 1 (3/3) and 2 (6/3), so the integer ceiling is 2.\", \"answer\": \"D\"}'\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the capital of the U.S. state of Georgia?\\nA. Tblisi\\nB. Atlanta\\nC. Nashville\\nD. Toronto\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": '{\"rationale\": \"The capital of the U.S. state of Georgia is Atlanta, located in the Northwest of the state.\", \"answer\": \"B\"}'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa3eed4-46b3-4b7d-9f2e-21a6425cdb39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create eval/questions df \n",
    "# GPT-4 generation prompt\n",
    "# I am benchmarking an LLM. I want you to create 100 MMLU-style questions. Return them in a JSON array of the format specified below. The questions should be a mix of easy/medium/hard difficulty. \n",
    "# The types should be \"math\", \"extraction\", \"reasoning\", \"facts\". \n",
    "# - \"Math\" questions should be related to arithmetic, calculus, or statistics. \n",
    "# - \"Extraction\" questions should focus on NLP-style NER tasks.\n",
    "# - \"Reasoning\" should focus on logic. \n",
    "# - \"Facts\" should be focused on facts related to science or nature.\n",
    "# Here is an example of a question (do not use this question).\n",
    "# ```\n",
    "# [\n",
    "# {\"question\": \"Suppose you have a data source that generates binary messages. Each message can either be 0 or 1. If both outcomes are equally likely, what is the entropy of this data source?\", \"options\": [{\"code\": \"A\", \"text\": \"0 bits\"}, {\"code\": \"B\", \"text\": \"0.5 bits\"}, {\"code\": \"C\", \"text\": \"1 bit\"}, {\"code\": \"D\", \"text\": \"2 bits\"}], \"solution\": \"C\", \"difficulty\": \"hard\", \"type\": \"math\"},\n",
    "# {\"question\": \"What element is represented by the symbol 'Na' on the periodic table?\", \"options\": [{\"code\": \"A\", \"text\": \"Nitrogen\"}, {\"code\": \"B\", \"text\": \"Nickel\"}, {\"code\": \"C\", \"text\": \"Neon\"}, {\"code\": \"D\", \"text\": \"Sodium\"}], \"solution\": \"D\", \"difficulty\": \"easy\", \"type\": \"facts\"},\n",
    "# ]\n",
    "# ```\n",
    "\n",
    "# Load questions.json\n",
    "q_file_path = os.getcwd() + '/data/question.json'\n",
    "q_file = open(q_file_path)\n",
    "q_list = json.load(q_file) # yields list of dicts \n",
    "\n",
    "\n",
    "# create list of dicts, with addtl. keys allocated for full question + llm input\n",
    "eval_df = pd.DataFrame(q_list).assign(\n",
    "     full_question = lambda df: df.apply(lambda row: row['question'] + '\\n' + '\\n'.join([o['code'] + '. ' + o['text'] for o in row['options']]),  axis = 1),\n",
    "     llm_input = lambda df: df.apply(lambda row: parse_phi(base_prompt + [{'role': 'assistant', 'content': row['full_question']}]), axis = 1)\n",
    ")\n",
    "\n",
    "# print(len(eval_df)) \n",
    "# print(eval_df['llm_input'][0]) # check on single input to ensure correct structure :) \n",
    "# eval_df.groupby('difficulty').count() # overall eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6177f90d-74d8-423e-9636-4bdb90fdaf5c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing question 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What element is represented by the symbol 'Na' on the periodic table? \n",
      "\n",
      "\n",
      "Now processing question 3\n",
      "From the text: 'The cat, which was grey, jumped over the sleepy dog.' Identify the color of the cat. \n",
      "\n",
      "\n",
      "Now processing question 4\n",
      "If it is true that all roses are flowers and some flowers fade quickly, which statement must be true? \n",
      "\n",
      "\n",
      "Now processing question 5\n",
      "What planet is known as the 'Red Planet'? \n",
      "\n",
      "\n",
      "Now processing question 7\n",
      "Extract the occupation of the main character from the sentence: 'John, a skilled carpenter, built the house in one year.' \n",
      "\n",
      "\n",
      "Now processing question 8\n",
      "If a tree falls in a forest and no one is around to hear it, does it make a sound? \n",
      "\n",
      "\n",
      "Now processing question 9\n",
      "What is the boiling point of water in Celsius? \n",
      "\n",
      "\n",
      "Now processing question 11\n",
      "From the statement: 'During the night, the shimmering lights of the city are visible from my apartment.', identify the time of day mentioned. \n",
      "\n",
      "\n",
      "Now processing question 12\n",
      "If all cars are vehicles and some vehicles are boats, what is definitely true? \n",
      "\n",
      "\n",
      "Now processing question 13\n",
      "What causes the seasons on Earth? \n",
      "\n",
      "\n",
      "Now processing question 15\n",
      "Analyze the tone of this statement: 'Unfortunately, the event was canceled due to rain.' \n",
      "\n",
      "\n",
      "Now processing question 16\n",
      "Is it logically consistent to claim that all bachelors are unmarried men and some men are married? \n",
      "\n",
      "\n",
      "Now processing question 17\n",
      "Which gas is most commonly associated with the greenhouse effect? \n",
      "\n",
      "\n",
      "Now processing question 19\n",
      "From the description: 'Sally, an experienced botanist, examines the rare orchids.', deduce Sally's profession. \n",
      "\n",
      "\n",
      "Now processing question 20\n",
      "If every square is a rectangle and no rectangle is a circle, which statement is true? \n",
      "\n",
      "\n",
      "Now processing question 21\n",
      "What is the primary source of energy for the Earth's climate system? \n",
      "\n",
      "\n",
      "Now processing question 23\n",
      "Which gas is primarily responsible for the trapping of heat in the Earth's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 24\n",
      "Identify the subject in the sentence: 'Quickly, the leopard pounced on its unsuspecting prey.' \n",
      "\n",
      "\n",
      "Now processing question 25\n",
      "How many moons does Jupiter have? \n",
      "\n",
      "\n",
      "Now processing question 27\n",
      "Extract the main verb from the sentence: 'The committee awards the prize annually.' \n",
      "\n",
      "\n",
      "Now processing question 29\n",
      "What chemical element is diamond primarily composed of? \n",
      "\n",
      "\n",
      "Now processing question 30\n",
      "From the description: 'The lawyer argued the case with great zeal.', identify the profession of the main character. \n",
      "\n",
      "\n",
      "Now processing question 31\n",
      "If Tom is older than Mary, and Mary is older than Ann, who is the youngest? \n",
      "\n",
      "\n",
      "Now processing question 32\n",
      "What is the most abundant gas in the Earth's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 34\n",
      "Extract the place mentioned in the sentence: 'The Eiffel Tower in Paris attracts millions of visitors annually.' \n",
      "\n",
      "\n",
      "Now processing question 35\n",
      "If all swans we have seen are white, what is the most logical conclusion? \n",
      "\n",
      "\n",
      "Now processing question 36\n",
      "Which layer of the Earth is composed mostly of iron and nickel? \n",
      "\n",
      "\n",
      "Now processing question 38\n",
      "Determine the theme from the sentence: 'Despite severe adversity, the community remained optimistic and hopeful.' \n",
      "\n",
      "\n",
      "Now processing question 39\n",
      "Which reasoning type is used when deriving specific conclusions from general statements? \n",
      "\n",
      "\n",
      "Now processing question 40\n",
      "What is the primary reason for the seasons on Earth? \n",
      "\n",
      "\n",
      "Now processing question 42\n",
      "From the statement: 'Dr. Wilson, an acclaimed neurosurgeon, performed the complex surgery.', identify Dr. Wilson's specialty. \n",
      "\n",
      "\n",
      "Now processing question 43\n",
      "If all poets are dreamers and no dreamers are punctual, which statement is true? \n",
      "\n",
      "\n",
      "Now processing question 44\n",
      "Which hormone regulates the blood sugar level? \n",
      "\n",
      "\n",
      "Now processing question 46\n",
      "From the text: 'Global warming is attributed to increased greenhouse gases.', extract the cause of global warming. \n",
      "\n",
      "\n",
      "Now processing question 47\n",
      "Using modus tollens, if 'if it rains, the ground is wet' and 'the ground is not wet', what can be concluded? \n",
      "\n",
      "\n",
      "Now processing question 48\n",
      "What causes ocean tides? \n",
      "\n",
      "\n",
      "Now processing question 50\n",
      "From the description: 'Samantha, an expert in renewable energy, spoke at the conference.', deduce Samantha's field of expertise. \n",
      "\n",
      "\n",
      "Now processing question 51\n",
      "Given that no square is round and all circles are round, what can be concluded? \n",
      "\n",
      "\n",
      "Now processing question 52\n",
      "What is the primary function of chlorophyll in plants? \n",
      "\n",
      "\n",
      "Now processing question 54\n",
      "Identify the main action from this sentence: 'The government implemented new regulations to reduce pollution.' \n",
      "\n",
      "\n",
      "Now processing question 55\n",
      "If a statement and its contrapositive are both true, what can be concluded about the original statement? \n",
      "\n",
      "\n",
      "Now processing question 56\n",
      "What percentage of the Earth's atmosphere is composed of nitrogen? \n",
      "\n",
      "\n",
      "Now processing question 58\n",
      "Extract the time phrase from this sentence: 'The store opens at 9:00 AM every day except Sundays.' \n",
      "\n",
      "\n",
      "Now processing question 59\n",
      "Which logical fallacy involves attacking the character of the person making an argument rather than the argument itself? \n",
      "\n",
      "\n",
      "Now processing question 60\n",
      "Which layer of the Earth's atmosphere contains the ozone layer? \n",
      "\n",
      "\n",
      "Now processing question 62\n",
      "Which gas makes up the majority of Earth's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 64\n",
      "What is the freezing point of water? \n",
      "\n",
      "\n",
      "Now processing question 66\n",
      "What is the capital of Japan? \n",
      "\n",
      "\n",
      "Now processing question 68\n",
      "How many hours are there in three days? \n",
      "\n",
      "\n",
      "Now processing question 69\n",
      "Identify the verb in the sentence: 'The bird sings beautifully.' \n",
      "\n",
      "\n",
      "Now processing question 70\n",
      "What vitamin is known as ascorbic acid? \n",
      "\n",
      "\n",
      "Now processing question 72\n",
      "Name the largest planet in our solar system. \n",
      "\n",
      "\n",
      "Now processing question 74\n",
      "From the information: 'Alice, a software developer, writes clean code.', identify Alice's profession. \n",
      "\n",
      "\n",
      "Now processing question 75\n",
      "Identify the natural satellite of the Earth. \n",
      "\n",
      "\n",
      "Now processing question 77\n",
      "From the novel description: 'In the story, Elizabeth moves to London to become a chef, escaping her small town life.', extract her new profession. \n",
      "\n",
      "\n",
      "Now processing question 79\n",
      "From the description: 'James, a renowned scientist, discovered a new species of butterfly.', deduce James' profession. \n",
      "\n",
      "\n",
      "Now processing question 81\n",
      "What is the largest organ in the human body? \n",
      "\n",
      "\n",
      "Now processing question 83\n",
      "Extract the location mentioned in the sentence: 'The Great Wall of China stretches over 13,000 miles.' \n",
      "\n",
      "\n",
      "Now processing question 84\n",
      "In propositional logic, what is the contrapositive of 'If p, then q'? \n",
      "\n",
      "\n",
      "Now processing question 85\n",
      "What is the process by which plants make their own food called? \n",
      "\n",
      "\n",
      "Now processing question 87\n",
      "Which planet is known as the 'Red Planet'? \n",
      "\n",
      "\n",
      "Now processing question 89\n",
      "Which gas is produced during photosynthesis? \n",
      "\n",
      "\n",
      "Now processing question 91\n",
      "Which continent is known as the 'Land of Fire and Ice'? \n",
      "\n",
      "\n",
      "Now processing question 93\n",
      "Which element has the chemical symbol 'H'? \n",
      "\n",
      "\n",
      "Now processing question 95\n",
      "Which gas do plants absorb from the air for photosynthesis? \n",
      "\n",
      "\n",
      "Now processing question 98\n",
      "What is the capital of France? \n",
      "\n",
      "\n",
      "Now processing question 101\n",
      "In the sentence 'Samantha brought her new puppy to the park', identify the proper noun. \n",
      "\n",
      "\n",
      "Now processing question 102\n",
      "If five cats can catch five mice in five minutes, how many cats are needed to catch 100 mice in 100 minutes? \n",
      "\n",
      "\n",
      "Now processing question 103\n",
      "What is the most abundant gas in the Earth's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 105\n",
      "From the document, extract the organization mentioned: 'NASA plans to send astronauts to Mars by 2030.' \n",
      "\n",
      "\n",
      "Now processing question 106\n",
      "Is the following statement true or false? 'A square is always a rectangle.' \n",
      "\n",
      "\n",
      "Now processing question 107\n",
      "Which planet is known as the 'Red Planet'? \n",
      "\n",
      "\n",
      "Now processing question 109\n",
      "Identify the location mentioned in the text: 'The conference was held in Geneva.' \n",
      "\n",
      "\n",
      "Now processing question 110\n",
      "What is the missing number in the sequence 2, 4, 8, 16, __? \n",
      "\n",
      "\n",
      "Now processing question 111\n",
      "Which element is used in the filaments of light bulbs? \n",
      "\n",
      "\n",
      "Now processing question 113\n",
      "Extract the person's name from the sentence: 'Dr. Emily Stone won the Nobel Prize in Chemistry.' \n",
      "\n",
      "\n",
      "Now processing question 114\n",
      "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? \n",
      "\n",
      "\n",
      "Now processing question 115\n",
      "What gas do plants absorb from the environment to perform photosynthesis? \n",
      "\n",
      "\n",
      "Now processing question 117\n",
      "From the following sentence, identify the date mentioned: 'The contract expires on June 30, 2023.' \n",
      "\n",
      "\n",
      "Now processing question 118\n",
      "If it takes 6 minutes to saw a log into 3 pieces, how long will it take to saw the same log into 6 pieces? \n",
      "\n",
      "\n",
      "Now processing question 119\n",
      "What vitamin is primarily obtained from sunlight exposure? \n",
      "\n",
      "\n",
      "Now processing question 121\n",
      "Identify the numerical value in the sentence: 'There are 24 hours in a day.' \n",
      "\n",
      "\n",
      "Now processing question 122\n",
      "A man is looking at a photograph of someone. His friend asks who it is. The man replies, 'Brothers and sisters, I have none. But this man's father is my father's son.' Who is the man in the photograph? \n",
      "\n",
      "\n",
      "Now processing question 123\n",
      "Which of these animals is a mammal? 'Shark, Dolphin, Octopus, Crocodile' \n",
      "\n",
      "\n",
      "Now processing question 125\n",
      "Extract the time mentioned in the following sentence: 'The meeting is scheduled to start at 3:00 PM.' \n",
      "\n",
      "\n",
      "Now processing question 126\n",
      "Which statement about even and odd numbers is correct? \n",
      "\n",
      "\n",
      "Now processing question 128\n",
      "From the article titled 'Innovations in Artificial Intelligence', extract all mentioned AI techniques. \n",
      "\n",
      "\n",
      "Now processing question 129\n",
      "If all Z are Y, and no Y are X, what can be concluded about Z and X? \n",
      "\n",
      "\n",
      "Now processing question 130\n",
      "What is the approximate mass of the Milky Way galaxy in solar masses? \n",
      "\n",
      "\n",
      "Now processing question 132\n",
      "Identify all animal species mentioned in the text: 'The Serengeti is home to lions, zebras, and elephants.' \n",
      "\n",
      "\n",
      "Now processing question 133\n",
      "If a proposition is true whenever its converse is true, what can be inferred about the proposition? \n",
      "\n",
      "\n",
      "Now processing question 134\n",
      "What is the critical temperature of water at 1 atmospheric pressure? \n",
      "\n",
      "\n",
      "Now processing question 136\n",
      "From the book summary, extract the main themes discussed in '1984' by George Orwell. \n",
      "\n",
      "\n",
      "Now processing question 137\n",
      "Which logical fallacy involves attacking the person instead of the argument? \n",
      "\n",
      "\n",
      "Now processing question 138\n",
      "What percentage of Earth's atmosphere is composed of argon? \n",
      "\n",
      "\n",
      "Now processing question 140\n",
      "Identify all chemical elements mentioned in the following sentence: 'Water is composed of hydrogen and oxygen.' \n",
      "\n",
      "\n",
      "Now processing question 141\n",
      "If two angles of a triangle are 45 degrees each, what is the nature of the third angle? \n",
      "\n",
      "\n",
      "Now processing question 142\n",
      "What is the lifespan of a typical neutron star? \n",
      "\n",
      "\n",
      "Now processing question 144\n",
      "Extract the historical figures mentioned in 'Napoleon and Julius Caesar were influential leaders.' \n",
      "\n",
      "\n",
      "Now processing question 145\n",
      "Which reasoning process involves starting from general premises and moving to specific conclusions? \n",
      "\n",
      "\n",
      "Now processing question 146\n",
      "How many known stable isotopes does hydrogen have? \n",
      "\n",
      "\n",
      "Now processing question 148\n",
      "From a report titled 'Global Warming Effects', list the natural disasters mentioned. \n",
      "\n",
      "\n",
      "Now processing question 149\n",
      "If every painter uses a brush and some artists use brushes, can we conclude some artists are painters? \n",
      "\n",
      "\n",
      "Now processing question 150\n",
      "What temperature does water boil at on top of Mount Everest? \n",
      "\n",
      "\n",
      "Now processing question 152\n",
      "Extract all tech companies mentioned in: 'Google, Apple, and Amazon dominate the tech industry.' \n",
      "\n",
      "\n",
      "Now processing question 154\n",
      "Identify all forms of energy mentioned in: 'Solar and wind are key to sustainable energy.' \n",
      "\n",
      "\n",
      "Now processing question 155\n",
      "What is the pH level of pure water at 25°C? \n",
      "\n",
      "\n",
      "Now processing question 157\n",
      "From the statement 'John is taller than Jake, and Jake is taller than Jim', what can be inferred about John and Jim? \n",
      "\n",
      "\n",
      "Now processing question 158\n",
      "What is the speed of light in vacuum in meters per second? \n",
      "\n",
      "\n",
      "Now processing question 160\n",
      "Extract the main economic sectors mentioned in: 'Agriculture and technology drive the economy.' \n",
      "\n",
      "\n",
      "Now processing question 161\n",
      "Which logical operation is represented by the NOR gate? \n",
      "\n",
      "\n",
      "Now processing question 162\n",
      "What is the atomic number of uranium? \n",
      "\n",
      "\n",
      "Now processing question 164\n",
      "From the description 'Leonardo da Vinci was a painter, inventor, and scientist.', extract the professions. \n",
      "\n",
      "\n",
      "Now processing question 165\n",
      "If a function is continuous everywhere and differentiable nowhere, what is it typically called? \n",
      "\n",
      "\n",
      "Now processing question 166\n",
      "How many moons does Jupiter have? \n",
      "\n",
      "\n",
      "Now processing question 167\n",
      "Extract all historical events mentioned in the text: 'The French Revolution and the Industrial Revolution changed society dramatically.' \n",
      "\n",
      "\n",
      "Now processing question 168\n",
      "If a statement is considered 'tautology', what does this imply about its truth value? \n",
      "\n",
      "\n",
      "Now processing question 169\n",
      "What element has the highest melting point? \n",
      "\n",
      "\n",
      "Now processing question 170\n",
      "Identify all living organisms mentioned in 'Whales and dolphins are intelligent marine mammals.' \n",
      "\n",
      "\n",
      "Now processing question 171\n",
      "If it is known that 'All that glitters is not gold', what can be inferred about something that glitters? \n",
      "\n",
      "\n",
      "Now processing question 172\n",
      "Which planet has the most moons? \n",
      "\n",
      "\n",
      "Now processing question 173\n",
      "From the document '2022 Economic Forecast', extract the key industries expected to grow. \n",
      "\n",
      "\n",
      "Now processing question 174\n",
      "In logic, what is the fallacy where the conclusion does not follow from the premises? \n",
      "\n",
      "\n",
      "Now processing question 175\n",
      "What is the most abundant gas in the Earth's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 176\n",
      "Extract the main concerns from the article 'Climate Change in the 21st Century'. \n",
      "\n",
      "\n",
      "Now processing question 177\n",
      "What logical operation is performed by the XOR gate? \n",
      "\n",
      "\n",
      "Now processing question 178\n",
      "Which gas is a primary component of Venus's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 179\n",
      "From the novel description, identify the main character in 'To Kill a Mockingbird'. \n",
      "\n",
      "\n",
      "Now processing question 180\n",
      "What is the fallacy of affirming the consequent in logical argumentation? \n",
      "\n",
      "\n",
      "Now processing question 181\n",
      "What percentage of Earth's water is fresh water? \n",
      "\n",
      "\n",
      "Now processing question 182\n",
      "Identify the diseases mentioned in 'Recent studies show increases in diabetes and heart disease rates.' \n",
      "\n",
      "\n",
      "Now processing question 183\n",
      "Which reasoning method involves drawing specific conclusions from general statements? \n",
      "\n",
      "\n",
      "Now processing question 184\n",
      "Which bird is known for having the largest wingspan? \n",
      "\n",
      "\n",
      "Now processing question 185\n",
      "Extract the future trends discussed in the report 'Technology Trends 2030'. \n",
      "\n",
      "\n",
      "Now processing question 186\n",
      "What is the logical fallacy of assuming the conclusion in the premise? \n",
      "\n",
      "\n",
      "Now processing question 187\n",
      "Extract the key products mentioned in the document 'Annual Tech Innovations': 'This year's innovations include smartwatches and wireless earbuds.' \n",
      "\n",
      "\n",
      "Now processing question 188\n",
      "If 'All roses are flowers' and 'Some flowers fade quickly', what can be inferred about roses? \n",
      "\n",
      "\n",
      "Now processing question 189\n",
      "Identify the genres mentioned in the movie review for 'Epic Adventures': 'The film blends action and comedy in a thrilling ride.' \n",
      "\n",
      "\n",
      "Now processing question 190\n",
      "What logical fallacy is present when an argument is dismissed based solely on someone's circumstances? \n",
      "\n",
      "\n",
      "Now processing question 191\n",
      "From the speech 'Future of Education', extract all technologies mentioned: 'Virtual reality and augmented reality can transform learning experiences.' \n",
      "\n",
      "\n",
      "Now processing question 192\n",
      "If it is given that 'No birds are mammals' and 'All sparrows are birds', what must be true about sparrows? \n",
      "\n",
      "\n",
      "Now processing question 193\n",
      "Extract the main topics discussed in the article 'Health Trends 2025': 'Emerging trends include telemedicine and personalized medicine.' \n",
      "\n",
      "\n",
      "Now processing question 194\n",
      "Which reasoning error involves concluding that one event causes another simply because they occur sequentially? \n",
      "\n",
      "\n",
      "Now processing question 195\n",
      "Identify the problems mentioned in the report 'Climate Risk Assessment': 'The risks include rising sea levels and increased hurricane activity.' \n",
      "\n",
      "\n",
      "Now processing question 196\n",
      "What is the logical error called when an arguer repeats the argument rather than proving it? \n",
      "\n",
      "\n",
      "Now processing question 197\n",
      "What is the flaw in the argument: 'If it rains, the ground is wet. The ground is wet, therefore it must have rained.' \n",
      "\n",
      "\n",
      "Now processing question 198\n",
      "If a conclusion can be drawn from two premises, 'All flowers are plants' and 'Some plants thrive in sunlight', what is the most appropriate logical inference? \n",
      "\n",
      "\n",
      "Now processing question 199\n",
      "Analyzing the argument: 'Nobody has ever proven that extraterrestrials exist, so they must not exist.' What is this fallacy called? \n",
      "\n",
      "\n",
      "Now processing question 201\n",
      "From the document titled 'Famous Scientists in History', extract the names of all scientists mentioned. \n",
      "\n",
      "\n",
      "Now processing question 202\n",
      "If some cats are mammals and all mammals have hearts, what must be true? \n",
      "\n",
      "\n",
      "Now processing question 203\n",
      "Which chemical element has the highest electronegativity? \n",
      "\n",
      "\n",
      "Now processing question 205\n",
      "Identify the economic indicators mentioned in 'Global Economic Report': 'The GDP and unemployment rate are key indicators.' \n",
      "\n",
      "\n",
      "Now processing question 206\n",
      "What logical error is present when a conclusion is based on a small or biased sample? \n",
      "\n",
      "\n",
      "Now processing question 207\n",
      "Which gas is the most abundant in the sun's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 209\n",
      "From the article 'Technological Advances of the 21st Century', extract the key advancements. \n",
      "\n",
      "\n",
      "Now processing question 210\n",
      "If 'All roses are flowers' and 'Some flowers fade quickly', what can be inferred about roses? \n",
      "\n",
      "\n",
      "Now processing question 211\n",
      "What is the boiling point of helium at standard atmospheric pressure? \n",
      "\n",
      "\n",
      "Now processing question 213\n",
      "Extract the technologies mentioned in the paper 'Emerging Tech in Healthcare': 'The report highlights telemedicine and AI as transformative technologies.' \n",
      "\n",
      "\n",
      "Now processing question 214\n",
      "If the premise 'All bachelors are unmarried' is true, what is the logical conclusion for 'John is a bachelor'? \n",
      "\n",
      "\n",
      "Now processing question 215\n",
      "Which element has the highest density? \n",
      "\n",
      "\n",
      "Now processing question 217\n",
      "Identify the characters mentioned in 'Shakespeare's Tragic Heroes': 'Hamlet, Macbeth, and Othello are among Shakespeare's greatest tragic heroes.' \n",
      "\n",
      "\n",
      "Now processing question 218\n",
      "Which logical fallacy involves dismissing an argument based on irrelevant personal characteristics of the arguer? \n",
      "\n",
      "\n",
      "Now processing question 219\n",
      "Which element has the smallest atomic radius? \n",
      "\n",
      "\n",
      "Now processing question 221\n",
      "From the book 'Leaders of the 20th Century', extract the names of all leaders who were primarily involved in the Cold War. \n",
      "\n",
      "\n",
      "Now processing question 222\n",
      "If 'All M are P' and 'No S are P', what can be concluded about M and S? \n",
      "\n",
      "\n",
      "Now processing question 223\n",
      "What is the atomic mass of plutonium? \n",
      "\n",
      "\n",
      "Now processing question 225\n",
      "Identify the medical procedures mentioned in 'Innovations in Cardiac Surgery': 'Recent advancements include minimally invasive surgeries and robotic assistance.' \n",
      "\n",
      "\n",
      "Now processing question 226\n",
      "What is the logical fallacy where a conclusion is reached based on popularity? \n",
      "\n",
      "\n",
      "Now processing question 227\n",
      "Which gas is predominantly responsible for the greenhouse effect on Earth? \n",
      "\n",
      "\n",
      "Now processing question 229\n",
      "Extract the critical financial terms mentioned in 'Economic Impact of Global Pandemics': 'The analysis highlights GDP contraction and inflation spikes.' \n",
      "\n",
      "\n",
      "Now processing question 230\n",
      "Which reasoning fallacy involves arguing that a lack of evidence proves something is true or false? \n",
      "\n",
      "\n",
      "Now processing question 231\n",
      "What is the density of diamond? \n",
      "\n",
      "\n",
      "Now processing question 233\n",
      "From the policy brief 'Climate Change Strategies', identify the main strategies discussed for reducing carbon emissions. \n",
      "\n",
      "\n",
      "Now processing question 234\n",
      "If a hypothetical imperative states that 'if you want to win, you must train', what is the logical implication if you are training? \n",
      "\n",
      "\n",
      "Now processing question 235\n",
      "What is the most abundant element in the universe? \n",
      "\n",
      "\n",
      "Now processing question 237\n",
      "Identify the key pollutants listed in 'Urban Air Quality Review': 'Significant concerns include particulate matter and nitrogen oxides.' \n",
      "\n",
      "\n",
      "Now processing question 238\n",
      "What is the error called when someone uses a personal experience rather than a broader statistical evidence as proof? \n",
      "\n",
      "\n",
      "Now processing question 239\n",
      "What temperature must water reach at sea level to boil? \n",
      "\n",
      "\n",
      "Now processing question 240\n",
      "From the novel 'Great Expectations', extract the main character's name. \n",
      "\n",
      "\n",
      "Now processing question 241\n",
      "If 'All fruits are healthy' and 'Apples are fruits', what can be concluded about apples? \n",
      "\n",
      "\n",
      "Now processing question 242\n",
      "Which planet is known for its extensive ring system? \n",
      "\n",
      "\n",
      "Now processing question 243\n",
      "Identify the historical figures mentioned in 'The Rise of Industrial America': 'Key figures include Andrew Carnegie and John D. Rockefeller.' \n",
      "\n",
      "\n",
      "Now processing question 244\n",
      "What logical fallacy is committed when an arguer diverts attention from the argument by changing the subject? \n",
      "\n",
      "\n",
      "Now processing question 245\n",
      "What is the primary gas in Earth's atmosphere? \n",
      "\n",
      "\n",
      "Now processing question 246\n",
      "Extract the main topic discussed in 'Modern Architectural Trends': 'Focus areas include sustainability and use of renewable resources.' \n",
      "\n",
      "\n",
      "Now processing question 247\n",
      "If a hypothesis states that 'Adding fertilizer increases plant growth' and plants with fertilizer show no growth difference, what is the implication? \n",
      "\n",
      "\n",
      "Now processing question 248\n",
      "Which element is essential for the construction of nuclear reactors due to its ability to absorb neutrons? \n",
      "\n",
      "\n",
      "Now processing question 249\n",
      "From the documentary 'Ocean Life', identify the sea creatures highlighted for their intelligence: 'Octopuses and dolphins are noted for their problem-solving skills.' \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: espeak: not found\n"
     ]
    }
   ],
   "source": [
    "# question + answer - generate validation dictionary \n",
    "# is it better to keep eval_df as a list of dicts like prev? isn't iterrows() slightly less efficient? \n",
    "curated_eval_df = eval_df[eval_df['type'] != 'math'] # have to remove math for now, failure rates too high; prob. issue w/ trying to output latex etc.\n",
    "\n",
    "val = []\n",
    "for idx, row in curated_eval_df.iterrows(): # limit to first 10 rows for now during testing \n",
    "    print(f\"Now processing question {idx}\") \n",
    "\n",
    "    # answer validation \n",
    "    keep_going = True \n",
    "    \n",
    "    while keep_going == True: \n",
    "        # generate response \n",
    "        response = eval_model(model = base_model, tokenizer = tokenizer, prompt = row['llm_input'])\n",
    "        # print(response)\n",
    "\n",
    "        # error handling for malformed outputs \n",
    "        response_json = re.findall(r'(?=.*\"rationale\")(?=.*\"answer\"){.*?}', response)[-1] # extract response + json\n",
    "\n",
    "        # initialize keep_going + check if response_json is empty list \n",
    "        try:\n",
    "            response_dict = json.loads(response_json)\n",
    "            \n",
    "            # validate model preds against correct answer \n",
    "            if response_dict['answer'] == row['solution']:\n",
    "                # print('✅ Good answer - 😎👍')\n",
    "                is_correct_pred = 1\n",
    "            elif response_dict['answer'] != row['solution']: \n",
    "                # print('❌ Wrong answer!!') \n",
    "                is_correct_pred = 0\n",
    "                \n",
    "            # validation dictionary \n",
    "            val_dict = {'question': row['question'], 'response': response_json,\n",
    "                        'difficulty': row['difficulty'],\n",
    "                        'answer': response_dict['answer'],\n",
    "                        'rationale': response_dict['rationale'],\n",
    "                        'correct_solution': row['solution'],\n",
    "                        'is_correct_pred': is_correct_pred} \n",
    "            print(val_dict['question'], '\\n\\n')\n",
    "            val.append(val_dict)\n",
    "            keep_going = False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "\n",
    "# notify when execution finishes\n",
    "text_to_speech(\"Hello, responses are done generating!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c008e3-9528-4047-838a-1ee04c1b4335",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_responses = 188\n",
      "accuracy: 0.675531914893617\n"
     ]
    }
   ],
   "source": [
    "# prediction summary (allow this to serve as control - eventually will want to store output more formally :))\n",
    "val_df = pd.DataFrame(val)\n",
    "\n",
    "# metrics \n",
    "n_responses = len(val_df)\n",
    "accuracy = sum(val_df['is_correct_pred'])/n_responses\n",
    "\n",
    "print(f\"n_responses = {n_responses}\\naccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa40d7-414b-453f-8431-86e269f02521",
   "metadata": {},
   "source": [
    "# Performance under perturbance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43268cb4-0552-4087-9ba9-c408dcce67be",
   "metadata": {},
   "source": [
    "There are two steps here: **(1) need to identify phi-3 activation layers** (can do this by doing a forward pass, storing output, and looking at output distribution) and **(2) determine/carry out weight culling using a given method** (e.g. randomly killing weights, sort weights by magnitude and kill the smallest first, forward passes over multiple inputs + look at average firing for neurons and cull the weights feeding into neurons with low activation across inputs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "360a38d0-1dc0-4dc0-b1f9-8efc428c7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify phi-3 activation layers \n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "\n",
    "    # store layer names (for testing) \n",
    "    layer_names.append({'idx': idx, 'name': name})\n",
    "\n",
    "# view layers \n",
    "# pd.DataFrame(layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacd118-3102-4207-80c1-ef843315ee2b",
   "metadata": {},
   "source": [
    "# Tracking activations (w/ forward hooks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea3aa5cf-2eb9-4252-b378-6ccfdde89cff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m h1 \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(getActivation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_attn\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# tk - question about how we know what comprises a layer? what's the right level of hierarchy to discuss at?\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# forward pass + store activations for this pass - this step has an issue; 'tuple object has no attribute detach'\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m test_response \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print activation \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(activation)\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, tokenizer, prompt)\u001b[0m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(res)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1679\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1672\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1673\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1674\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1675\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1676\u001b[0m     )\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1679\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1693\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1694\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1699\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1700\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2468\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2465\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2467\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2468\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/920b6cf52a79ecff578cc33f61922b23cbc88115/modeling_phi3.py:1286\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1283\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1299\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/920b6cf52a79ecff578cc33f61922b23cbc88115/modeling_phi3.py:1164\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1155\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1156\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         use_cache,\n\u001b[1;32m   1162\u001b[0m     )\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m, in \u001b[0;36mgetActivation.<locals>.hook\u001b[0;34m(model, input, output)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhook\u001b[39m(model, \u001b[38;5;28minput\u001b[39m, output): \n\u001b[0;32m----> 8\u001b[0m     activation[name] \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# store activations \n",
    "activation = {} \n",
    "\n",
    "# define function to capture activations \n",
    "def getActivation(name): \n",
    "    # hook signature \n",
    "    def hook(model, input, output): \n",
    "        activation[name] = output.detach() \n",
    "    return hook\n",
    "\n",
    "# set a single, sample input \n",
    "test_input = eval_df['llm_input'][0]\n",
    "\n",
    "# register forward hooks on a chosen layer - let's choose model.layers.0.self_attn.o_proj.weight for now :) \n",
    "h1 = base_model.model.layers[0].register_forward_hook(getActivation('self_attn')) # tk - question about how we know what comprises a layer? what's the right level of hierarchy to discuss at?\n",
    "\n",
    "# forward pass + store activations for this pass - this step has an issue; 'tuple object has no attribute detach'\n",
    "test_response = eval_model(model = base_model, tokenizer = tokenizer, prompt = test_input)[0]\n",
    "\n",
    "# print activation \n",
    "print(activation)\n",
    "\n",
    "# detach hooks \n",
    "h1.remove()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78304bfc-f2b8-4bb9-bbbf-b88defb88ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to zero out like 10% of a single layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4bd49a63-914f-40e4-bd81-4f100f1dbafd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn Phi3Attention(\n",
      "  (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "  (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
      "  (rotary_emb): Phi3RotaryEmbedding()\n",
      ")\n",
      "mlp Phi3MLP(\n",
      "  (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "  (activation_fn): SiLU()\n",
      ")\n",
      "input_layernorm Phi3RMSNorm()\n",
      "resid_attn_dropout Dropout(p=0.0, inplace=False)\n",
      "resid_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "post_attention_layernorm Phi3RMSNorm()\n"
     ]
    }
   ],
   "source": [
    "activation = {}\n",
    "for name, child in base_model.model.layers[0].named_children(): \n",
    "    print(name, child)\n",
    "    if name == 'mlp': \n",
    "        my_child_h1 = child.register_forward_hook(getActivation('mlp'))\n",
    "\n",
    "\n",
    "# detach hooks at end to prevent memory errors \n",
    "\n",
    "outputs = []\n",
    "with torch.no_grad(): \n",
    "    # base_model('banana')\n",
    "    bn_tok = tokenizer('banana', return_tensors = 'pt').to(device)\n",
    "    outputs.append(base_model(**bn_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ff23ef3b-621e-4b3b-a8f9-06f918d7b9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "mlp_output = activation['mlp'].squeeze().cpu().numpy().flatten() # squeeze to remove batch dim.; flatten - necessary for plotting \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a8d76d1b-ba22-443c-8664-fb7c49195505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.2   , -1.294 , -5.566 , ...,  1.135 ,  1.343 ,  0.2869],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_output*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b8bdc2f9-ddfd-441d-a446-0622fee30247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 1.000e+00,\n",
       "        7.810e+02, 5.348e+03, 6.000e+00, 2.000e+00, 0.000e+00, 1.000e+00,\n",
       "        0.000e+00, 1.000e+00]),\n",
       " array([-214.        , -197.75      , -181.5       , -165.25      ,\n",
       "        -149.        , -132.75      , -116.5       , -100.25      ,\n",
       "         -84.        ,  -67.75      ,  -51.53125   ,  -35.28125   ,\n",
       "         -19.03125   ,   -2.79101562,   13.453125  ,   29.703125  ,\n",
       "          45.9375    ,   62.1875    ,   78.4375    ,   94.6875    ,\n",
       "         110.9375    ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mlp_output*100, bins = 20, alpha = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9db228c3-7ad8-487e-9908-28c4530c8f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_proj Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "qkv_proj Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
      "rotary_emb Phi3RotaryEmbedding()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear4bit(in_features=3072, out_features=3072, bias=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get activations for internal layers do below\n",
    "for name, child in my_child.named_children(): \n",
    "    print(name, child)\n",
    "    if name == 'o_proj': \n",
    "        my_grandie_h1 = child.register_forward_hook(getActivation('o_proj'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e4bd9c36-edea-4fc3-9f6f-7553e8547fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f3205b18d60>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_grandie_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe11aab2-5f97-415a-bb06-7b5b65d04bdc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'named_children'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for module in base_model.model.layers[0].modules(): \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     print(module)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m layer \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_parameters()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_children\u001b[49m():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name, module)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'named_children'"
     ]
    }
   ],
   "source": [
    "# for module in base_model.model.layers[0].modules(): \n",
    "#     print(module)\n",
    "\n",
    "layer = base_model.model.named_parameters()\n",
    "for name, module in layer.named_children():\n",
    "    print(name, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1f4498f-c546-496b-b7f1-efdcdde9a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 model.embed_tokens.weight Parameter containing:\n",
      "tensor([[-5.8594e-02, -4.0894e-03,  1.5564e-03,  ..., -2.3438e-02,\n",
      "          3.8818e-02, -5.9082e-02],\n",
      "        [-3.0273e-02,  9.1309e-02,  5.6152e-02,  ...,  1.0132e-02,\n",
      "         -2.1606e-02, -2.4170e-02],\n",
      "        [-3.3264e-03,  3.1982e-02,  9.0942e-03,  ...,  9.4414e-05,\n",
      "         -6.9275e-03, -2.7832e-02],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "1 model.layers.0.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[148],\n",
      "            [158],\n",
      "            [191],\n",
      "            ...,\n",
      "            [ 99],\n",
      "            [ 62],\n",
      "            [212]], device='cuda:0', dtype=torch.uint8))\n",
      "2 model.layers.0.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 53],\n",
      "            [231],\n",
      "            [229],\n",
      "            ...,\n",
      "            [143],\n",
      "            [ 82],\n",
      "            [103]], device='cuda:0', dtype=torch.uint8))\n",
      "3 model.layers.0.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[126],\n",
      "            [ 53],\n",
      "            [134],\n",
      "            ...,\n",
      "            [139],\n",
      "            [ 87],\n",
      "            [157]], device='cuda:0', dtype=torch.uint8))\n",
      "4 model.layers.0.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 71],\n",
      "            [165],\n",
      "            [169],\n",
      "            ...,\n",
      "            [109],\n",
      "            [ 94],\n",
      "            [ 45]], device='cuda:0', dtype=torch.uint8))\n",
      "5 model.layers.0.input_layernorm.weight Parameter containing:\n",
      "tensor([0.0061, 0.0043, 0.0070,  ..., 0.0107, 0.0381, 0.0101], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "6 model.layers.0.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.0493, 0.0422, 0.0371,  ..., 0.0437, 0.0623, 0.0442], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "7 model.layers.1.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[245],\n",
      "            [ 90],\n",
      "            [ 57],\n",
      "            ...,\n",
      "            [ 94],\n",
      "            [136],\n",
      "            [ 68]], device='cuda:0', dtype=torch.uint8))\n",
      "8 model.layers.1.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[225],\n",
      "            [ 42],\n",
      "            [  6],\n",
      "            ...,\n",
      "            [ 41],\n",
      "            [ 61],\n",
      "            [108]], device='cuda:0', dtype=torch.uint8))\n",
      "9 model.layers.1.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[199],\n",
      "            [104],\n",
      "            [151],\n",
      "            ...,\n",
      "            [ 57],\n",
      "            [ 89],\n",
      "            [150]], device='cuda:0', dtype=torch.uint8))\n",
      "10 model.layers.1.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[132],\n",
      "            [114],\n",
      "            [187],\n",
      "            ...,\n",
      "            [ 89],\n",
      "            [106],\n",
      "            [151]], device='cuda:0', dtype=torch.uint8))\n",
      "11 model.layers.1.input_layernorm.weight Parameter containing:\n",
      "tensor([0.0195, 0.0269, 0.0159,  ..., 0.0200, 0.0238, 0.1611], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "12 model.layers.1.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.1021, 0.0933, 0.0947,  ..., 0.0972, 0.0991, 0.0649], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "13 model.layers.2.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[118],\n",
      "            [135],\n",
      "            [129],\n",
      "            ...,\n",
      "            [184],\n",
      "            [194],\n",
      "            [248]], device='cuda:0', dtype=torch.uint8))\n",
      "14 model.layers.2.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[100],\n",
      "            [ 55],\n",
      "            [102],\n",
      "            ...,\n",
      "            [101],\n",
      "            [202],\n",
      "            [ 85]], device='cuda:0', dtype=torch.uint8))\n",
      "15 model.layers.2.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[167],\n",
      "            [ 97],\n",
      "            [ 11],\n",
      "            ...,\n",
      "            [104],\n",
      "            [ 74],\n",
      "            [ 28]], device='cuda:0', dtype=torch.uint8))\n",
      "16 model.layers.2.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[235],\n",
      "            [102],\n",
      "            [247],\n",
      "            ...,\n",
      "            [118],\n",
      "            [150],\n",
      "            [163]], device='cuda:0', dtype=torch.uint8))\n",
      "17 model.layers.2.input_layernorm.weight Parameter containing:\n",
      "tensor([0.0471, 0.0515, 0.0481,  ..., 0.0498, 0.0500, 0.1387], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "18 model.layers.2.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.1328, 0.1328, 0.1289,  ..., 0.1299, 0.1328, 0.0942], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "19 model.layers.3.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[193],\n",
      "            [ 19],\n",
      "            [157],\n",
      "            ...,\n",
      "            [ 70],\n",
      "            [237],\n",
      "            [183]], device='cuda:0', dtype=torch.uint8))\n",
      "20 model.layers.3.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[227],\n",
      "            [145],\n",
      "            [ 35],\n",
      "            ...,\n",
      "            [ 86],\n",
      "            [ 70],\n",
      "            [103]], device='cuda:0', dtype=torch.uint8))\n",
      "21 model.layers.3.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 34],\n",
      "            [161],\n",
      "            [100],\n",
      "            ...,\n",
      "            [202],\n",
      "            [ 39],\n",
      "            [ 99]], device='cuda:0', dtype=torch.uint8))\n",
      "22 model.layers.3.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[185],\n",
      "            [186],\n",
      "            [ 93],\n",
      "            ...,\n",
      "            [ 39],\n",
      "            [  5],\n",
      "            [212]], device='cuda:0', dtype=torch.uint8))\n",
      "23 model.layers.3.input_layernorm.weight Parameter containing:\n",
      "tensor([0.1650, 0.1416, 0.1289,  ..., 0.1387, 0.1426, 0.1719], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "24 model.layers.3.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.1475, 0.1465, 0.1465,  ..., 0.1523, 0.1494, 0.1177], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "25 model.layers.4.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[139],\n",
      "            [130],\n",
      "            [155],\n",
      "            ...,\n",
      "            [195],\n",
      "            [162],\n",
      "            [129]], device='cuda:0', dtype=torch.uint8))\n",
      "26 model.layers.4.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 55],\n",
      "            [ 55],\n",
      "            [107],\n",
      "            ...,\n",
      "            [200],\n",
      "            [118],\n",
      "            [ 88]], device='cuda:0', dtype=torch.uint8))\n",
      "27 model.layers.4.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[175],\n",
      "            [ 78],\n",
      "            [165],\n",
      "            ...,\n",
      "            [102],\n",
      "            [ 58],\n",
      "            [123]], device='cuda:0', dtype=torch.uint8))\n",
      "28 model.layers.4.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[148],\n",
      "            [149],\n",
      "            [ 65],\n",
      "            ...,\n",
      "            [192],\n",
      "            [252],\n",
      "            [ 16]], device='cuda:0', dtype=torch.uint8))\n",
      "29 model.layers.4.input_layernorm.weight Parameter containing:\n",
      "tensor([0.2617, 0.2256, 0.2109,  ..., 0.2334, 0.2266, 0.2500], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "30 model.layers.4.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.1650, 0.1699, 0.1660,  ..., 0.1650, 0.1689, 0.1387], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "31 model.layers.5.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 18],\n",
      "            [151],\n",
      "            [226],\n",
      "            ...,\n",
      "            [ 87],\n",
      "            [ 53],\n",
      "            [215]], device='cuda:0', dtype=torch.uint8))\n",
      "32 model.layers.5.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[183],\n",
      "            [179],\n",
      "            [129],\n",
      "            ...,\n",
      "            [ 29],\n",
      "            [ 81],\n",
      "            [236]], device='cuda:0', dtype=torch.uint8))\n",
      "33 model.layers.5.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[110],\n",
      "            [130],\n",
      "            [182],\n",
      "            ...,\n",
      "            [ 99],\n",
      "            [234],\n",
      "            [139]], device='cuda:0', dtype=torch.uint8))\n",
      "34 model.layers.5.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[157],\n",
      "            [200],\n",
      "            [133],\n",
      "            ...,\n",
      "            [233],\n",
      "            [132],\n",
      "            [ 69]], device='cuda:0', dtype=torch.uint8))\n",
      "35 model.layers.5.input_layernorm.weight Parameter containing:\n",
      "tensor([0.2754, 0.2637, 0.2480,  ..., 0.2539, 0.2480, 0.2695], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "36 model.layers.5.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.1865, 0.1865, 0.1885,  ..., 0.1855, 0.1934, 0.1602], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "37 model.layers.6.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 40],\n",
      "            [105],\n",
      "            [123],\n",
      "            ...,\n",
      "            [119],\n",
      "            [125],\n",
      "            [ 90]], device='cuda:0', dtype=torch.uint8))\n",
      "38 model.layers.6.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[196],\n",
      "            [149],\n",
      "            [ 74],\n",
      "            ...,\n",
      "            [169],\n",
      "            [ 58],\n",
      "            [ 44]], device='cuda:0', dtype=torch.uint8))\n",
      "39 model.layers.6.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 86],\n",
      "            [181],\n",
      "            [177],\n",
      "            ...,\n",
      "            [146],\n",
      "            [154],\n",
      "            [116]], device='cuda:0', dtype=torch.uint8))\n",
      "40 model.layers.6.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[167],\n",
      "            [ 33],\n",
      "            [212],\n",
      "            ...,\n",
      "            [ 40],\n",
      "            [229],\n",
      "            [136]], device='cuda:0', dtype=torch.uint8))\n",
      "41 model.layers.6.input_layernorm.weight Parameter containing:\n",
      "tensor([0.3164, 0.3203, 0.3047,  ..., 0.3145, 0.3047, 0.3086], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "42 model.layers.6.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2021, 0.2012, 0.2041,  ..., 0.1973, 0.2051, 0.1738], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "43 model.layers.7.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[114],\n",
      "            [ 23],\n",
      "            [115],\n",
      "            ...,\n",
      "            [ 90],\n",
      "            [179],\n",
      "            [ 75]], device='cuda:0', dtype=torch.uint8))\n",
      "44 model.layers.7.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[133],\n",
      "            [149],\n",
      "            [153],\n",
      "            ...,\n",
      "            [129],\n",
      "            [185],\n",
      "            [107]], device='cuda:0', dtype=torch.uint8))\n",
      "45 model.layers.7.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[209],\n",
      "            [ 41],\n",
      "            [194],\n",
      "            ...,\n",
      "            [111],\n",
      "            [152],\n",
      "            [155]], device='cuda:0', dtype=torch.uint8))\n",
      "46 model.layers.7.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[124],\n",
      "            [133],\n",
      "            [ 24],\n",
      "            ...,\n",
      "            [136],\n",
      "            [152],\n",
      "            [ 23]], device='cuda:0', dtype=torch.uint8))\n",
      "47 model.layers.7.input_layernorm.weight Parameter containing:\n",
      "tensor([0.3516, 0.3496, 0.3398,  ..., 0.3438, 0.3379, 0.3301], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "48 model.layers.7.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2217, 0.2178, 0.2256,  ..., 0.2207, 0.2285, 0.1885], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "49 model.layers.8.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 83],\n",
      "            [ 83],\n",
      "            [ 54],\n",
      "            ...,\n",
      "            [ 57],\n",
      "            [101],\n",
      "            [252]], device='cuda:0', dtype=torch.uint8))\n",
      "50 model.layers.8.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[129],\n",
      "            [ 13],\n",
      "            [ 73],\n",
      "            ...,\n",
      "            [ 59],\n",
      "            [246],\n",
      "            [107]], device='cuda:0', dtype=torch.uint8))\n",
      "51 model.layers.8.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[120],\n",
      "            [123],\n",
      "            [ 20],\n",
      "            ...,\n",
      "            [107],\n",
      "            [227],\n",
      "            [ 90]], device='cuda:0', dtype=torch.uint8))\n",
      "52 model.layers.8.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[169],\n",
      "            [196],\n",
      "            [194],\n",
      "            ...,\n",
      "            [131],\n",
      "            [124],\n",
      "            [155]], device='cuda:0', dtype=torch.uint8))\n",
      "53 model.layers.8.input_layernorm.weight Parameter containing:\n",
      "tensor([0.3672, 0.3945, 0.3691,  ..., 0.3770, 0.3770, 0.3516], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "54 model.layers.8.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2363, 0.2334, 0.2451,  ..., 0.2344, 0.2432, 0.2002], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "55 model.layers.9.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 54],\n",
      "            [197],\n",
      "            [220],\n",
      "            ...,\n",
      "            [ 67],\n",
      "            [135],\n",
      "            [ 63]], device='cuda:0', dtype=torch.uint8))\n",
      "56 model.layers.9.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[139],\n",
      "            [ 73],\n",
      "            [156],\n",
      "            ...,\n",
      "            [ 38],\n",
      "            [ 68],\n",
      "            [156]], device='cuda:0', dtype=torch.uint8))\n",
      "57 model.layers.9.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[138],\n",
      "            [ 52],\n",
      "            [149],\n",
      "            ...,\n",
      "            [203],\n",
      "            [146],\n",
      "            [ 73]], device='cuda:0', dtype=torch.uint8))\n",
      "58 model.layers.9.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 51],\n",
      "            [121],\n",
      "            [210],\n",
      "            ...,\n",
      "            [123],\n",
      "            [163],\n",
      "            [145]], device='cuda:0', dtype=torch.uint8))\n",
      "59 model.layers.9.input_layernorm.weight Parameter containing:\n",
      "tensor([0.3574, 0.3574, 0.3613,  ..., 0.3340, 0.3496, 0.3164], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "60 model.layers.9.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2480, 0.2432, 0.2559,  ..., 0.2500, 0.2539, 0.2119], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "61 model.layers.10.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 67],\n",
      "            [ 38],\n",
      "            [113],\n",
      "            ...,\n",
      "            [100],\n",
      "            [ 40],\n",
      "            [117]], device='cuda:0', dtype=torch.uint8))\n",
      "62 model.layers.10.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 33],\n",
      "            [195],\n",
      "            [162],\n",
      "            ...,\n",
      "            [242],\n",
      "            [153],\n",
      "            [102]], device='cuda:0', dtype=torch.uint8))\n",
      "63 model.layers.10.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 53],\n",
      "            [210],\n",
      "            [ 74],\n",
      "            ...,\n",
      "            [116],\n",
      "            [123],\n",
      "            [ 35]], device='cuda:0', dtype=torch.uint8))\n",
      "64 model.layers.10.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 81],\n",
      "            [164],\n",
      "            [197],\n",
      "            ...,\n",
      "            [ 86],\n",
      "            [202],\n",
      "            [184]], device='cuda:0', dtype=torch.uint8))\n",
      "65 model.layers.10.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4102, 0.4219, 0.4258,  ..., 0.4082, 0.4043, 0.3965], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "66 model.layers.10.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2598, 0.2500, 0.2695,  ..., 0.2539, 0.2676, 0.2188], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "67 model.layers.11.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[174],\n",
      "            [210],\n",
      "            [ 44],\n",
      "            ...,\n",
      "            [ 87],\n",
      "            [ 78],\n",
      "            [ 54]], device='cuda:0', dtype=torch.uint8))\n",
      "68 model.layers.11.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[125],\n",
      "            [218],\n",
      "            [184],\n",
      "            ...,\n",
      "            [195],\n",
      "            [ 76],\n",
      "            [118]], device='cuda:0', dtype=torch.uint8))\n",
      "69 model.layers.11.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[237],\n",
      "            [141],\n",
      "            [ 59],\n",
      "            ...,\n",
      "            [ 51],\n",
      "            [ 70],\n",
      "            [ 65]], device='cuda:0', dtype=torch.uint8))\n",
      "70 model.layers.11.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[  1],\n",
      "            [211],\n",
      "            [107],\n",
      "            ...,\n",
      "            [ 45],\n",
      "            [ 78],\n",
      "            [  9]], device='cuda:0', dtype=torch.uint8))\n",
      "71 model.layers.11.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4023, 0.4160, 0.4141,  ..., 0.3887, 0.3984, 0.3574], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "72 model.layers.11.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2676, 0.2598, 0.2773,  ..., 0.2617, 0.2676, 0.2266], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "73 model.layers.12.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 82],\n",
      "            [195],\n",
      "            [ 59],\n",
      "            ...,\n",
      "            [ 69],\n",
      "            [123],\n",
      "            [ 25]], device='cuda:0', dtype=torch.uint8))\n",
      "74 model.layers.12.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[196],\n",
      "            [ 22],\n",
      "            [ 98],\n",
      "            ...,\n",
      "            [165],\n",
      "            [ 19],\n",
      "            [ 72]], device='cuda:0', dtype=torch.uint8))\n",
      "75 model.layers.12.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[187],\n",
      "            [ 43],\n",
      "            [ 80],\n",
      "            ...,\n",
      "            [181],\n",
      "            [150],\n",
      "            [165]], device='cuda:0', dtype=torch.uint8))\n",
      "76 model.layers.12.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 34],\n",
      "            [156],\n",
      "            [171],\n",
      "            ...,\n",
      "            [126],\n",
      "            [244],\n",
      "            [135]], device='cuda:0', dtype=torch.uint8))\n",
      "77 model.layers.12.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4277, 0.4512, 0.4453,  ..., 0.4336, 0.4316, 0.4102], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "78 model.layers.12.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2695, 0.2637, 0.2793,  ..., 0.2656, 0.2676, 0.2295], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "79 model.layers.13.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[105],\n",
      "            [194],\n",
      "            [210],\n",
      "            ...,\n",
      "            [131],\n",
      "            [158],\n",
      "            [ 97]], device='cuda:0', dtype=torch.uint8))\n",
      "80 model.layers.13.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 85],\n",
      "            [118],\n",
      "            [170],\n",
      "            ...,\n",
      "            [101],\n",
      "            [ 24],\n",
      "            [ 70]], device='cuda:0', dtype=torch.uint8))\n",
      "81 model.layers.13.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 66],\n",
      "            [215],\n",
      "            [215],\n",
      "            ...,\n",
      "            [209],\n",
      "            [ 60],\n",
      "            [ 68]], device='cuda:0', dtype=torch.uint8))\n",
      "82 model.layers.13.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 92],\n",
      "            [ 95],\n",
      "            [ 26],\n",
      "            ...,\n",
      "            [201],\n",
      "            [  4],\n",
      "            [ 36]], device='cuda:0', dtype=torch.uint8))\n",
      "83 model.layers.13.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4629, 0.4648, 0.4746,  ..., 0.4727, 0.4629, 0.4141], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "84 model.layers.13.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2812, 0.2715, 0.2930,  ..., 0.2773, 0.2793, 0.2461], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "85 model.layers.14.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[199],\n",
      "            [182],\n",
      "            [ 55],\n",
      "            ...,\n",
      "            [148],\n",
      "            [137],\n",
      "            [ 53]], device='cuda:0', dtype=torch.uint8))\n",
      "86 model.layers.14.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 99],\n",
      "            [ 10],\n",
      "            [113],\n",
      "            ...,\n",
      "            [144],\n",
      "            [ 26],\n",
      "            [107]], device='cuda:0', dtype=torch.uint8))\n",
      "87 model.layers.14.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[  8],\n",
      "            [182],\n",
      "            [134],\n",
      "            ...,\n",
      "            [ 65],\n",
      "            [ 19],\n",
      "            [154]], device='cuda:0', dtype=torch.uint8))\n",
      "88 model.layers.14.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[165],\n",
      "            [ 70],\n",
      "            [142],\n",
      "            ...,\n",
      "            [ 83],\n",
      "            [181],\n",
      "            [119]], device='cuda:0', dtype=torch.uint8))\n",
      "89 model.layers.14.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4414, 0.4473, 0.4531,  ..., 0.4434, 0.4375, 0.4004], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "90 model.layers.14.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2871, 0.2715, 0.2969,  ..., 0.2793, 0.2773, 0.2520], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "91 model.layers.15.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 99],\n",
      "            [103],\n",
      "            [ 99],\n",
      "            ...,\n",
      "            [101],\n",
      "            [183],\n",
      "            [123]], device='cuda:0', dtype=torch.uint8))\n",
      "92 model.layers.15.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 83],\n",
      "            [ 84],\n",
      "            [168],\n",
      "            ...,\n",
      "            [ 76],\n",
      "            [183],\n",
      "            [ 59]], device='cuda:0', dtype=torch.uint8))\n",
      "93 model.layers.15.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 89],\n",
      "            [134],\n",
      "            [180],\n",
      "            ...,\n",
      "            [ 11],\n",
      "            [211],\n",
      "            [184]], device='cuda:0', dtype=torch.uint8))\n",
      "94 model.layers.15.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[102],\n",
      "            [ 44],\n",
      "            [ 53],\n",
      "            ...,\n",
      "            [141],\n",
      "            [133],\n",
      "            [101]], device='cuda:0', dtype=torch.uint8))\n",
      "95 model.layers.15.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4395, 0.4629, 0.4746,  ..., 0.4551, 0.4531, 0.4141], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "96 model.layers.15.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.2930, 0.2832, 0.3047,  ..., 0.2871, 0.2891, 0.2617], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "97 model.layers.16.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 26],\n",
      "            [233],\n",
      "            [130],\n",
      "            ...,\n",
      "            [ 76],\n",
      "            [133],\n",
      "            [ 85]], device='cuda:0', dtype=torch.uint8))\n",
      "98 model.layers.16.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[221],\n",
      "            [238],\n",
      "            [176],\n",
      "            ...,\n",
      "            [ 75],\n",
      "            [138],\n",
      "            [236]], device='cuda:0', dtype=torch.uint8))\n",
      "99 model.layers.16.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[195],\n",
      "            [162],\n",
      "            [ 86],\n",
      "            ...,\n",
      "            [ 80],\n",
      "            [ 61],\n",
      "            [ 39]], device='cuda:0', dtype=torch.uint8))\n",
      "100 model.layers.16.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[157],\n",
      "            [101],\n",
      "            [ 79],\n",
      "            ...,\n",
      "            [170],\n",
      "            [109],\n",
      "            [136]], device='cuda:0', dtype=torch.uint8))\n",
      "101 model.layers.16.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4766, 0.4824, 0.4941,  ..., 0.4785, 0.4824, 0.4414], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "102 model.layers.16.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.3086, 0.2910, 0.3145,  ..., 0.2988, 0.2949, 0.2793], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "103 model.layers.17.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[195],\n",
      "            [154],\n",
      "            [190],\n",
      "            ...,\n",
      "            [103],\n",
      "            [ 70],\n",
      "            [139]], device='cuda:0', dtype=torch.uint8))\n",
      "104 model.layers.17.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[193],\n",
      "            [120],\n",
      "            [ 27],\n",
      "            ...,\n",
      "            [ 55],\n",
      "            [136],\n",
      "            [222]], device='cuda:0', dtype=torch.uint8))\n",
      "105 model.layers.17.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[101],\n",
      "            [106],\n",
      "            [214],\n",
      "            ...,\n",
      "            [ 88],\n",
      "            [107],\n",
      "            [135]], device='cuda:0', dtype=torch.uint8))\n",
      "106 model.layers.17.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[120],\n",
      "            [194],\n",
      "            [171],\n",
      "            ...,\n",
      "            [220],\n",
      "            [157],\n",
      "            [101]], device='cuda:0', dtype=torch.uint8))\n",
      "107 model.layers.17.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4746, 0.4863, 0.5039,  ..., 0.4941, 0.4941, 0.4590], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "108 model.layers.17.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.3203, 0.3047, 0.3262,  ..., 0.3145, 0.3105, 0.2949], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "109 model.layers.18.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[186],\n",
      "            [227],\n",
      "            [225],\n",
      "            ...,\n",
      "            [104],\n",
      "            [104],\n",
      "            [179]], device='cuda:0', dtype=torch.uint8))\n",
      "110 model.layers.18.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[152],\n",
      "            [138],\n",
      "            [ 74],\n",
      "            ...,\n",
      "            [ 99],\n",
      "            [143],\n",
      "            [184]], device='cuda:0', dtype=torch.uint8))\n",
      "111 model.layers.18.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 98],\n",
      "            [180],\n",
      "            [104],\n",
      "            ...,\n",
      "            [184],\n",
      "            [201],\n",
      "            [120]], device='cuda:0', dtype=torch.uint8))\n",
      "112 model.layers.18.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[129],\n",
      "            [108],\n",
      "            [199],\n",
      "            ...,\n",
      "            [ 59],\n",
      "            [156],\n",
      "            [ 44]], device='cuda:0', dtype=torch.uint8))\n",
      "113 model.layers.18.input_layernorm.weight Parameter containing:\n",
      "tensor([0.4844, 0.4922, 0.5039,  ..., 0.4902, 0.4902, 0.4746], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "114 model.layers.18.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.3320, 0.3203, 0.3418,  ..., 0.3281, 0.3301, 0.3145], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "115 model.layers.19.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[177],\n",
      "            [211],\n",
      "            [194],\n",
      "            ...,\n",
      "            [ 94],\n",
      "            [153],\n",
      "            [130]], device='cuda:0', dtype=torch.uint8))\n",
      "116 model.layers.19.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[150],\n",
      "            [  5],\n",
      "            [170],\n",
      "            ...,\n",
      "            [ 26],\n",
      "            [121],\n",
      "            [ 74]], device='cuda:0', dtype=torch.uint8))\n",
      "117 model.layers.19.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[118],\n",
      "            [ 32],\n",
      "            [198],\n",
      "            ...,\n",
      "            [ 84],\n",
      "            [199],\n",
      "            [135]], device='cuda:0', dtype=torch.uint8))\n",
      "118 model.layers.19.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 75],\n",
      "            [181],\n",
      "            [152],\n",
      "            ...,\n",
      "            [ 70],\n",
      "            [ 94],\n",
      "            [182]], device='cuda:0', dtype=torch.uint8))\n",
      "119 model.layers.19.input_layernorm.weight Parameter containing:\n",
      "tensor([0.5039, 0.5156, 0.5078,  ..., 0.5117, 0.5117, 0.5195], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "120 model.layers.19.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.3594, 0.3438, 0.3652,  ..., 0.3477, 0.3418, 0.3379], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "121 model.layers.20.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[180],\n",
      "            [215],\n",
      "            [134],\n",
      "            ...,\n",
      "            [155],\n",
      "            [102],\n",
      "            [ 73]], device='cuda:0', dtype=torch.uint8))\n",
      "122 model.layers.20.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[115],\n",
      "            [131],\n",
      "            [ 73],\n",
      "            ...,\n",
      "            [204],\n",
      "            [105],\n",
      "            [180]], device='cuda:0', dtype=torch.uint8))\n",
      "123 model.layers.20.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[193],\n",
      "            [ 58],\n",
      "            [ 99],\n",
      "            ...,\n",
      "            [244],\n",
      "            [140],\n",
      "            [ 73]], device='cuda:0', dtype=torch.uint8))\n",
      "124 model.layers.20.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 27],\n",
      "            [172],\n",
      "            [104],\n",
      "            ...,\n",
      "            [190],\n",
      "            [ 17],\n",
      "            [107]], device='cuda:0', dtype=torch.uint8))\n",
      "125 model.layers.20.input_layernorm.weight Parameter containing:\n",
      "tensor([0.5352, 0.5352, 0.5273,  ..., 0.5273, 0.5352, 0.5469], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "126 model.layers.20.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.3711, 0.3594, 0.3828,  ..., 0.3711, 0.3594, 0.3535], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "127 model.layers.21.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[126],\n",
      "            [195],\n",
      "            [ 75],\n",
      "            ...,\n",
      "            [  5],\n",
      "            [ 71],\n",
      "            [161]], device='cuda:0', dtype=torch.uint8))\n",
      "128 model.layers.21.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[201],\n",
      "            [210],\n",
      "            [211],\n",
      "            ...,\n",
      "            [158],\n",
      "            [106],\n",
      "            [152]], device='cuda:0', dtype=torch.uint8))\n",
      "129 model.layers.21.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[210],\n",
      "            [ 71],\n",
      "            [ 28],\n",
      "            ...,\n",
      "            [ 52],\n",
      "            [ 81],\n",
      "            [162]], device='cuda:0', dtype=torch.uint8))\n",
      "130 model.layers.21.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 26],\n",
      "            [ 29],\n",
      "            [187],\n",
      "            ...,\n",
      "            [152],\n",
      "            [ 74],\n",
      "            [ 84]], device='cuda:0', dtype=torch.uint8))\n",
      "131 model.layers.21.input_layernorm.weight Parameter containing:\n",
      "tensor([0.5547, 0.5508, 0.5469,  ..., 0.5508, 0.5469, 0.5664], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "132 model.layers.21.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.3887, 0.3730, 0.3965,  ..., 0.3867, 0.3789, 0.3789], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "133 model.layers.22.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[  5],\n",
      "            [124],\n",
      "            [163],\n",
      "            ...,\n",
      "            [122],\n",
      "            [169],\n",
      "            [212]], device='cuda:0', dtype=torch.uint8))\n",
      "134 model.layers.22.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 64],\n",
      "            [187],\n",
      "            [ 43],\n",
      "            ...,\n",
      "            [217],\n",
      "            [ 21],\n",
      "            [106]], device='cuda:0', dtype=torch.uint8))\n",
      "135 model.layers.22.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 99],\n",
      "            [ 81],\n",
      "            [ 27],\n",
      "            ...,\n",
      "            [110],\n",
      "            [ 87],\n",
      "            [ 75]], device='cuda:0', dtype=torch.uint8))\n",
      "136 model.layers.22.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 74],\n",
      "            [108],\n",
      "            [139],\n",
      "            ...,\n",
      "            [226],\n",
      "            [ 92],\n",
      "            [134]], device='cuda:0', dtype=torch.uint8))\n",
      "137 model.layers.22.input_layernorm.weight Parameter containing:\n",
      "tensor([0.5625, 0.5742, 0.5625,  ..., 0.5469, 0.5664, 0.5938], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "138 model.layers.22.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.4082, 0.3984, 0.4121,  ..., 0.4102, 0.3984, 0.3984], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "139 model.layers.23.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[134],\n",
      "            [130],\n",
      "            [120],\n",
      "            ...,\n",
      "            [ 25],\n",
      "            [ 20],\n",
      "            [ 32]], device='cuda:0', dtype=torch.uint8))\n",
      "140 model.layers.23.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 86],\n",
      "            [ 84],\n",
      "            [102],\n",
      "            ...,\n",
      "            [192],\n",
      "            [215],\n",
      "            [138]], device='cuda:0', dtype=torch.uint8))\n",
      "141 model.layers.23.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[215],\n",
      "            [102],\n",
      "            [209],\n",
      "            ...,\n",
      "            [181],\n",
      "            [ 89],\n",
      "            [102]], device='cuda:0', dtype=torch.uint8))\n",
      "142 model.layers.23.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 52],\n",
      "            [104],\n",
      "            [190],\n",
      "            ...,\n",
      "            [167],\n",
      "            [182],\n",
      "            [194]], device='cuda:0', dtype=torch.uint8))\n",
      "143 model.layers.23.input_layernorm.weight Parameter containing:\n",
      "tensor([0.5820, 0.5938, 0.5898,  ..., 0.5859, 0.5938, 0.6211], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "144 model.layers.23.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.4258, 0.4160, 0.4414,  ..., 0.4355, 0.4297, 0.4219], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "145 model.layers.24.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[166],\n",
      "            [ 52],\n",
      "            [130],\n",
      "            ...,\n",
      "            [ 83],\n",
      "            [183],\n",
      "            [180]], device='cuda:0', dtype=torch.uint8))\n",
      "146 model.layers.24.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 77],\n",
      "            [ 50],\n",
      "            [166],\n",
      "            ...,\n",
      "            [ 89],\n",
      "            [146],\n",
      "            [ 57]], device='cuda:0', dtype=torch.uint8))\n",
      "147 model.layers.24.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[125],\n",
      "            [134],\n",
      "            [202],\n",
      "            ...,\n",
      "            [ 54],\n",
      "            [213],\n",
      "            [ 58]], device='cuda:0', dtype=torch.uint8))\n",
      "148 model.layers.24.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 89],\n",
      "            [ 84],\n",
      "            [ 43],\n",
      "            ...,\n",
      "            [100],\n",
      "            [ 20],\n",
      "            [113]], device='cuda:0', dtype=torch.uint8))\n",
      "149 model.layers.24.input_layernorm.weight Parameter containing:\n",
      "tensor([0.6016, 0.6172, 0.5898,  ..., 0.6016, 0.6094, 0.6367], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "150 model.layers.24.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.4609, 0.4434, 0.4668,  ..., 0.4629, 0.4570, 0.4492], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "151 model.layers.25.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 74],\n",
      "            [218],\n",
      "            [118],\n",
      "            ...,\n",
      "            [201],\n",
      "            [ 27],\n",
      "            [ 71]], device='cuda:0', dtype=torch.uint8))\n",
      "152 model.layers.25.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 54],\n",
      "            [166],\n",
      "            [ 90],\n",
      "            ...,\n",
      "            [165],\n",
      "            [167],\n",
      "            [ 86]], device='cuda:0', dtype=torch.uint8))\n",
      "153 model.layers.25.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 96],\n",
      "            [ 85],\n",
      "            [198],\n",
      "            ...,\n",
      "            [222],\n",
      "            [ 34],\n",
      "            [ 24]], device='cuda:0', dtype=torch.uint8))\n",
      "154 model.layers.25.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[100],\n",
      "            [ 94],\n",
      "            [ 88],\n",
      "            ...,\n",
      "            [115],\n",
      "            [109],\n",
      "            [ 87]], device='cuda:0', dtype=torch.uint8))\n",
      "155 model.layers.25.input_layernorm.weight Parameter containing:\n",
      "tensor([0.6484, 0.6562, 0.6211,  ..., 0.6328, 0.6445, 0.6875], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "156 model.layers.25.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.4785, 0.4746, 0.4902,  ..., 0.4902, 0.4805, 0.4824], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "157 model.layers.26.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[112],\n",
      "            [174],\n",
      "            [ 52],\n",
      "            ...,\n",
      "            [ 73],\n",
      "            [191],\n",
      "            [154]], device='cuda:0', dtype=torch.uint8))\n",
      "158 model.layers.26.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[118],\n",
      "            [ 90],\n",
      "            [188],\n",
      "            ...,\n",
      "            [203],\n",
      "            [186],\n",
      "            [ 29]], device='cuda:0', dtype=torch.uint8))\n",
      "159 model.layers.26.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[170],\n",
      "            [165],\n",
      "            [ 37],\n",
      "            ...,\n",
      "            [ 24],\n",
      "            [203],\n",
      "            [ 77]], device='cuda:0', dtype=torch.uint8))\n",
      "160 model.layers.26.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 85],\n",
      "            [103],\n",
      "            [124],\n",
      "            ...,\n",
      "            [  3],\n",
      "            [ 59],\n",
      "            [181]], device='cuda:0', dtype=torch.uint8))\n",
      "161 model.layers.26.input_layernorm.weight Parameter containing:\n",
      "tensor([0.6680, 0.6758, 0.6172,  ..., 0.6602, 0.6719, 0.7188], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "162 model.layers.26.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.5117, 0.5000, 0.5156,  ..., 0.5156, 0.4980, 0.5078], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "163 model.layers.27.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[149],\n",
      "            [226],\n",
      "            [ 99],\n",
      "            ...,\n",
      "            [121],\n",
      "            [124],\n",
      "            [ 63]], device='cuda:0', dtype=torch.uint8))\n",
      "164 model.layers.27.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 68],\n",
      "            [165],\n",
      "            [231],\n",
      "            ...,\n",
      "            [ 50],\n",
      "            [177],\n",
      "            [228]], device='cuda:0', dtype=torch.uint8))\n",
      "165 model.layers.27.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 43],\n",
      "            [131],\n",
      "            [126],\n",
      "            ...,\n",
      "            [221],\n",
      "            [192],\n",
      "            [135]], device='cuda:0', dtype=torch.uint8))\n",
      "166 model.layers.27.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[214],\n",
      "            [253],\n",
      "            [149],\n",
      "            ...,\n",
      "            [ 43],\n",
      "            [121],\n",
      "            [ 53]], device='cuda:0', dtype=torch.uint8))\n",
      "167 model.layers.27.input_layernorm.weight Parameter containing:\n",
      "tensor([0.6992, 0.7188, 0.6719,  ..., 0.6914, 0.7031, 0.7461], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "168 model.layers.27.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.5352, 0.5273, 0.5352,  ..., 0.5469, 0.5273, 0.5391], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "169 model.layers.28.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[233],\n",
      "            [142],\n",
      "            [165],\n",
      "            ...,\n",
      "            [103],\n",
      "            [243],\n",
      "            [102]], device='cuda:0', dtype=torch.uint8))\n",
      "170 model.layers.28.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 43],\n",
      "            [135],\n",
      "            [122],\n",
      "            ...,\n",
      "            [ 80],\n",
      "            [ 38],\n",
      "            [ 17]], device='cuda:0', dtype=torch.uint8))\n",
      "171 model.layers.28.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[199],\n",
      "            [214],\n",
      "            [238],\n",
      "            ...,\n",
      "            [ 82],\n",
      "            [224],\n",
      "            [153]], device='cuda:0', dtype=torch.uint8))\n",
      "172 model.layers.28.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[120],\n",
      "            [237],\n",
      "            [205],\n",
      "            ...,\n",
      "            [102],\n",
      "            [ 75],\n",
      "            [187]], device='cuda:0', dtype=torch.uint8))\n",
      "173 model.layers.28.input_layernorm.weight Parameter containing:\n",
      "tensor([0.7109, 0.7305, 0.6875,  ..., 0.7070, 0.7266, 0.7539], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "174 model.layers.28.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.5625, 0.5508, 0.5664,  ..., 0.5625, 0.5625, 0.5586], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "175 model.layers.29.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[164],\n",
      "            [112],\n",
      "            [ 59],\n",
      "            ...,\n",
      "            [212],\n",
      "            [101],\n",
      "            [ 61]], device='cuda:0', dtype=torch.uint8))\n",
      "176 model.layers.29.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[134],\n",
      "            [180],\n",
      "            [ 69],\n",
      "            ...,\n",
      "            [ 36],\n",
      "            [105],\n",
      "            [133]], device='cuda:0', dtype=torch.uint8))\n",
      "177 model.layers.29.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[171],\n",
      "            [171],\n",
      "            [218],\n",
      "            ...,\n",
      "            [100],\n",
      "            [ 53],\n",
      "            [212]], device='cuda:0', dtype=torch.uint8))\n",
      "178 model.layers.29.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[103],\n",
      "            [ 13],\n",
      "            [ 81],\n",
      "            ...,\n",
      "            [ 66],\n",
      "            [ 83],\n",
      "            [ 82]], device='cuda:0', dtype=torch.uint8))\n",
      "179 model.layers.29.input_layernorm.weight Parameter containing:\n",
      "tensor([0.7188, 0.7461, 0.7148,  ..., 0.7109, 0.7578, 0.7578], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "180 model.layers.29.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.5977, 0.5820, 0.5938,  ..., 0.5859, 0.5938, 0.5859], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "181 model.layers.30.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[244],\n",
      "            [ 77],\n",
      "            [ 99],\n",
      "            ...,\n",
      "            [102],\n",
      "            [ 54],\n",
      "            [190]], device='cuda:0', dtype=torch.uint8))\n",
      "182 model.layers.30.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 15],\n",
      "            [165],\n",
      "            [202],\n",
      "            ...,\n",
      "            [169],\n",
      "            [208],\n",
      "            [ 69]], device='cuda:0', dtype=torch.uint8))\n",
      "183 model.layers.30.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[149],\n",
      "            [192],\n",
      "            [219],\n",
      "            ...,\n",
      "            [181],\n",
      "            [ 58],\n",
      "            [113]], device='cuda:0', dtype=torch.uint8))\n",
      "184 model.layers.30.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[155],\n",
      "            [212],\n",
      "            [107],\n",
      "            ...,\n",
      "            [  7],\n",
      "            [194],\n",
      "            [ 94]], device='cuda:0', dtype=torch.uint8))\n",
      "185 model.layers.30.input_layernorm.weight Parameter containing:\n",
      "tensor([0.7148, 0.7500, 0.7031,  ..., 0.7266, 0.7422, 0.7500], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "186 model.layers.30.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.6094, 0.6055, 0.6094,  ..., 0.6094, 0.6133, 0.6289], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "187 model.layers.31.self_attn.o_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[161],\n",
      "            [214],\n",
      "            [179],\n",
      "            ...,\n",
      "            [233],\n",
      "            [ 34],\n",
      "            [154]], device='cuda:0', dtype=torch.uint8))\n",
      "188 model.layers.31.self_attn.qkv_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[115],\n",
      "            [177],\n",
      "            [106],\n",
      "            ...,\n",
      "            [ 72],\n",
      "            [ 22],\n",
      "            [ 29]], device='cuda:0', dtype=torch.uint8))\n",
      "189 model.layers.31.mlp.gate_up_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[237],\n",
      "            [207],\n",
      "            [ 19],\n",
      "            ...,\n",
      "            [103],\n",
      "            [117],\n",
      "            [ 22]], device='cuda:0', dtype=torch.uint8))\n",
      "190 model.layers.31.mlp.down_proj.weight Parameter containing:\n",
      "Parameter(Params4bit([[ 87],\n",
      "            [149],\n",
      "            [ 59],\n",
      "            ...,\n",
      "            [ 78],\n",
      "            [ 59],\n",
      "            [198]], device='cuda:0', dtype=torch.uint8))\n",
      "191 model.layers.31.input_layernorm.weight Parameter containing:\n",
      "tensor([0.6914, 0.6914, 0.6680,  ..., 0.6914, 0.6953, 0.7227], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "192 model.layers.31.post_attention_layernorm.weight Parameter containing:\n",
      "tensor([0.5703, 0.5742, 0.5547,  ..., 0.5859, 0.6055, 0.6016], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "193 model.norm.weight Parameter containing:\n",
      "tensor([1.4688, 1.4922, 1.5078,  ..., 1.5078, 1.5469, 1.5781], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "194 lm_head.weight Parameter containing:\n",
      "tensor([[-0.0493, -0.0129,  0.0231,  ..., -0.0161,  0.0630,  0.0723],\n",
      "        [ 0.0094, -0.0144, -0.0201,  ...,  0.0269,  0.0239, -0.0026],\n",
      "        [ 0.0562, -0.0168, -0.0282,  ...,  0.0292,  0.0869,  0.0092],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(base_model.named_parameters()): \n",
    "    print(idx, name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ca2ce4c-4e5f-46d8-a60d-3c804b5137de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f3205ce4eb0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c1280b-0834-4517-a14a-885a5cf2376c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Killing weights randomly across layers \n",
    "# Here, we simply kill a random portion of weights across layers, scaling up from 10-90% of weights in 5% increments. The only goal here is \n",
    "# to see how performance changes. \n",
    "\n",
    "# Desired output: line chart tracking successively increasing performance decay. \n",
    "\n",
    "# 1. Subset out activation layers (these are the ones we can modify) and count # of neurons across them so we can figure out which quantity = 10, 15, 20% etc. :) \n",
    "## a. Look at layer outputs; run forward pass and store intermediary computations - this tells us which are the activation layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71feecdd-7783-4b5a-b405-53d37a5e0fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-30): 31 x Phi3DecoderLayer(\n",
       "    (self_attn): Phi3Attention(\n",
       "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "      (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "      (rotary_emb): Phi3RotaryEmbedding()\n",
       "    )\n",
       "    (mlp): Phi3MLP(\n",
       "      (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "      (activation_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): Phi3RMSNorm()\n",
       "    (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (post_attention_layernorm): Phi3RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer_names # these are the available layers \n",
    "torch.nn.ModuleList([layer for i, layer in enumerate(base_model.model.layers) if i != 16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
