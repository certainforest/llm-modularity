{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b92319-88b3-4c7d-91fa-aa605c617cab",
   "metadata": {},
   "source": [
    "# Selective lesioning\n",
    "Now, our aim is to erode a model in a **controlled manner**. Model performance is tracked by benchmarking over a question set (generated w/ an assist from gpt-4 :)). \n",
    "\n",
    "To do: \n",
    "+ Load base model + replace mps code \n",
    "+ Pull in question set\n",
    "+ Establish basic eval framework\n",
    "\n",
    "Notes: \n",
    "+ Keep an eye on mem. use - disk space can be monitored via `du -hs $HOME /workspace/*` - we have 100GB avail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f59eef-37fe-4ce0-81a0-a4ccde05b7fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from dotenv import main\n",
    "import torch\n",
    "import json\n",
    "import jinja2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # for quantization\n",
    "from torch.nn import Softmax\n",
    "import plotly\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# auth for gated repos (like llama) - gen token here: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "# model ids\n",
    "model_id = [\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "# Set seed for reproducibility \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Increase max width of pd df columns \n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "# Instantiate jinja environment - used later for icl prompting \n",
    "environment = jinja2.Environment()\n",
    "\n",
    "# requirements.txt\n",
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4908544e-3af6-467c-98de-42664c2b9597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.000000GB\n",
      "Reserved: 0.000000GB\n",
      "Total: 47.535889GB\n"
     ]
    }
   ],
   "source": [
    "# Define utility functions \n",
    "# mem. monitoring! \n",
    "def check_memory():\n",
    "    print(\"Allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"Reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Total: %fGB\"%(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024))\n",
    "\n",
    "# notification/text-to-speech\n",
    "def text_to_speech(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        os.system(f'say \"{text}\"')\n",
    "    elif sys.platform.startswith('linux'):\n",
    "        os.system(f'espeak \"{text}\"')\n",
    "    else:\n",
    "        print(\"Text-to-speech is not supported on this platform.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3eed4-46b3-4b7d-9f2e-21a6425cdb39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
